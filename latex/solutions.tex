\chapter{SOLUTIONS TO EXERCISES}

\section{Exercises in chapter 01}

\begin{prf}\label{sol_1exer1}(Solution to~\ref{1exer1})
Find those numbers $x$ such that $d(x,-2) \le 5$. In other words, solve the inequality
   \[\abs{x+2} = \abs{x-(-2)} \le 5. \]
This may be rewritten as
   \[-5 \le x+2 \le 5, \]
which is the same as
   \[-7 \le x \le 3.\]
Thus the points in the closed interval $[-7,3]$ are those that lie within $5$ units of $-2$.
\end{prf}

\begin{prf}\label{sol_d_less_e}(Solution to~\ref{d_less_e})
If $x \in J_\delta(a)$, then $\abs{x-a} < \delta \le \epsilon$. Thus $x$ lies within
$\epsilon$ units of~$a$; that is, $x \in J_\epsilon(a)$.
\end{prf}

\begin{prf}\label{sol_1exer3}(Solution to~\ref{1exer3})
Factor the left side of the inequality $x^2-x-6 \ge 0$. This yields $(x+2)(x-3) \ge~0$.  This
inequality holds for those $x$ satisfying $x \le -2$ and for those satisfying $x \ge 3$.  Thus
$A = (-\infty,-2] \cup [3,\infty)$. No neighborhood of $-2$ or of $3$ lies in~$A$. Thus
$A^\circ = (-\infty,-2) \cup (3,\infty)$ and $A^\circ \ne A$.
\end{prf}

\begin{prf}\label{sol_intr_intersection}(Solution to~\ref{intr_intersection})
Since $A \cap B \subseteq A$ we have $\intr{(A \cap B)} \subseteq \intr A$ by
\ref{intr_subsets}. Similarly, $\intr{(A \cap B)} \subseteq \intr B$. Thus $\intr{(A \cap B)}
\subseteq \intr A \cap \intr B$. To obtain the reverse inclusion take $x \in \intr A \cap
\intr B$. Then there exist $\epsilon_1,\epsilon_2 > 0$ such that $J_{\epsilon_1}(x) \subseteq
A$ and $J_{\epsilon_2}(x) \subseteq B$. Then $J_\epsilon(x)  \subseteq A \cap B$ where
$\epsilon = \min\{\epsilon_1,\epsilon_2\}$. This shows that $x \in \intr{(A \cap B)}$.
\end{prf}

\begin{prf}\label{sol_intr_union}(Solution to~\ref{intr_union})
Since $A \subseteq \bigcup \sfml A$ for every $A \in \sfml A$, we can conclude from
proposition \ref{intr_subsets} that $\intr A \subseteq \intr{\bigl(\bigcup \sfml
A\bigr)}$ for every $A \in \sfml A$. Thus $\bigcup\{\intr A: A \in \sfml A\} \subseteq
\intr{\bigl(\bigcup \sfml A\bigr)}$. (See \ref{union_of_fam1}.)
\end{prf}



%END Chapter 01



\section{Exercises in chapter 02}

\begin{prf}\label{sol_union_intersection_open}(Solution to~\ref{union_intersection_open})
(a)\quad Let $\sfml{S}$ be a family of open subsets of $\R$. By
proposition~\ref{union_open_intervals} each nonempty member of $\sfml{S}$ is a union of
bounded open intervals.  But then $\bigcup \sfml{S}$ is itself a union of bounded open
intervals.  So $\bigcup \sfml{S}$ is open.

(b)\quad We show that if $S_1$ and $S_2$ are open subsets of $\R$, then $S_1 \cap S_2$ is
open. From this it follows easily by mathematical induction that if $S_1, \dots, S_n$ are all
open in $\R$, then $S_1 \cap \dots \cap S_n$ is open. Let us suppose then that $S_1$ and $S_2$
are open subsets of $\R$. If $S_1 \cap S_2 = \emptyset$, there is nothing to prove; so we
assume that $S_1 \cap S_2 \ne \emptyset$.  Let $x$ be an arbitrary point in $S_1 \cap S_2$.
Since $x \in S_1 = {S_1}^\circ$, there exists $\epsilon_1 > 0$ such that $J_{\epsilon_1}(x)
\subseteq S_1$.  Similarly, there exists $\epsilon_2 > 0$ such that $J_{\epsilon_2}(x)
\subseteq S_2$. Let $\epsilon$ be the smaller of $\epsilon_1$ and $\epsilon_2$.  Then clearly
$J_\epsilon(x) \subseteq S_1 \cap S_2$, which shows that $x$ is an interior point of $S_1 \cap
S_2$.  Since every point of the set $S_1 \cap S_2$ is an interior point of the set, $S_1 \cap
S_2$ is open.
\end{prf}

\begin{prf}\label{sol_exam_clo_rats}(Solution to~\ref{exam_clo_rats})
Since no point in $\R$ has an $\epsilon$-neighborhood consisting entirely of rational numbers,
$A^\circ = \emptyset$.  If $x \ge 0$, then every $\epsilon$-neighborhood of $x$ contains
infinitely many positive rational numbers.  Thus each such $x$ belongs to $A'$. If $x < 0$, it
is possible to find an $\epsilon$-neighborhood of $x$ which contains no positive rational
number. Thus $A' = [0,\infty)$ and $\clo A = A \cup A' = [0,\infty)$.
\end{prf}

\begin{prf}\label{sol_intr_vs_clo}(Solution to~\ref{intr_vs_clo})
(a)\quad First we show that $\clo{A^c} \subseteq A^{\circ c}$. If $x \in \clo{A^c}$, then
either $x \in A^c$ or $x$ is an accumulation point of $A^c$. If $x \in A^c$, then $x$ is
certainly not in the interior of $A$; that is, $x \in A^{\circ c}$.  On the other hand, if $x$
is an accumulation point of $A^c$, then every $\epsilon$-neighborhood of $x$ contains points
of $A^c$. This means that no $\epsilon$-neighborhood of $x$ lies entirely in $A$. So, in this
case too, $x \in A^{\circ c}$.

For the reverse inclusion suppose $x \in A^{\circ c}$.  Since $x$ is not in the interior of
$A$, no $\epsilon$-neighborhood of $x$ lies entirely in $A$.  Thus either $x$ itself fails to
be in $A$, in which case $x$ belongs to $A^c$ and therefore to $\clo{A^c}$, or else every
$\epsilon$-neighborhood of $x$ contains a point of $A^c$ different from~$x$.  In this latter
case also, $x$ belongs to the closure of~$A^c$.

\begin{rem}  It is interesting to observe that the proof of (a) can be accomplished by a single
string of ``iff'' statements.  That is, each step of the argument uses a reversible
implication.  One needs to be careful, however, with the negation of quantifiers (see
section~\ref{neg} of appendix~\ref{log_con}). It will be convenient to let
$J_\epsilon^\ast(x)$ denote the $\epsilon$-neighborhood of $x$ with $x$ deleted; that is,
$J_\epsilon^\ast(x) = (x - \epsilon, x) \cup (x, x + \epsilon)$. The proof goes like this:
 \begin{align*}
    x \in A^{\circ c}
        &\text{ iff } \sim(x \in A^\circ) \\
        &\text{ iff } \sim((\exists\epsilon > 0)\,J_\epsilon(x)
                           \subseteq A) \\
        &\text{ iff } (\forall\epsilon > 0)\, J_\epsilon(x)
                            \nsubseteq A \\
        &\text{ iff } (\forall\epsilon>0)\,J_\epsilon(x)
                            \cap A^c \ne \emptyset\\
        &\text{ iff } (\forall\epsilon > 0)\, (x \in A^c
              \text{ or } J_\epsilon^\ast(x) \cap A^c
                             \ne \emptyset)\\
        &\text{ iff } (x \in A^c \text{ or } (\forall\epsilon >
               0)\, J_\epsilon^\ast(x) \cap A^c \ne \emptyset)\\
        &\text{ iff } (x \in A^c \text{ or } x \in (A^c)') \\
        &\text{ iff } x \in \clo{A^c}.
 \end{align*}
Proofs of this sort are not universally loved.  Some people admire their precision and
efficiency.  Others feel that reading such a proof has all the charm of reading computer code.
\end{rem}

(b)\quad The easiest proof of (b) is produced by substituting $A^c$ for $A$ in part~(a).  Then
$\clo A = \clo{A^{cc}} = A^{c \circ c}$.  Take complements to get ${\clo A}^c = A^{c \circ}$.
\end{prf}

%END Chapter 02







\section{Exercises in chapter 03}


\begin{prf}\label{sol_cont_exam2}(Solution to~\ref{cont_exam2})
Let $a \in \R$. Given $\epsilon > 0$, choose $\delta = \epsilon/5$. If $\abs{x - a} < \delta$,
then $\abs{f(x) - f(a)} = 5\abs{x - a} < 5 \delta  = \epsilon$.
\end{prf}


\begin{prf}\label{sol_cont_exam3}(Solution to~\ref{cont_exam3})\nopagebreak
Given $\epsilon > 0$, choose $\delta = \min\{1, \epsilon/7\}$, the smaller of the numbers $1$
and $\epsilon/7$.  If $\abs{x - a} = \abs{x + 1} < \delta$, then $\abs{x} = \abs{x + 1 - 1}
\le \abs{x + 1} + 1 < \delta + 1 \le 2$. Therefore,
 \begin{align}   \abs{f(x) - f(a)}
              &= \abs{x^3 - (-1)^3}     \tag{i}  \\
              &= \abs{x^3 + 1}          \tag{ii}  \\
              &= \abs{x + 1}\, \abs{x^2 - x + 1}
                                            \tag{iii}  \\
              &\le \abs{x+ 1} (x^2 + \abs{x} + 1)
                                            \tag{iv}   \\
              &\le \abs{x + 1}\,(4 + 2 + 1)
                                            \tag{v}    \\
              &= 7\abs{x + 1}   \tag{vi}   \\
              &< 7\delta        \tag{vii}  \\
              &\le \epsilon.    \tag{viii}
 \end{align}
\end{prf}

 \begin{rem}  How did we know to choose $\delta = \min\{1, \epsilon/7\}$?  As scratch work
we do steps (i)--(iv), a purely algebraic process, and obtain
 \[ \abs{f(x) - f(a)} \le \abs{x+1}(x^2 + \abs{x} + 1)\,. \]
Now how do we guarantee that the quantity in parentheses doesn't get ``too large''.  The
answer is to require that $x$ be ``close to'' $a=-1$.  What do we mean by close?  Almost
\emph{anything} will work.  Here it was decided, arbitrarily, that $x$ should be no more than
$1$ unit from $-1$.  In other words, we wish $\delta$ to be no larger than $1$.  Then $\abs{x}
\le 2$ and consequently $x^2+\abs{x}+1 \le 7$; so we arrive at step (vi)
\[\abs{f(x) - f(a)} \le 7\abs{x+1}\,.\]
Since we assume that $\abs{x-(-1)} = \abs{x+1} < \delta$, we have~(vii)
 \[\abs{f(x) - f(a)} < 7\delta\,.\]
What we \emph{want} is $\abs{f(x) - f(a)} < \epsilon$.  This can be achieved by choosing
$\delta$ to be no greater than $\epsilon/7$. Notice that we have required two things of
$\delta$:
 \[\delta \le 1 \quad \text{ and } \quad \delta \le \epsilon/7\,.\]
The easiest way to arrange that $\delta$ be no larger than each of two numbers is to make it
the smaller of the two.  Thus our choice is $\delta = \min\{1, \epsilon/7\}$.

A good exercise is to repeat the preceding argument, but at (iv) require $x$ to be within $2$
units of $-1$ (rather than 1 unit as above).  This will change some things in the proof, but
should not create any difficulty.
 \end{rem}

\begin{prf}\label{sol_cont_exam4}(Solution to~\ref{cont_exam4})
Let $a \in \R$. Given $\epsilon > 0$, choose $\delta = \min\{1, (4\abs{a} +
2)^{-1}\epsilon\}$.  If $\abs{x - a} < \delta$, then
  \[  \abs{x} \le \abs{x - a} + \abs{a} < 1 + \abs{a}.\]
Therefore
 \begin{align*}  \abs{f(x) - f(a)}
                   &=  \abs{(2x^2 - 5) - (2a^2 - 5)}\\
                   &= 2\abs{x^2 - a^2}\\
                   &= 2\abs{x -  a}\, \abs{x + a} \\
                   &\le  2\abs{x - a}\,(\abs{x} + \abs{a}) \\
                   &\le  2\abs{x - a}\,(1 + \abs{a} + \abs{a}) \\
                   &\le  (4\abs{a} + 2)\abs{x - a} \\
                   &< (4\abs{a} + 2) \delta \\
                   &\le \epsilon.  \qedhere
 \end{align*}
\end{prf}


\begin{prf}\label{sol_cond3_cont}(Solution to~\ref{cond3_cont})
Suppose f is continuous. Let $V$ be an open subset of~$\R$. To show that $f^\gets(V)$ is open
it suffices to prove that each point of $f^\gets(V)$ is an interior point of that set. (Notice
that if $f^\gets(V)$ is empty, then there is nothing to prove. The null set is open.) If $a
\in f^\gets(V)$, then $V$ is a neighborhood of $f(a)$. Since $f$ is continuous at $a$, the set
$f^\gets(V)$ contains a neighborhood of $a$, from which we infer that $a$ is an interior point
of~$f^\gets(V)$.

Conversely, suppose that $\open{f^\gets(V)}{\R}$ whenever $\open V{\R}$.  To see that $f$ is
continuous at an arbitrary point $a$ in $\R$, notice that if $V$ is a neighborhood of $f(a)$,
then $a \in \open{f^\gets(V)}{\R}$. That is, $f^\gets(V)$ is a neighborhood of $a$. So $f$ is
continuous at~$a$.
\end{prf}




%END chapter 03











\section{Exercises in chapter 04}

\begin{prf}\label{sol_abs_seq}(Solution to~\ref{abs_seq})
Let $\epsilon > 0$.  Notice that $x_n \in J_\epsilon(0)$ if and only if $\abs{x_n} \in
J_\epsilon(0)$. Thus $(x_n)$ is eventually in $J_\epsilon(0)$ if and only if $(\abs{x_n})$ is
eventually in~$J_\epsilon(0)$.
\end{prf}

\begin{prf}\label{sol_conv_bdd}(Solution to~\ref{conv_bdd})
Let $x_n \sto a$ where $(x_n)$ is a sequence of real numbers. Then by problem
\ref{cond_conv_seq}(a) there exists $n_0 \in \N$ such that $n \ge n_0$ implies $\abs{x_n - a}
< 1$.  Then by \ref{prob_abs_val}(c)
   \[ \bigl\lvert \abs{x_n} - \abs a \bigr\lvert \le \abs{x_n - a} < 1 \]
for all $n \ge n_0$.  Thus, in particular,
   \[ \abs{x_n} < \abs a + 1 \]
for all $n \ge n_0$.  If $M = \max\{\abs{x_1},\abs{x_2},\dots,\abs{x_{n_0-1}}, \abs a+1\}$,
then $\abs{x_n} \le M$ for all $n \in \N$.
\end{prf}

\begin{prf}\label{sol_m_subseq}(Solution to~\ref{m_subseq})
Let $(a_n)$ be a sequence in~$\R$. As suggested in the hint, consider two cases.  First,
suppose that there is a subsequence $\left(a_{n_k}\right)$ consisting of peak terms. Then for
each~$k$,
   \[ a_{n_k} \ge a_{n_{k+1}}\,. \]
That is, the subsequence $\left(a_{n_k}\right)$ is decreasing.

Now consider the second possibility: there exists a term $a_p$ beyond which there are no peak
terms. Let $n_1 = p + 1$. Since $a_{n_1}$ is not a peak term, there exists $n_2 > n_1$ such
that $a_{n_2} > a_{n_1}$. Since $a_{n_2}$ is not a peak term, there exists $n_3 > n_2$ such
that $a_{n_3} > a_{n_2}$. Proceeding in this way we choose an increasing (in fact, strictly
increasing) subsequence $\left(a_{n_k}\right)$ of the sequence $(a_n)$.

In both of the preceding cases we have found a monotone subsequence of the original sequence.
\end{prf}

\begin{prf}\label{sol_sc_closure}(Solution to~\ref{sc_closure})
Suppose that $b \in \clo A$.  There are two possibilities; $b \in A$ or $b \in A'$.  If $b \in
A$, then the constant sequence $(b,b,b,\dots)$ is a sequence in $A$ which converges to $b$. On
the other hand, if $b \in A'$, then for every $n \in \N$ there is a point $a_n \in J_{1/n}(b)$
such that $a_n \in A$ and $a_n \ne b$.  Then $(a_n)$ is a sequence in $A$ which converges
to~$b$.

Conversely, suppose there exists a sequence $(a_n)$ in $A$ such that $a_n \sto b$.  Either
$a_n = b$ for some $n$ (in which case $b \in A$) or else $a_n$ is different from $b$ for all
$n$.  In the latter case every neighborhood of $b$ contains points of $A$---namely, the
$a_n$'s for $n$ sufficiently large---other than~$b$.  Thus in either case $b \in \clo A$. \qed
\end{prf}

\begin{prf}\label{sol_4exer1}(Solution to~\ref{4exer1})
If $\ell = \lim_{n \sto \infty}x_n$ exists, then taking limits as $n \sto \infty$ of both
sides of the expression $4x_{n+1} = {x_n}^3$ yields $ 4\ell = \ell^3$. That is,
\[\ell^3 - 4\ell = \ell(\ell - 2)(\ell + 2) = 0.\]
Thus if $\ell$ exists, it must be $-2$, $0$, or $2$.  Next notice that
 \begin{align*}
   x_{n+1} - x_n &= \frac14{x_n}^3 - x_n \\
                 &= \frac14x_n(x_n - 2)(x_n + 2).
 \end{align*}
Therefore
   \begin{equation}\label{4loc2}
                x_{n+1} > x_n \quad \text{ if } \quad x_n \in (-2,0) \cup (2,\infty)
   \end{equation}
and
   \begin{equation}\label{4loc3}
               x_{n+1} < x_n \quad \text{ if } \quad x_n \in (-\infty,-2) \cup (0,2).
   \end{equation}

Now consider the seven cases mentioned in the hint. Three of these are trivial: if $x_1 = -2$,
$0$, or $2$, then the resulting sequence is constant (therefore certainly convergent).

Next suppose $x_1 < -2$. Then $x_n < -2$ for every $n$. [The verification is an easy
induction: If $x_n < -2$, then ${x_n}^3 < -8$; so $x_{n+1} = \frac14 {x_n}^3 < -2$.]  From
this and \eqref{4loc3} we see that $x_{n+1} < x_n$ for every $n$.  That is, the sequence
$(x_n)$ decreases. Since the only possible limits are $-2$, $0$, and $2$, the sequence cannot
converge. (It must, in fact, be unbounded.)

The case $x_1 > 2$ is similar. We see easily that $x_n > 2$ for all $n$ and therefore [by
\eqref{4loc2}] the sequence $(x_n)$ is increasing. Thus it diverges (and is unbounded).

If $-2 < x_1 < 0$, then $-2 < x_n < 0$ for every $n$. [Again an easy inductive proof: If $-2 <
x_n < 0$, then $-8 < {x_n}^3 < 0$; so $-2 < \frac14{x_n}^3 = x_{n+1} < 0$.] From \eqref{4loc2}
we conclude that  $(x_n)$ is increasing. Being bounded above it must converge [see proposition
\ref{bdd_monotone}] to some real number~$\ell$.  The only available candidate is $\ell = 0$.

Similarly, if $0 < x_1 < 2$, then $0 < x_n < 2$ for all $n$ and $(x_n)$ is decreasing. Again
the limit is $\ell = 0$.

We have shown that the sequence $(x_n)$ converges if and only if $x_1 \in [-2,2]$. If $x_1 \in
(-2,2)$, then $\lim x_n = 0$; if $x_1 = -2$, then $\lim x_n = -2$; and if $x_1 = 2$, then
$\lim x_n = 2$.
\end{prf}





%END chapter 04














\section{Exercises in chapter 05}


\begin{prf}\label{sol_nasc_conn}(Solution to~\ref{prop_nasc_conn})
Suppose there exists a nonempty set $U$ which is properly contained in $A$ and which is both
open and closed in $A$. Then, clearly, the sets $U$ and $U^c$ (both open in $A$) disconnect
$A$. Conversely, suppose that $A$ is disconnected by sets $U$ and $V$ (both open in $A$). Then
the set $U$ is not the null set, is not equal to $A$ (because $V$, its complement with respect
to $A$, is nonempty), is open in $A$, and is closed in $A$ (because $V$ is open in~$A$).
\end{prf}

\begin{prf}\label{sol_contimg_conn}(Solution to~\ref{thm_contimg_conn})
Let $A$ be a subset of $\R$ and $f\colon A \sto \R$ be continuous.  Suppose that $\ran f$ is
disconnected. Then there exist disjoint nonempty sets $U$ and $V$ both open in $\ran f$ whose
union is~$\ran f$.  By \ref{cond3a_cont} the sets $f^\gets(U)$ and $f^\gets(V)$ are open
subsets of~$A$.  Clearly these two sets are nonempty, they are disjoint, and their union
is~$A$. This shows that $A$ is disconnected.
\end{prf}

\begin{prf}\label{sol_ivt2}(Solution to~\ref{thm_ivt2})
The continuous image of an interval is an interval (by theorem~\ref{thm_ivt1}).  Thus the
range of $f$ (that is, $f^\sto(J)$) is an interval.  So a point belongs to the range of $f$ if
it lies between two other points which belong to the range of~$f$. That is, if $a$, $b \in
\ran f$ and $z$ lies between $a$ and $b$, then there exists a point $x$ in the domain of $f$
such that $z = f(x)$.
\end{prf}

\begin{prf}\label{sol_ivt_eqn}(Solution to~\ref{exam_ivt_eqn})
Let
   \[ f(x) = x^{27} + 5x^{13} + x - x^3 - x^5 - \frac2{\sqrt{1+3x^2}} \]
for all $x \in \R$.  The function $f$ is defined on an interval (the real line), and it is
easy, using the results of chapter~\ref{cont_on_R}, to show that $f$ is continuous.  Notice
that $f(0) = -2$ and that $f(1) = 4$. Since $-2$ and $4$ are in the range of $f$ and $0$ lies
between $-2$ and $4$, we conclude from the \emph{intermediate value theorem} that $0$ is in
the range of $f$. In fact, there exists an $x$ such that $0 < x < 1$ and $f(x) = 0$.  Such a
number $x$ is a solution to~\eqref{ivt_soln_eqn2}. Notice that we have not only shown the
existence of a solution to~\eqref{ivt_soln_eqn2} but also located it between consecutive
integers.
\end{prf}

\begin{prf}\label{sol_ivt_fp}(Solution to~\ref{prop_ivt_fp})
Let $f \colon [a,b] \sto [a,b]$ be continuous. If $f(a) = a$ or $f(b) = b$, then the result is
obvious; so we suppose that $f(a) > a$ and $f(b) < b$. Define $g(x) = x - f(x)$. The function
$g$ is continuous on the interval $[a,b]$. (Verify the last assertion.)  Notice that $g(a) = a
- f(a) < 0$ and that $g(b) = b - f(b) > 0$. Since $g(a) < 0 < g(b)$, we may conclude from the
\emph{intermediate value theorem} that $0 \in \ran g$. That is, there exists $z$ in $(a,b)$
such that $g(z) = z - f(z) = 0$. Thus $z$ is a fixed point of $f$.
\end{prf}





%END chapter 05













\section{Exercises in chapter 06}

\begin{prf}\label{sol_cpt_cb}(Solution to~\ref{cpt_cb})
Let $A$ be a compact subset of $\R$. To show that $A$ is closed, prove that $A^c$ is open. Let
$y$ be a point of $A^c$. For each $x \in A$ choose numbers $r_x > 0$ and $s_x > 0$ such that
the open intervals $J_{r_x}(x)$ and $J_{s_x}(y)$ are disjoint. The family $\{J_{r_x}(x) \colon
x \in A\}$ is an open cover for $A$.  Since $A$ is compact there exist $x_1, \dots, x_n \in A$
such that $\{J_{r_{x_i}}(x_i) \colon 1 \le i \le n\}$ covers $A$. It is easy to see that if
    \[ t = \min\{s_{x_1}, \dots, s_{x_n}\} \]
then $J_t(y)$ is disjoint from $\bigcup_{i=1}^n J_{r_{x_i}}(x_i)$ and hence from~$A$.  This
shows that $y \in A^{c \circ}$. Since $y$ was arbitrary, $A^c$ is open.

To prove that $A$ is bounded, we need consider only the case where $A$ is nonempty. Let $a$ be
any point in $A$. Then $\{J_n(a) \colon n \in \N\}$ covers $A$ (it covers all of $\R$!). Since
$A$ is compact a finite subcollection of these open intervals will cover $A$. In this finite
collection there is a largest open interval; it contains~$A$.
\end{prf}

\begin{prf}\label{sol_cpt_cnd4}(Solution to~\ref{cpt_cnd4})
Let $\sfml V$ be a family of open subsets of $\R$ which covers $f^{\sto}(A)$. The family
   \[ \sfml U := \{f^{\gets}(V) \colon V \in \sfml V\} \]
is a family of open sets which covers $A$ (see \ref{cond3_cont}). Since $A$ is compact we
may choose sets $V_1, \dots, V_n \in \sfml V$ such that $\bigcup_{k=1}^n f^{\gets}(V_k)
\supseteq A$. We complete the proof by showing that $f^{\sto}(A)$ is covered by the
finite subfamily $\{V_1, \dots, V_n\}$ of $\sfml V$. If $y \in f^{\sto}(A)$, then $y =
f(x)$ for some $x \in A$. This element $x$ belongs to at least one set $f^{\gets}(V_k)$;
so (by proposition~\ref{prop_f_finv})
   \[ y = f(x) \in f^{\sto}(f^{\gets}(V_k)) \subseteq V_k \,. \]
Thus $f^{\sto}(A) \subseteq \bigcup_{k=1}^n V_k$.
\end{prf}

\begin{prf}\label{sol_evthm}(Solution to~\ref{evthm})
We prove that $f$ assumes a maximum on $A$.  (To see that $f$ has a minimum apply the present
result to the function $-f$.) By theorem~\ref{cpt_cnd4} the image of $f$ is a compact subset
of $\R$ and is therefore (see \ref{cpt_cb}) closed and bounded.  Since it is bounded the image
of $f$ has a least upper bound, say $l$. By example \ref{sup_in_clo} the number $l$ is in the
closure of~$\ran f$. Since the range of $f$ is closed, $l \in \ran f$. Thus there exists a
point $a$ in $A$ such that $f(a) = l \ge f(x)$ for all $x \in A$.
\end{prf}




%END chapter 06












\section{Exercises in chapter 07}

\begin{prf}\label{sol_rlim_uniq}(Solution to~\ref{rlim_uniq})
Argue by contradiction.  If $b \ne c$, then $\epsilon := \abs{b - c} > 0$.  Thus there exists
$\delta_1 > 0$ such that $\abs{f(x) - b} < \epsilon/2$ whenever $x \in A$ and $0 < \abs{x- a}
< \delta_1$ and there exists $\delta_2 > 0$ such that $\abs{f(x) - c} < \epsilon/2$ whenever
$x \in A$ and $0 < \abs{x - a} < \delta_2$.  Let $\delta = \min\{\delta_1,\delta_2\}$.  Since
$a \in A'$, the set $A \cap J_\delta(a)$ is nonempty.  Choose a point $x$ in this set.  Then
   \[ \epsilon = \abs{b - c} \le \abs{b - f(x)} + \abs{f(x) - c}
                             < \epsilon/2 + \epsilon/2 = \epsilon\,. \]
This is a contradiction.

It is worth noticing that the preceding proof cannot be made to work if $a$ is not required to
be an accumulation point of $A$. To obtain a contradiction we must know that the condition $0
< \abs{x - a} < \delta$ is satisfied for at least one $x$ in the domain of~$f$.
\end{prf}

\begin{prf}\label{sol_cnt_vs_lim}(Solution to~\ref{cnt_vs_lim})
Both halves of the proof make use of the fact that in $A$ the open interval about $a$ of
radius $\delta$ is just $J_\delta(a) \cap A$ where $J_\delta(a)$ denotes the corresponding
open interval in~$\R$.

Suppose $f$ is continuous at $a$.  Given $\epsilon > 0$ choose $\delta > 0$ so that $x \in
J_\delta(a) \cap A$ implies $f(x) \in J_\epsilon(f(a))$.  If $x \in A$ and $0 < \abs{x - a} <
\delta$, then $\abs{f(x) - f(a)} < \epsilon$. That is, $\lim_{x \sto a}f(x) = f(a)$.

Conversely, suppose that $\lim_{x \sto a}f(x) = f(a)$.  Given $\epsilon > 0$ choose $\delta >
0$ so that $\abs{f(x) - f(a)} < \epsilon$ whenever $x \in A$ and $0 < \abs{x - a} < \delta$.
If $x = a$, then $\abs{f(x) - f(a)} = 0 < \epsilon$.  Thus $x \in J_\delta(a) \cap A$ implies
$f(x) \in J_\epsilon(f(a))$.  This shows that $f$ is continuous at $a$.
\end{prf}

\begin{prf}\label{sol_trnsl_lim}(Solution to~\ref{trnsl_lim})
Let $g \colon h \mapsto f(a+h)$. Notice that $h \in \dom g$ if and only if $a + h \in \dom f$;
   \begin{equation}\label{7eqn1} \dom f = a + \dom g\,.
   \end{equation}
That is, $\dom f = \{a + h \colon  h \in \dom g\}$.

First we suppose that
   \begin{equation}\label{7eqn2} l := \lim_{h \sto 0}g(h) =
                                 \lim_{h \sto 0}f(a + h) \text{ exists.}
   \end{equation}
We show that $\lim_{x \sto a}f(x)$ exists and equals~$l$.  Given $\epsilon > 0$ there exists
(by \eqref{7eqn2}) a number $\delta > 0$ such that
   \begin{equation}\label{7eqn3} \abs{g(h) - l} < \epsilon
              \text{ whenever } h \in \dom g \text{ and } 0 < \abs{h} < \delta.
   \end{equation}
Now suppose that $x \in \dom f$ and $0 < \abs{x - a} < \delta$.  Then by \eqref{7eqn1}
   \[ x - a \in \dom g \]
and by \eqref{7eqn3}
   \[ \abs{f(x) - l} = \abs{g(x - a) - l} < \epsilon\,. \]
Thus given $\epsilon > 0$ we have found $\delta > 0$ such that $\abs{f(x) - l} < \epsilon$
whenever $x \in \dom f$ and $0 < \abs{x - a} < \delta$.  That is, $\lim_{x \sto a}f(x) = l$.

The converse argument is similar.  Suppose $l := \lim_{x \sto a}f(x)$ exists.  Given $\epsilon
> 0$ there exists $\delta > 0$ such that $\abs{f(x) - l} < \epsilon$ whenever $x \in \dom f$ and
$0 < \abs{x - a} < \delta$.  If $h \in \dom g$ and $0 < \abs{h} < \delta$, then $a + h \in
\dom f$ and $0 < \abs{(a + h) - a} < \delta$. Therefore $\abs{g(h) - l} = \abs{f(a + h) - l} <
\epsilon$, which shows that $\lim_{h \sto 0}\,g(h) = l$.
\end{prf}





%END chapter 07










\section{Exercises in chapter 08}

\begin{prf}\label{sol_lin_vs_o}(Solution to~\ref{lin_vs_o})
Suppose that $T \in \ofml L \cap \lobo o$.  Let $\epsilon > 0$.  Since $T \in \lobo o$,
there exists $\delta > 0$ so that $\abs{Ty} \le \epsilon \abs y$ whenever $\abs y <
\delta$.  Since $T \in \ofml L$ there exists $m \in \R$ such that $Tx = mx$ for all $x$
(see example~\ref{lin_R}).  Now, suppose $0 < \abs y < \delta$.  Then
   \[ \abs m\,\abs y = \abs{Ty} \le \epsilon y \]
so $\abs m \le \epsilon$.  Since $\epsilon$ was arbitrary, we conclude that $m = 0$.  That is,
$T$ is the constant function~$0$.
\end{prf}

\begin{prf}\label{sol_O_closed}(Solution to~\ref{O_closed})
Let $f$, $g \in \lobo O$. Then there exist positive numbers $M$, $N$, $\delta$, and $\eta$
such that $\abs{f(x)} \le M\abs{x}$  whenever $\abs x < \delta$ and $\abs{g(x)} < N\abs x$
whenever $\abs x < \eta$.  Then $\abs{f(x) + g(x)} \le (M + N)\abs x$ whenever $\abs x$ is
less than the smaller of $\delta$ and $\eta$.  So $f + g \in \lobo O$.

If $c$ is a constant, then $\abs{cf(x)} = \abs c\,\abs{f(x)} \le \abs c\, M\,\abs x$ whenever
$\abs x < \delta$; so $cf \in \lobo O$.
\end{prf}

\begin{prf}\label{sol_O_comp_o}(Solution to~\ref{O_comp_o})
(The domain of $f \circ g$ is taken to be the set of all numbers $x$ such that $g(x)$ belongs
to the domain of $f$; that is, $\dom (f\circ g) = g^\gets(\dom f)$.)  Since $f \in \lobo O$
there exist $M, \delta > 0$ such that $\abs{f(y)} \le M\abs y$ whenever $\abs y < \delta$. Let
$\epsilon > 0$.  Since $g \in \lobo o$ there exists $\eta > 0$ such that $\abs{g(x)} \le
\epsilon M^{-1} \abs x$ whenever $\abs x \le \eta$.

Now if $\abs{x}$ is less than the smaller of $\eta$ and $M \epsilon^{-1} \delta$, then
$\abs{g(x)} \le \epsilon M^{-1} \abs x < \delta$, so that
  \[ \abs{(f \circ g)(x)} \le M\abs{g(x)} \le \epsilon \abs x. \]
Thus $f \circ g \in \lobo o$.
\end{prf}

\begin{prf}\label{sol_mult_o}(Solution to~\ref{mult_o})
Since $\phi$ and $f$ belong to $\lobo O$, there exist positive numbers $M$, $N$, $\delta$, and
$\eta$ such that $\abs{\phi(x)} \le M\abs x$ whenever $\abs x < \delta$ and $\abs{f(x)} <
N\abs x$ whenever $\abs x < \eta$. Suppose $\epsilon > 0$. If $\abs x$ is less than the
smallest of $\epsilon M^{-1}N^{-1}$, $\delta$, and $\eta$, then
  \[ \abs{(\phi f)(x)} = \abs{\phi(x)}\,\abs{f(x)} \le MNx^2 \le \epsilon \abs{x}\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_8exer1}(Solution to~\ref{8exer1})
Clearly $f(0) - g(0) = 0$. Showing that $\D\lim_{x\sto 0}\frac{f(x) - g(x)}x = 0$ is a routine
computation:
 \begin{align*}
   \lim_{x \sto 0}\frac{f(x) - g(x)}x
           &= \lim_{x \sto 0} \frac1x\left(x^2 - 4x - 1 - \frac1{3x^2+4x-1}\right) \\
           &= \lim_{x \sto 0} \frac{3x^4-8x^3-20x^2}{x(3x^2+4x-1)} = 0.  \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_tan_er}(Solution to~\ref{tan_er})
Reflexivity is obvious. Symmetry: if $f \simeq g$, then $f - g \in \lobo o$; so $g - f =
(-1)(f - g) \in \lobo o$ (by proposition \ref{o_closed}).  Thus $g \simeq f$.  Transitivity:
if $f \simeq g$ and $g \simeq h$, then $g - f \in \lobo o$ and $h - g \in \lobo o$; so $h - f
= (h - g) + (g - f) \in \lobo o$ (again by \ref{o_closed}).  Thus $f \simeq h$.
\end{prf}

\begin{prf}\label{sol_tan_uniq}(Solution to~\ref{tan_uniq})
Since $S \simeq f$ and $T \simeq f$, we conclude from proposition \ref{tan_er} that $S
\simeq T$.  Then $S - T \in \ofml L \cap \lobo o$ and thus $S - T = 0$ by
proposition~\ref{lin_vs_o}.
\end{prf}

\begin{prf}\label{sol_del_sum}(Solution to~\ref{del_sum})
For each $h$ in the domain of $\Delta(f + g)_a$ we have
  \begin{align*}
         \Delta(f+g)_a(h) &= (f + g)(a + h) - (f + g)(a) \\
                          &= f(a + h) + g(a + h) - f(a) - g(a) \\
                          &= \Delta f_a(h) + \Delta g_a(h) \\
                          &= (\Delta f_a + \Delta g_a)(h).  \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_del_comp}(Solution to~\ref{del_comp})
For every $h$ in the domain of $\Delta(g \circ f)_a$ we have
  \begin{align*}
     \Delta(g \circ f)_a(h) &= g(f(a+h)) - g(f(a)) \\
                            &= g(f(a) + f(a+h) - f(a)) - g(f(a)) \\
                            &= g(f(a) + \Delta f_a(h)) - g(f(a)) \\
                            &= \bigl(\Delta g_{f(a)} \circ \Delta f_a\bigr)(h). \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_del_in_O}(Solution to~\ref{del_in_O})
If $f$ is differentiable at $a$, then
  \[ \Delta f_a = \bigl(\Delta f_a - df_a\bigr) + df_a \in \lobo o +
              \ofml L \subseteq \lobo O + \lobo O \subseteq \lobo O\, . \qedhere \]
\end{prf}

\begin{prf}\label{sol_diff_cont}(Solution to~\ref{diff_cont})
If $f \in \fml D_a$ , then $\Delta f_a \in \lobo O \subseteq \fml C_0$.  Since $\Delta f_a$ is
continuous at $0$, $f$ is continuous at~$a$.
\end{prf}

\begin{prf}\label{sol_diff_sm}(Solution to~\ref{diff_sm})
Since $f$ is differentiable at $a$, its differential exists and $\Delta f_a \simeq df_a$. Then
  \[ \Delta(\alpha f)_a = \alpha \Delta f_a \simeq \alpha df_a \]
by propositions~\ref{del_sm} and~\ref{tan_alg}.  Since $\alpha df_a$ is a linear function
which is tangent to $\Delta(\alpha f)_a$, we conclude that it must be the differential of
$\alpha f$ at $a$ (see proposition \ref{diff_uniq}).  That is, $\alpha df_a = d(\alpha f)_a$.
\end{prf}

\begin{prf}\label{sol_diff_mult}(Solution to~\ref{diff_mult})
It is easy to check that $\phi(a)df_a + f(a)d\phi_a$ is a linear function.  From $\Delta f_a
\simeq df_a$ we infer that $\phi(a)\Delta f_a \simeq \phi(a)df_a$ (by
proposition~\ref{tan_alg}), and from $\Delta\phi_a \simeq d\phi_a$ we infer that
$f(a)\Delta\phi_a \simeq f(a)d\phi_a$ (also by \ref{tan_alg}).  From
propositions~\ref{del_in_O} and~\ref{mult_o} we see that
  \[ \Delta\phi_a \cdot \Delta f_a \in \lobo O \cdot \lobo O \subseteq \lobo o\,; \]
that is, $\Delta\phi_a \cdot \Delta f_a \simeq 0$. Thus by propositions~\ref{del_mult}
and~\ref{tan_alg}
  \begin{align*}
     \Delta(\phi f)_a
            &= \phi(a)\Delta f_a + f(a)\Delta\phi_a + \Delta\phi_a \cdot \Delta f_a \\
            &\simeq \phi(a)df_a + f(a)d\phi_a + 0 \\
            &= \phi(a)df_a + f(a)d\phi_a.  \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_ch_rul}(Solution to~\ref{ch_rul})
By hypothesis $\Delta f_a \simeq df_a$ and $\Delta g_{f(a)} \simeq dg_{f(a)}$.  By
proposition~\ref{del_in_O} $\Delta f_a \in \lobo O$.  Then by proposition~\ref{tan_O}
  \begin{equation}\label{8eq1} \Delta g_{f(a)} \circ \Delta f_a
                               \simeq dg_{f(a)} \circ \Delta f_a\,;
  \end{equation}
and by proposition \ref{L_tan}
  \begin{equation}\label{8eq2} dg_{f(a)} \circ \Delta f_a \simeq dg_{f(a)} \circ df_a\,.
  \end{equation}
According to proposition \ref{del_comp}
  \begin{equation}\label{8eq3} \Delta(g \circ f)_a = \Delta g_{f(a)} \circ \Delta f_a\,.
  \end{equation}
From \eqref{8eq1}, \eqref{8eq2} , and \eqref{8eq3}, and proposition~\ref{tan_er} we conclude
that
   \[ \Delta(g \circ f)_a \simeq dg_{f(a)} \circ df_a\,. \]
Since $dg_{f(a)} \circ df_a$ is a linear function, the desired conclusion is an immediate
consequence of proposition~\ref{diff_uniq}.
\end{prf}






%END chapter 08










\section{Exercises in chapter 09}

\begin{prf}\label{sol_met_pos}(Solution to~\ref{met_pos})
If $x$, $y \in M$, then
  \[ 0 = d(x,x) \le d(x,y) + d(y,x) = 2\,d(x,y)\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_3equiv}(Solution to~\ref{3equiv})
It is clear that $\max\{a,b\} \le a + b$.  Expanding the right side of $0 \le (a \pm b)^2$ we
see that $2\abs{ab}\le a^2 + b^2$. Thus
 \begin{align*}
          (a+b)^2 &\le a^2 + 2|ab| + b^2 \\
                  &\le 2(a^2 + b^2)\,.
 \end{align*}
Taking square roots we get $a + b \le \sqrt2\,\sqrt{a^2 + b^2}$. Finally,
 \begin{align*}
       \sqrt2\,\sqrt{a^2+b^2} &\le \sqrt2\,\sqrt{2(\max\{a,b\})^2} \\
                              &= 2\max\{a,b\}\,.
 \end{align*}
We have established the claim made in the hint. Now if $x = (x_1,x_2)$ and $y = (y_1,y_2)$ are
points in $\R^2$, then
 \begin{align*}
       \max\{\abs{x_1-y_1},\abs{x_2-y_2}\}
             &\le \abs{x_1-y_1} + \abs{x_2-y_2} \\
             &\le \sqrt2\,\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2} \\
             &\le 2\max\{\abs{x_1-y_1}, \abs{x_2-y_2}\}\,.
 \end{align*}
In other words
   \[ d_u(x,y) \le d_1(x,y) \le \sqrt2\,d(x,y) \le 2\,d_u(x,y)\,. \qedhere \]
\end{prf}





%END chapter 09



















\section{Exercises in chapter 10}

\begin{prf}\label{sol_icb1ex}(Solution to~\ref{icb1ex})
The three open balls are $B_1(-1) = \{-1\}$, $B_1(0) = [0,1)$, and $B_2(0) = \{-1\} \cup [0,2)$.
\end{prf}

\begin{prf}\label{sol_icb2ex}(Solution to~\ref{icb2ex})
(a)\quad $\intr A = \emptyset$; $A' = \{0\}$; $\clo A = \partial A = \{0\} \cup A$.

(b)\quad $\intr A = \emptyset$; $A' = \clo A = \partial A = [0,\infty)$.

(c)\quad $\intr A = A$; $A' = \clo A = A \cup \{0\}$; $\partial A = \{0\}$.
\end{prf}

\begin{prf}\label{sol_sm_in_lar}(Solution to~\ref{sm_in_lar})
Let $t = r - d(a,c)$. (Note that $t > 0$.)  If $x \in B_t(c)$, then $d(a,x) \le d(a,c) +
d(c,x) < (r - t) + t = r$; so $x \in B_r(a)$.  (Easier solution: This is just a special case
of proposition~\ref{ball_int}. Take $b = a$ and $s = r$ there.)
\end{prf}

\begin{prf}\label{sol_prop_int}(Solution to~\ref{prop_int})
(a)\quad If $x \in \intr A$, then there exists $r > 0$ such that $B_r(x) \subseteq A \subseteq
B$. So $x \in \intr B$.

(b)\quad Since $\intr A \subseteq A$ we may conclude from (a) that $A^{\circ\circ} \subseteq
\intr A$. For the reverse inclusion take $a \in \intr A$. Then there exists $r > 0$ such that
$B_r(a) \subseteq A$. By lemma \ref{sm_in_lar}, about each point $b$ in the open ball $B_r(a)$
we can find an open ball $B_s(b)$ contained in $B_r(a)$ and hence in $A$. This shows that
$B_r(a) \subseteq \intr A$. Since some open ball about $a$ lies inside $\intr A$, we conclude
that $a \in A^{\circ\circ}$.
\end{prf}

\begin{prf}\label{sol_un_int}(Solution to~\ref{un_int})
(a)\quad Since $A \subseteq \bigcup \sfml A$ for every $A \in \sfml A$, we can conclude
from proposition \ref{prop_int}(a) that $\intr A \subseteq \intr{\bigl(\bigcup \sfml
A\bigr)}$ for every $A \in \sfml A$.  Thus $\bigcup\{\intr A \colon A \in \sfml A\}
\subseteq \intr{\bigl(\bigcup \sfml A\bigr)}$.

(b)\quad Let $\R$ be the metric space and $\sfml A = \{A,B\}$ where $A = \Q$ and $B =
\Q^c$. Then $\intr A \cup \intr B = \emptyset$ while $\intr{(A \cup B)} = \R$.
\end{prf}




%END chapter 10










\section{Exercises in chapter 11}

\begin{prf}\label{sol_in_lar}(Solution to~\ref{in_lar})
Since $\intr A$ is an open set contained in $A$ (by \ref{prop_int}(b)), it is contained in the
union of all such sets. That is,
  \[ \intr A \subseteq \textstyle\bigcup\{\,U \colon U \subseteq A
                                              \text{ and $U$ is open}\}.\]
On the other hand, if $U$ is an open subset of $A$, then (by \ref{prop_int}(a)) $U = \intr U
\subseteq \intr A$. Thus
   \[ \textstyle\bigcup\{U \colon U \subseteq A \text{ and $U$ is open}\}
               \subseteq \intr A\,.  \qedhere\]
\end{prf}

\begin{prf}\label{sol_op_vs_cl}(Solution to~\ref{op_vs_cl})
Let $A$ be a subset of a metric space.  Using problem \ref{int_vs_cl} we see that
 \begin{align*}
   A \text{ is open }
          &\text{ if{f} } A = \intr A \\
          &\text{ if{f} } A^c = \bigl(\intr A \bigr)^c = \clo{A^c} \\
          &\text{ if{f} $A^c$ is closed.}        \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_cnd_dns}(Solution to~\ref{cnd_dns})
Suppose that $D$ is dense in $M$. Argue by contradiction. If there is an open ball $B$ which
contains no point of $D$ (that is, $B \subseteq D^c$), then
  \[ B = \intr B \subseteq \intr{\bigl(D^c\bigr)}
              = \bigl(\,\clo D \,\bigr)^c = M^c = \emptyset\,, \]
which is not possible.

Conversely, suppose that $D$ is not dense in $M$. Then $\clo D$ is a proper closed subset of
$M$, making $\bigl(\, \clo D \,\bigr)^c$ a nonempty open set.  Choose an open ball $B
\subseteq \bigl(\, \clo D \,\bigr)^c$.  Since $\bigl(\,\clo D \,\bigr)^c \subseteq D^c$, the
ball $B$ contains no point of~$D$.
\end{prf}

\begin{prf}\label{sol_mtr_vs_top}(Solution to~\ref{mtr_vs_top})
It is enough to show that $\sfml T_1 \subseteq \sfml T_2$. Thus we suppose that $U$ is an
open subset of $(M,d_1)$ and prove that it is an open subset of $(M,d_2)$. For $a \in M$,
$r > 0$, and $k=1,2$ let $B_r^k(a)$ be the open ball about $a$ of radius $r$ in the space
$(M,d_k)$; that is,
  \[ B_r^k(a) = \{x \in M: d_k(x,a) < r\}\,. \]
To show that $U$ is an open subset of $(M,d_2)$ choose an arbitrary point $x$ in $U$ and find
an open ball $B_r^2(x)$ about $x$ contained in $U$.  Since $U$ is assumed to be open in
$(M,d_1)$ there exists $s>0$ such that $B_s^1(x) \subseteq U$. The metrics $d_1$ and $d_2$ are
equivalent, so, in particular, there is a constant $\alpha > 0$ such that $d_1(u,v) \le
\alpha\,d_2(u,v)$ for all $u,v \in M$. Let $r = s\,\alpha^{-1}$. Then if $y \in B_r^2(x)$, we
see that
  \[ d_1(y,x) \le \alpha\,d_2(y,x) < \alpha\,r = s\,. \]
Thus $B_r^2(x) \subseteq B_s^1(x) \subseteq U$.
\end{prf}




%END chapter 11










\section{Exercises in chapter 12}

\begin{prf}\label{sol_scm_closed}(Solution to~\ref{scm_closed})
Suppose that $A$ is closed in $M$. Let $(a_n)$ be a sequence in $A$ which converges to a point
$b$ in $M$.  If $b$ is in $A^c$ then, since $A^c$ is a neighborhood of $b$, the sequence
$(a_n)$ is eventually in $A^c$, which is not possible. Therefore $b \in A$.

Conversely, if $A$ is not closed, there exists an accumulation point $b$ of $A$ which does not
belong to $A$. Then for every $n \in \N$ we may choose a point $a_n$ in $B_{1/n}(b) \cap A$.
The sequence $(a_n)$ lies in $A$ and converges to $b$; but $b \notin A$.
\end{prf}

\begin{prf}\label{sol_3equiv_ms}(Solution to~\ref{3equiv_ms})
As in \ref{3prods}, let $\rho_1$ be the metric on $M_1$ and $\rho_2$ be the metric on $M_2$.
Use the inequalities given in the hint to \ref{3equiv} with $a = \rho_1(x_1,y_1)$ and $b =
\rho_2(x_2,y_2)$ to obtain
   \[ d_u(x,y) \le d_1(x,y) \le \sqrt2\,d(x,y) \le 2\,d_u(x,y)\,.  \qedhere \]
\end{prf}





%END chapter 12











\section{Exercises in chapter 13}

\begin{prf}\label{sol_uc_exer1}(Solution to~\ref{uc_exer1})
Let $C$ be the set of all functions defined on $[0,1]$ such that $0 < g(x) < 2$ for all $x \in
[0,1]$.  It is clear that $B_1(f) \subseteq C$. The reverse inclusion, however, is not
correct. For example, let
   \[g(x) = \begin{cases} 1, &\text{ if $x = 0$} \\
                          x, &\text{ if $0 < x \le 1$}.
            \end{cases}\]
Then $g$ belongs to $C$; but it does \emph{not} belong to $B_1(f)$ since
  \[ d_u(f,g) = \sup\{\abs{f(x) - g(x)}: 0 \le x \le 1\} = 1\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_uc_exer2}(Solution to~\ref{uc_exer2})
Let $f$, $g$, $h \in \fml B(S)$. There exist positive constants $M$, $N$, and $P$ such that
$\abs{f(x)}\le M$, $\abs{g(x)}\le N$, and $\abs{h(x)}\le P$ for all $x$ in $S$.

First show that $d_u$ is real valued. (That is, show that $d_u$ is never infinite.) This is
easy:
 \begin{align*}
     d_u(f,g) &= \sup\{\abs{f(x) - g(x)} \colon x \in S\} \\
              &\le \sup\{\abs{f(x)} + \abs{g(x)} \colon x \in S\} \\
              &\le M + N.
 \end{align*}
Now verify conditions (1)--(3) of the definition of ``metric'' in~\ref{def_met}. Condition~(1)
follows from the observation that
  \[ \abs{f(x) - g(x)} = \abs{g(x) - f(x)} \qquad\text{for all $x \in S$.} \]
To establish condition (2) notice that for every $x \in S$
 \begin{align*}
    \abs{f(x) - h(x)} &\le \abs{f(x) - g(x)} + \abs{g(x) - h(x)} \\
                      &\le d_u(f,g) + d_u(g,h);
 \end{align*}
and therefore
  \[ d_u(f,h) = \sup\{\abs{f(x) - h(x)} \colon x \in S\} \le d_u(f,g) + d_u(g,h)\,. \]
Finally, condition (3) holds since
 \begin{align*}
   d_u(f,g) = 0
       &\text{ iff } \abs{f(x) - g(x)}=0 \qquad\text{for all $x \in S$} \\
       &\text{ iff } f(x) = g(x) \qquad\text{for all $x \in S$} \\
       &\text{ iff } f = g.   \qedhere
\end{align*}
\end{prf}

\begin{prf}\label{sol_uc_vs_pwc}(Solution to~\ref{uc_vs_pwc})
Let $(f_n)$ be a sequence of functions in $\fml F(S,\R)$ and suppose that $f_n \sto g
\text{\,(unif)}$ in~$\fml F(S,\R)$.  Then for every $x \in S$
  \[ \abs{f_n(x) - g(x)} \le \sup\{\abs{f_n(y) - g(y)} \colon y \in S\}
               \sto 0  \qquad\text{as $n \sto \infty$}.\]
Thus $f_n \sto g \text{\,(ptws)}$.

On the other hand, if we define for each $n \in \N$ a function $f_n$ on $\R$ by
  \[ f_n(x) = \begin{cases} 1, &\text{if $x \ge n$} \\
                            0, &\text{if $x < n$},
              \end{cases} \]
then it is easy to see that the sequence $(f_n)$ converges pointwise to the zero function $\vc
0$; but since $d_u(f_n,\vc 0) = 1$ for every $n$, the sequence does not converge uniformly
to~$\vc 0$.
\end{prf}

\begin{prf}\label{sol_unif_lim_bdd}(Solution to~\ref{unif_lim_bdd})
(a)\quad Since $f_n \sto g \text{\,(unif)}$, there exists $m \in \N$ such that
  \[ \abs{f_n(x) - g(x)} \le \sup\{\abs{f_n(y) - g(y)} \colon y \in S\} < 1 \]
whenever $n \ge m$ and $x \in S$. Thus, in particular,
  \[ \abs{g(x)} \le \abs{f_m(x) - g(x)} + \abs{f_m(x)} < 1 + K \]
where $K$ is a number satisfying $\abs{f_m(x)} \le K$ for all $x \in S$.

(b)\quad Let
  \[ f_n(x) = \begin{cases} x, &\text{if $\abs{x} \le n$} \\
                            0, &\text{if $\abs{x} > n$}
              \end{cases} \]
and $g(x) = x$ for all $x$ in~$\R$. Then $f_n \sto g \text{\,(ptws)}$, each $f_n$ is bounded,
but $g$ is not.
\end{prf}

\begin{prf}\label{sol_uc_exer3}(Solution to~\ref{uc_exer3})
If $0 \le x < 1$, then
  \[ f_n(x) = x^n - x^{2n} \sto 0 \qquad\text{as $n \sto \infty$}\,. \]
This and the obvious fact that $f_n(1) = 0$ for every $n$ tell us that
  \[ f_n \sto \vc 0 \text{\,(ptws)}\,. \]
Observe that $x^{2n} \le x^n$ whenever $0 \le x \le 1$ and $n \in \N$. Thus $f_n \ge 0$ for
each $n$. Use techniques from beginning calculus to find the maximum value of each $f_n$.
Differentiating we see that
  \[ f_n'(x) = n\,x^{n-1} - 2n\,x^{2n-1} = n\,x^{n-1}(1 - 2x^n) \]
for $n > 1$. Thus the function $f_n$ has a critical point, in fact assumes a maximum, when $1
- 2x^n = 0$; that is, when $x = 2^{-1/n}$. But then
   \[ \sup\{\abs{f_n(x)-0} \colon 0 \le x \le 1\} = f_n(2^{-1/n})
                         = \tfrac12 - \tfrac14 = \tfrac14 \]
for all $n > 1$.  Since $\sup\{\abs{f_n(x) - 0} \colon 0 \le x \le 1\} \nrightarrow 0$ as $n
\sto \infty$, the convergence is not uniform.

Is it possible that the sequence $f_n$ converges uniformly to some function $g$ other than the
zero function? The answer is \emph{no}. According to proposition~\ref{uc_vs_pwc} if $f_n \sto
g \ne 0 \text{\,(unif)}$, then $f_n \sto g \text{\,(ptws)}$.  But this contradicts what we
have already shown, namely, $f_n \sto 0 \text{\,(ptws)}$.

Note: Since each of the functions $f_n$ belongs to $\fml B([0,1])$, it is permissible, and
probably desirable, in the preceding proof to replace each occurrence of the rather cumbersome
expression $\sup\{\abs{f_n(x) - 0} \colon 0 \le x \le 1\}$ by $d_u(f_n,\vc 0)$.
\end{prf}





%END chapter 13



















\section{Exercises in chapter 14}

\begin{prf}\label{sol_top_ch_cont}(Solution to~\ref{top_ch_cont})
Suppose $f$ is continuous.  Let $\open U{M_2}$.  To show that $f^\gets(U)$ is an open subset
of $M_1$, it suffices to prove that each point of $f^\gets(U)$ is an interior point of
$f^\gets(U)$.  If $a \in f^\gets(U)$, then $f(a) \in U$. Since $f$ is continuous at $a$, the
set $U$, which is a neighborhood of $f(a)$, must contain the image under $f$ of a neighborhood
$V$ of~$a$.  But then
   \[ a \in V \subseteq f^\gets(f^{\sto}(V)) \subseteq f^\gets(U) \]
which shows that $a$ lies in the interior of $f^\gets(U)$.

Conversely, suppose that $\open{f^\gets(U)}{M_1}$ whenever $\open U{M_2}$.  To see that $f$ is
continuous at an arbitrary point $a$ in $M_1$, notice that if $V$ is a neighborhood of $f(a)$,
then $a \in \open{f^\gets(V)}{M_1}$. Thus $f^\gets(V)$ is a neighborhood of $a$ whose image
$f^{\sto}(f^\gets(V))$ is contained in $V$.  Thus $f$ is continuous at $a$.
\end{prf}

\begin{prf}\label{sol_cont_mult}(Solution to~\ref{cont_mult})
Show that $M$ is continuous at an arbitrary point $(a,b)$ in $\R^2$.  Since the metric
$d_1$ (defined in \ref{taxicab}) is equivalent to the usual metric on $\R^2$,
proposition~\ref{cont_eqmtr} assures us that it is enough to establish continuity of the
function $M$ with respect to the metric $d_1$.  Let $K = \abs{a} + \abs{b} + 1$.  Given
$\epsilon > 0$, choose $\delta = \min\{\epsilon/K,1\}$.  If $(x,y)$ is a point in $\R^2$
such that $\abs{x - a} + \abs{y - b} = d_1((x,y), (a,b)) < \delta$, then
   \[\abs{x} \le \abs{a} + \abs{x - a} < \abs{a} + \delta \le
                \abs{a} + 1 \le K.\]
Thus for all such points $(x,y)$
 \begin{align*}   \abs{M(x, y) - M(a,b)}
                &= \abs{xy - xb + xb - ab} \\
                &\le \abs{x}\,\abs{y-b} + \abs{x-a}\,\abs{b} \\
                &\le K\abs{y-b} + K\abs{x-a} \\
                &< K\delta \\
                &\le \epsilon.    \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_seq_ch_cont}(Solution to~\ref{seq_ch_cont})
Suppose that $f$ is continuous at $a$.  Let $x_n \sto a$ and $B_2$ be a neighborhood of
$f(a)$.  There exists a neighborhood $B_1$ of $a$ such that $f^{\sto}(B_1) \subseteq B2$.
Choose $n_0 \in \N$ so that $x_n \in B_1$ whenever $n \ge n_0$. Then $f(x_n) \in f^{\sto}(B_1)
\subseteq B_2$ whenever $n \ge n_0$.  That is, the sequence $\bigl(f(x_n)\bigr)$ is eventually
in the neighborhood $B_2$. Since $B_2$ was arbitrary, $f(x_n) \sto f(a)$.

Conversely, suppose that $f$ is not continuous at $a$.  Then there exists $\epsilon > 0$ such
that the image under $f$ of $B_\delta(a)$ contains points which do not belong to
$B_\epsilon(f(a))$, no matter how small $\delta > 0$ is chosen. Thus for every $n \in \N$
there exists $x_n \in B_{1/n}(a)$ such that $f(x_n) \notin B_\epsilon(f(a))$.  Then clearly
$x_n \sto a$ but $f(x_n) \nrightarrow f(a)$.
\end{prf}

\begin{prf}\label{sol_coord_projs}(Solution to~\ref{coord_projs})
Let $(a,b)$ be an arbitrary point in $M_1 \times M_2$. If $(x_n,y_n) \sto (a,b)$ in $M_1
\times M_2$, then proposition \ref{seq_prod} tells us that $\pi_1(x_n,y_n) = x_n \sto a =
\pi_1(a,b)$.  This shows that $\pi_1$ is continuous at $(a,b)$. Similarly, $\pi_2$ is
continuous at~$(a,b)$.
\end{prf}

\begin{prf}\label{sol_components}(Solution to~\ref{components})
Suppose $f$ is continuous. Then the components $f^1 = \pi_1 \circ f$ and $f^2 = \pi_2 \circ
f$, being composites of continuous functions, are continuous. Conversely, suppose that $f^1$
and $f^2$ are continuous.  Let $a$ be an arbitrary point in $N$.  If $x_n \sto a$, then (by
proposition \ref{seq_ch_cont}) $f^1(x_n) \sto f^1(a)$ and $f^2(x_n) \sto f^2(a)$. From
proposition \ref{seq_prod} we conclude that
 \[ f(x_n) = (f^1(x_n),f^2(x_n)) \sto (f^1(a),f^2(a)) = f(a)\,; \]
so $f$ is continuous at~$a$.
\end{prf}

\begin{prf}\label{sol_prod_cont}(Solution to~\ref{prod_cont})
(a)\quad  The function $fg$ is the composite of continuous functions (that is, $fg = M \circ
(f,g)$); so it is continuous by corollary~\ref{cmp_cnt}.

(b)\quad This is just a special case of (a) where $f$ is the constant function whose value is
$\alpha$.
\end{prf}

\begin{prf}\label{sol_jc_sc}(Solution to~\ref{jc_sc})
For each $a \in M_1$ let $j_a\colon M_2 \sto M_1 \times M_2$ be defined by $j_a(y) = (a,y)$.
Then $j_a$ is continuous.  (Proof: If $y_n \sto c$, then $j_a(y_n) = (a,y_n) \sto (a,c) =
j_a(c)$.)  Since $f(a,\,\cdot\,) = f \circ j_a$, it too is continuous.  The continuity of each
$f(\,\cdot\,b)$ is established in a similar manner.
\end{prf}

\begin{prf}\label{sol_unif_lim_cont}(Solution to~\ref{unif_lim_cont})
Show that $g$ is continuous at an arbitrary point $a$ in $M$.  Let $\epsilon > 0$.  Since $f_n
\sto g \text{\,(unif)}$ there exists $n \in \N$ such that
 \[ \abs{f_m(x) - g(x)} \le \sup\{\abs{f_n(y) - g(y)}\colon  y \in M\} < \epsilon/3 \]
whenever $m \ge n$ and $x \in M$.  Since $f_n$ is continuous at $a$, there exists $\delta > 0$
such that
 \[ \abs{f_n(x) - f_n(a)} < \epsilon/3 \]
whenever $d(x,a) < \delta$.  Thus for  $x \in B_\delta(a)$
 \begin{align*}
     \abs{g(x) - g(a)}
           &\le \abs{g(x) - f_n(x)} + \abs{f_n(x) - f_n(a)}
                + \abs{f_n(a) - g(a)} \\
           &< \tfrac\epsilon3 + \tfrac\epsilon3 +
                \tfrac\epsilon3 = \epsilon\,.
 \end{align*}
This shows that $g$ is continuous at~$a$.
\end{prf}

\begin{prf}\label{sol_mslim_uniq}(Solution to~\ref{mslim_uniq})
Argue by contradiction.  If $b \ne c$, then $\epsilon = d(b,c) > 0$.  Thus there exists
$\delta_1 > 0$ such that $d(f(x),b) < \epsilon/2$ whenever $x \in A$ and $0 < d(x,a) <
\delta_1$, and there exists $\delta_2 > 0$ such that $d(f(x),c) < \epsilon/2$ whenever $x \in
A$ and $0 < d(x,a) < \delta_2$.  Let $\delta = \min\{\delta_1,\delta_2\}$. Since $a \in A'$,
the set $A \cap B_\delta(a)$ is nonempty. Choose a point $x$ in this set.  Then
   \[ \epsilon = d(b,c) \le d(b,f(x)) + d(f(x),c) <
            \tfrac\epsilon2 + \tfrac\epsilon2 = \epsilon\,. \]
This is a contradiction.
\end{prf}

It is worth noticing that the preceding proof cannot be made to work if $a$ is not required to
be an accumulation point of~$A$.  To obtain a contradiction we must know that the condition $0
< d(x,a) < \delta$ is satisfied for at least one $x$ in the domain of~$f$.

\begin{prf}\label{sol_dbl_vs_iter}(Solution to~\ref{dbl_vs_iter})
Let $g(x) = \lim_{y \sto b}f(x,y)$ for all $x \in \R$. It is enough to show that $\lim_{x \sto
a}\bigl(\lim_{y \sto b}f(x,y)\bigr) = \lim_{x \sto a}g(x) = l$. Given $\epsilon > 0$, choose
$\delta > 0$ so that $\abs{f(x,y) - l} < \epsilon/2$ whenever $0 < d\bigl((x,y),(a,b)\bigr) <
\delta$. (This is possible because $l = \lim_{(x,y) \sto (a,b)}f(x,y)$.) Suppose that $0 <
\abs{x-a} < \delta/\sqrt2$. Then (by the definition of $g$) there exists $\eta_x > 0$ such
that $\abs{g(x) - f(x,y)} < \epsilon/2$ whenever $0 < \abs{y -b} < \eta_x$.  Now choose any
$y$ such that $0 < \abs{y -b} < \min\{\delta/\sqrt2,\eta_x\}$.  Then (still supposing that $0
< \abs{x -a} < \delta/\sqrt2$) we see that
 \begin{align*}
                   0 &< d\bigl((x,y),(a,b)\bigr)  \\
                     &= \bigl((x-a)^2 + (y-b)^2\bigr)^{1/2} \\
                     &< \bigl((\delta^2/2 +
                                 \delta^2/2\bigr)^{1/2} \\
                     &= \delta
 \end{align*}
so
 \[\abs{f(x,y) - l} < \epsilon/2\]
and therefore
 \begin{align*}
      \abs{g(x) - l}
            &\le \abs{g(x) - f(x,y)} + \abs{f(x,y) - l}  \\
            &< \tfrac\epsilon2 + \tfrac\epsilon2 = \epsilon.
 \end{align*}
That is, $\lim_{x \sto a}g(x) = l$.
\end{prf}

\begin{prf}\label{sol_lim_nex}(Solution to~\ref{lim_nex})
By the remarks preceding this exercise, we need only find two distinct values which the
function $f$ assumes in every neighborhood of the origin. This is easy. Every neighborhood of
the origin contains points $(x,0)$ distinct from $(0,0)$ on the $x$-axis.  At every such point
$f(x,y) = f(x,0) = 0$.  Also, every neighborhood of the origin contains points $(x,x)$
distinct from $(0,0)$ which lie on the line $y = x$.  At each such point $f(x,y) = f(x, x) =
1/17$. Thus $f$ has no limit at the origin since in every neighborhood of $(0,0)$ it assumes
both the values $0$ and $1/17$.  (Notice, incidentally, that both iterated limits, $\lim_{x
\sto 0}\bigl(\lim_{y \sto 0}f(x, y)\bigr)$ and $\lim_{y \sto 0}\bigl(\lim_{x \sto 0}f(x,
y)\bigr)$ exist and equal~$0$.)
\end{prf}




%END chapter 14











\section{Exercises in chapter 15}

\begin{prf}\label{sol_mtc}(Solution to~\ref{mtc})
Suppose that $A$ is compact. Let $\sfml V$ be a family of open subsets of $M$ which
covers~$A$. Then
   \[ \sfml U := \{V \cap A \colon  V \in \sfml V\} \]
is a family of open subsets of $A$ which covers~$A$.  Since $A$ is compact we may choose
a subfamily $\{U_1, \dots ,U_n\} \subseteq \sfml U$ which covers $A$. For $1 \le k \le n$
choose $V_k \in \sfml V$ so that $U_k = V_k \cap A$. Then $\{V_1, \dots, V_n\}$ is a
finite subfamily of $\sfml V$ which covers~$A$.

Conversely, Suppose every cover of $A$ by open subsets of $M$ has a finite subcover. Let
$\sfml U$ be a family of open subsets of $A$ which covers $A$.  According to
proposition~\ref{open_in_subsp} there is for each $U$ in $\sfml U$ a set $V_U$ open in
$M$ such that $U = V_U \cap A$. Let $\sfml V$ be $\{V_U \colon U \in \sfml U\}$.  This is
a cover for $A$ by open subsets of~$M$. By hypothesis there is a finite subfamily $\{V_1,
\dots, V_n\}$ of $\sfml V$ which covers $A$.  For $1 \le k \le n$ let $U_k = V_k \cap A$.
Then $\{U_1, \dots, U_n\}$ is a finite subfamily of $\sfml U$ which covers $A$.  Thus $A$
is compact.
\end{prf}




%END chapter 15











\section{Exercises in chapter 16}

\begin{prf}\label{sol_scpt_tbdd}(Solution to~\ref{scpt_tbdd})
If a metric space $M$ is not totally bounded then there exists a positive number $\epsilon$
such that for every finite subset $F$ of $M$ there is a point $a$ in $M$ such that $F \cap
B_{\epsilon}(a) = \emptyset$.  Starting with an arbitrary point $x_1$ in $M$, construct a
sequence $(x_k)$ inductively as follows: having chosen $x_1, \dots, x_n$ no two of which are
closer together than $\epsilon$, choose $x_{n+1} \in M$ so that
  \[ B_{\epsilon}(x_{n+1}) \cap \{x_1, \dots, x_n\} = \emptyset\,. \]
Since no two terms of the resulting sequence are closer together than $\epsilon$, it has no
convergent subsequence.
\end{prf}

\begin{prf}\label{sol_cpt_scpt}(Solution to~\ref{cpt_scpt})
(1) $\Rightarrow$ (2): Suppose that there exists an infinite subset $A$ of $M$ which has no
accumulation point in~$M$. Then $\clo A = A$, so that $A$ is closed. Each point $a \in A$ has
a neighborhood $B_a$ which contains no point of $A$ other than $a$. Thus $\{B_a \colon a \in
A\}$ is a cover for $A$ by open subsets of $M$ no finite subfamily of which covers~$A$.  This
shows that $A$ is not compact. We conclude from proposition~\ref{CPT_cnd1} that $M$ is not
compact.
 \vskip 7 pt
(2) $\Rightarrow$ (3): Suppose that (2) holds. Let $a$ be a sequence in~$M$.  If the range of
$a$ is finite, then $a$ has a constant (therefore convergent) subsequence. Thus we assume that
the image of $a$ is infinite. By hypothesis $\ran a$ has an accumulation point $m \in M$. We
define $n \colon \N \sto \N$ inductively: let $n(1) = 1$; if $n(1), \dots, n(k)$ have been
defined, let $n(k+1)$ be an integer greater than $n(k)$ such that $a_{n_{k+1}} \in
B_{1/(k+1)}(m)$. (This is possible since $B_{1/(k+1)}(m)$ contains infinitely many distinct
points of the range of~$a$.) It is clear that $a \circ n$ is a subsequence of $a$ and that
$a_{n_k} \sto m$ as $k \sto \infty$.
 \vskip 7 pt
\qquad (3) $\Rightarrow$ (1): Let $\sfml U$ be an open cover for $M$. By
corollary~\ref{scpt_sep} the space $M$ is separable. Let $A$ be a countable dense subset
of $M$ and $\sfml B$ be the family of all open balls $B(a;r)$ such that
 \begin{align*}
        \text{(i)\quad} &a \in A;\\
       \text{(ii)\quad} &r \in \Q \text{ and;}\\
      \text{(iii)\quad} &B_r(a) \text{ is contained in at least one member of~$\sfml U$.}
 \end{align*}
Then for each $B \in \sfml B$ choose a set $U_B$ in $\sfml U$ which contains $B$.  Let
  \[ \sfml V = \{U_B \in \sfml U : B \in \sfml B\}\,. \]
It is clear that $\sfml V$ is countable; we show that $V$ covers $M$.

Let $x \in M$. There exist $U_0$ in $\sfml U$ and $r > 0$ such that $B_r(x) \subseteq
U_0$. Since $A$ is dense in $M$, proposition~\ref{cnd_dns} allows us to select a point
$a$ in $A \cap B_{\frac13r}(x)$. Next let $s$ be any rational number such that $\frac13r
< s < \frac23r$.  Then $x \in B_s(a) \subseteq B_r(x) \subseteq U_0$. [Proof: if $y \in
B_s(a)$, then
   \[ d(y,x) \le d(y,a) + d(a,x) < s + \frac13r < r\,; \]
so $y \in B_r(x)$.]  This shows that $B_s(a)$ belongs to $\sfml B$ and that $x \in
U_{B_s(a)} \in \sfml V$. Thus $\sfml V$ covers $M$. Now enumerate the members of $\sfml
V$ as a sequence $(V_1, V_2, V_3, \dots)$ and let $W_n = \cup_{k=1}^n V_k$ for each $n
\in \N$. To complete the proof it suffices to find an index $n$ such that $W_n = M$.
Assume there is no such~$n$.  Then for every $k$ we may choose a point $x_k$ in
${W_k}^c$. The sequence $(x_k)$ has, by hypothesis, a convergent subsequence $(x_{n_k})$.
Let $b$ be the limit of this sequence.  Then for some $m$ in $\N$ we have $b \in V_m
\subseteq W_m$. Thus $W_m$ is an open set which contains $b$ but only finitely many of
the points $x_{n_k}$. ($W_m$ contains at most the points $x_1, \dots, x_{m-1}$.) Since
$x_{n_k} \sto b$, this is not possible.
\end{prf}

\begin{prf}\label{sol_HBThm}(Solution to~\ref{HBThm})
A compact subset of \emph{any} metric space is closed and bounded (by
problem~\ref{cpt_clbdd}). It is the converse we are concerned with here.

Let $A$ be a closed and bounded subset of $\R^n$. Since it is bounded, there exist closed
bounded intervals $J_1, \dots, J_n$ in $\R$ such that
  \[ A \subseteq J \equiv J_1 \times\dots\times J_n\,. \]
Each $J_k$ is compact by example~\ref{cpt_exm3}.  Their product J is compact by
corollary~\ref{prod_cpt}.  Since $J$ is a compact subset of $\R^n$ under the product metric,
it is a compact subset of $\R^n$ under its usual Euclidean metric (see
proposition~\ref{mtr_vs_top} and the remarks preceding it). Since $A$ is a closed subset
of~$J$, it is compact by proposition~\ref{CPT_cnd1}.
\end{prf}






%END chapter 16












\section{Exercises in chapter 17}

\begin{prf}\label{sol_prop_conn_oc}(Solution to~\ref{prop_conn_oc})
Suppose there exists a nonempty set $U$ which is properly contained in $M$ and which is both
open and closed.  Then, clearly, the open sets $U$ and $U^c$ disconnect~$M$.  Conversely,
suppose that the space $M$ is disconnected by sets $U$ and~$V$.  Then the set $U$ is not the
null set, is not equal to $M$ (because its complement $V$ is nonempty), is open, and is closed
(because $V$ is open).
\end{prf}

\begin{prf}\label{sol_prop_mut_sep2}(Solution to~\ref{prop_mut_sep2})
If $N$ is disconnected, it can be written as the union of two disjoint nonempty sets $U$ and
$V$ which are open in~$N$.  (These sets need not, of course, be open in $M$.)  We show that
$U$ and $V$ are mutually separated. It suffices to prove that $U \cap \clo V$ is empty, that
is, that $U \subseteq {\clo V}^c$. To this end suppose that $u \in U$. Since $U$ is open in
$N$, there exists $\delta > 0$ such that
  \[ N \cap B_\delta(u) = \{x \in N \colon d(x,u) < \delta\} \subseteq U \subseteq V^c\,. \]
Clearly $B_\delta(u)$ is the union of two sets: $N \cap B_\delta(u)$ and $N^c \cap
B_\delta(u)$.  We have just shown that the first of these is contained in $V^c$.  The second
contains no points of $N$ and therefore no points of~$V$.  Thus $B_\delta(u) \subseteq V^c$.
This shows that $u$ does not belong to the closure (in $M$) of the set~$V$; so $u \in {\clo
V}^c$.  Since $u$ was an arbitrary point of $U$, we conclude that $U \subseteq {\clo V}^c$.

Conversely, suppose that $N = U \cup V$ where $U$ and $V$ are nonempty sets mutually separated
in~$M$.  To show that the sets $U$ and $V$ disconnect $N$, we need only show that they are
open in $N$, since they are obviously disjoint.

We prove that $U$ is open in $N$.  Let $u \in U$ and notice that since $U \cap \clo V$ is
empty, $u$ cannot belong to $\clo V$. Thus there exists $\delta > 0$ such that $B_\delta(u)$
is disjoint from~$V$.  Then certainly $N \cap B_\delta(u)$ is disjoint from~$V$.  Thus $N \cap
B_\delta(u)$ is contained in $U$. Conclusion: $U$ is open in~$N$.
\end{prf}

\begin{prf}\label{sol_exam_sin_conn}(Solution to~\ref{exam_sin_conn})
Let $G = \{(x,y) \colon y = \sin x\}$.  The function $x \mapsto (x,\sin x)$ is a continuous
surjection from $\R$ (which is connected by proposition~\ref{prop_conn_int}) onto $G \subseteq
\R^2$.  Thus $G$ is connected by theorem~\ref{thm_contimg_conn2}.
\end{prf}

\begin{prf}\label{sol_prop_un_conn}(Solution to~\ref{prop_un_conn})
Let the metric space $M$ be the union of a family $\sfml C$ of connected subsets of~$M$
and suppose that $\bigcap \sfml C \ne \emptyset$.  Argue by contradiction.  Suppose that
$M$ is disconnected by disjoint nonempty open sets $U$ and $V$.  Choose an element $p$
in~$\bigcap \sfml C$.  Without loss of generality suppose that $p \in U$. Choose $v \in
V$.  There is at least one set C in $\sfml C$ such that $v \in C$.  We reach a
contradiction by showing that the sets $U \cap C$ and $V \cap C$ disconnect~$C$. These
sets are nonempty [$p$ belongs to $C \cap U$ and $v$ to $C \cap V$] and open in~$C$. They
are disjoint because $U$ and $V$ are, and their union is $C$, since
  \[ (U \cap C) \cup (V \cap C) = (U \cup V) \cap C = M \cap C = C\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_exam_sqr_conn}(Solution to~\ref{exam_sqr_conn})
Between an arbitrary point $x$ in the unit square and the origin there is a straight line
segment, denote it by~$[0,x]$.  Line segments are connected because they are continuous images
of (in fact, homeomorphic to) intervals in~$\R$. The union of all the segments $[0,x]$ where
$x$ is in the unit square is the square itself. The intersection of all these segments is the
origin.  Thus by proposition~\ref{prop_un_conn} the square is connected.
\end{prf}

\begin{prf}\label{sol_exam_conn_notarcw}(Solution to~\ref{exam_conn_notarcw})
The set $B = \{(x,\sin x^{-1}) \colon  0 < x \le 1\}$ is a connected subset of $\R^2$ since it
is the continuous image of the connected set $(0,1]$ (see theorem~\ref{thm_contimg_conn2}).
Then by proposition~\ref{prop_clconn_conn} the set $M := \clo B$ is also connected.  Notice
that $M = A \cup B$ where $A = \{(0,y) \colon \abs y \le 1\}$.

To show that $M$ is not arcwise connected, argue by contradiction. Assume that there exists a
continuous function $f \colon [0,1] \sto M$ such that $f(0) \in A$ and $f(1) \in B$.  We
arrive at a contradiction by showing that the component function $f^2 = \pi_2 \circ f$ is not
continuous at the point $t_0 = \sup f^\gets(A)$.

To this end notice first that, since $A$ is closed in $M$ and $f$ is continuous, the set
$f^\gets(A)$ is closed in $[0,1]$.  By example~\ref{sup_in_clo} the point $t_0$ belongs to
$f^\gets(A)$. Without loss of generality we may suppose that $f^2(t_0) \le 0$. We need only
show that for every $\delta > 0$ there exists a number $t \in [0,1]$ such that $\abs{t- t_0} <
\delta$ and $\abs{f^2(t) - f^2(t_0)} \ge 1$.

Let $\delta > 0$.  Choose a point $t_1$ in $(t_0, t_0 + \delta) \cap [0,1]$.  By
proposition~\ref{prop_conn_int} the interval $[t_0,t_1]$ is connected, so its image
$\bigl(f^1\bigr)^\sto[t_0,t_1]$ under the continuous function $f^1 = \pi_1 \circ f$ is also a
connected subset of $[0,1]$ (by theorem~\ref{thm_contimg_conn2}) and therefore itself an
interval. Let $c = f^1(t_1)$.  From $t_1 > t_0$ infer that $t_1 \in f^\gets(B)$ and that
therefore $c > 0$.  Since $t_0 \in f^\gets(A)$ it is clear that $f^1(t_0) = 0$.  Thus the
interval $[0,c]$ is not a single point and it is contained in $\bigl(f^1\bigr)^\sto[t_0,t_1]$.
Choose $n \in \N$ sufficiently large that
  \[ x = \frac2{(4n+1)\pi} < c\,. \]
Since $x$ belongs to $\bigl(f^1\bigr)^\sto[t_0,t_1]$, there exists $t \in [t_0,t_1]$ such that
$x = f^1(t)$.  And since $x > 0$ the point $f(t)$ belongs to~$B$.  This implies that
  \[ f(t) = \bigl(f^1(t),f^2(t)\bigr) = (x,\sin x^{-1}) =
                             (x,\sin(4n+1)\tfrac\pi2) = (x,1)\,. \]
But then (since $f^2(t_0) \le 0$)
  \[ \abs{f^2(t) - f^2(t_0)} = \abs{1 - f^2(t_0)} \ge 1\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_opconn_arcw}(Solution to~\ref{prop_opconn_arcw})
Let $A$ be a connected open subset of $\R^n$. If $A$ is empty the result is obvious, so
suppose that it is not.  Choose $a \in A$.  Let $U$ be the set of all points $x$ in $A$ for
which there exists a continuous function $f \colon [0,1] \sto A$ such that $f(0) = a$ and
$f(1) = x$. The set $U$ is nonempty [it contains $a$].  Let $V = A \setminus U$.  We wish to
show that $V$ is empty.  Since $A$ is connected it suffices to show that both $U$ and $V$ are
open.

To show that $U$ is open let $u \in U$ and let $f \colon [0,1] \sto A$ be a continuous
function such that $f(0) = a$ and $f(1) = u$.  Since $A$ is an open subset of $\R^n$ there
exists $\delta > 0$ such that $B_\delta(u) \subseteq A$.  Every point $b$ in $B_\delta(u)$ can
be joined to $u$ by the parametrized line segment $\ell \colon [0,1] \sto A$ defined by
  \[ \ell(t) = \bigl((1-t)u_1 + tb_1, \dots , (1-t)u_n + tb_n\bigr)\,. \]
It is easy to see that since $b$ can be joined to $u$ and $u$ to $a$, the point $b$ can be
joined to~$a$.  [Proof: If $f \colon [0,1] \sto A$ and $\ell \colon [0,1] \sto A$ are
continuous functions satisfying $f(0) = a$, $f(1) = u$, $\ell(0) = u$, and $\ell(1) = b$, then
the function $g \colon [0,1] \sto A$ defined by
  \[ g(t) = \begin{cases}
                   f(2t),  &\text{for $0 \le t \le \frac12$} \\
              \ell(2t-1),  &\text{for $\frac12 < t \le 1$}
            \end{cases} \]
is continuous, $g(0) = a$, and $g(1) = b$.]  This shows that $B_\delta(u) \subseteq U$.

To see that $V$ is open let $v \in V$ and choose $\epsilon > 0$ so that $B_\epsilon(v)
\subseteq A$.  If some point $y$ in $B_\epsilon(v)$ could be joined to $a$ by an arc in~$A$,
then $v$ could be so joined to $a$ (\emph{via}~$y$).  Since this is not possible, we have that
$B_\epsilon(v) \subseteq V$.
\end{prf}





%END chapter 17














\section{Exercises in chapter 18}

\begin{prf}\label{sol_conv_cau}(Solution to~\ref{conv_cau})
Suppose $(x_n)$ is a convergent sequence in a metric space and $a$ is its limit.  Given
$\epsilon > 0$ choose $n_0 \in \N$ so that $d(x_n,a) < \frac12\epsilon$ whenever $n \ge n_0$.
Then $d(x_m,x_n) \le d(x_m,a) + d(a,x_n) < \frac12\epsilon + \frac12\epsilon = \epsilon$
whenever $m,n \ge n_0$. This shows that $(x_n)$ is Cauchy.
\end{prf}

\begin{prf}\label{sol_cs_css}(Solution to~\ref{cs_css})
Suppose that $\left(x_{n_k}\right)$ is a convergent subsequence of a Cauchy sequence $(x_n)$
and that $x_{n_k} \sto a$.  Given $\epsilon > 0$ choose $n_0$ such that $d(x_m,x_n) <
\frac12\epsilon$ whenever $m$, $n \ge n_0$.  Next choose $k \in \N$ such that $n_k \ge n_0$
and $d\left(x_{n_k},a\right) < \frac12\epsilon$.  Then for all $m \ge n_0$
  \[ d(x_m,a) \le d\left(x_m,x_{n_k}\right) + d\left(x_{n_k},a\right)
               < \tfrac12\epsilon + \tfrac12\epsilon = \epsilon\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_cau_bdd}(Solution to~\ref{cau_bdd})
A sequence in a metric space $M$, being a function, is said to be bounded if its range is a
bounded subset of $M$.  If $(x_n)$ is a Cauchy sequence in $M$, then there exists $n_0 \in \N$
such that $d(x_m,x_n) < 1$ whenever $m,n \ge n_0$.  For $1 \le k \le n_0 - 1$, let $d_k =
d\left(x_k,x_{n_0}\right)$; and let $r = \max\left\{d_1, \dots, d_{n_0 - 1},1\right\}$.  Then
for every $k \in \N$ it is clear that $x_k$ belongs to $C_r\left(x_{n_0}\right)$ (the closed
ball about $x_{n_0}$ of radius~$r$).  Thus the range of the sequence $(x_n)$ is bounded.
\end{prf}

\begin{prf}\label{sol_prod_compl}(Solution to~\ref{prod_compl})
Let $(M,d)$ and $(N,\rho)$ be complete metric spaces. Let $d_1$ be the usual product metric on
$M \times N$ (see \ref{prod_met}).  If $\left((x_n,y_n)\right)_{n=1}^\infty$ is a Cauchy
sequence in $M \times N$, then $(x_n)$ is a Cauchy sequence in $M$ since
 \begin{align*}
       d(x_m,x_n) &\le d(x_m,x_n) + \rho(y_m,y_n) \\
                  &= d_1\left((x_m,y_m)\,,\,(x_n,y_n)\right)\sto 0
 \end{align*}
as $m$, $n \sto \infty$.

Similarly, $(y_n)$ is a Cauchy sequence in $N$.  Since $M$ and $N$ are complete, there are
points $a$ and $b$ in $M$ and $N$ respectively such that $x_n \sto a$ and $y_n \sto b$.  By
proposition \ref{seq_prod},  $(x_n,y_n) \sto (a,b)$.  Thus $M \times N$ is complete.
\end{prf}

\begin{prf}\label{sol_eq_compl}(Solution to~\ref{eq_compl})
It suffices to show that if $(M,d)$ is complete, then $(M,\rho)$ is.  There exist
$\alpha,\beta > 0$ such that $d(x,y) \le \alpha\, \rho(x,y)$ and $\rho(x,y) \le \beta\,
d(x,y)$ for all $x,y \in M$.  Let $(x_n)$ be a Cauchy sequence in $(M,\rho)$.  Then since
  \[ d(x_m,x_n) \le \alpha\,\rho(x_m,x_n) \sto 0 \qquad \text{as $m$, $n \sto \infty$}\,, \]
the sequence $(x_n)$ is Cauchy in~$(M,d)$.  By hypothesis $(M,d)$ is complete; so there is a
point $a$ in $M$ such that $x_n \sto a$ in $(M,d)$.  But then $x_n \sto a$ in $(M,\rho)$ since
  \[ \rho(x_n,a) \le \beta\,d(x_n,a) \sto 0 \qquad \text{as $n \sto \infty$}\,. \]
This shows that $(M,\rho)$ is complete.
\end{prf}

\begin{prf}\label{sol_bdd_compl}(Solution to~\ref{bdd_compl})
Let $(f_n)$ be a Cauchy sequence in $\fml B(S,\R)$. Since for every $x \in S$
  \[ \abs{f_m(x) - f_n(x)} \le d_u(f_m,f_n) \sto 0 \qquad \text{as $m$, $n \sto \infty$}\,, \]
it is clear that $\bigl(f_n(x)\bigr)$ is a Cauchy sequence in $\R$ for each $x \in S$.  Since
$\R$ is complete, there exists, for each $x \in S$, a real number $g(x)$ such that $f_n(x)
\sto g(x)$ as $n \sto \infty$.  Consider the function $g$ defined by
   \[ g \colon S \sto \R \colon x \mapsto g(x)\,. \]
We show that $g$ is bounded and that $f_n \sto g \text{\,(unif)}$.  Given $\epsilon > 0$
choose $n_0 \in \N$ so that $d_u(f_m,f_n) < \epsilon$ whenever $m$, $n \ge n_0$.  Then for
each such $m$ and $n$
  \[ \abs{f_m(x) - f_n(x)} < \epsilon \quad \text{whenever $x \in S$}\,. \]
Take the limit as  $m \sto \infty$ and obtain
  \[ \abs{g(x) - f_n(x)} \le \epsilon \]
for every $n \ge n_0$ and $x \in S$.  This shows that $g - f_n$ is bounded and that
$d_u(g,f_n) \le \epsilon$.  Therefore the function
 \[ g = (g - f_n) + f_n \]
is bounded and $d_u(g,f_n) \sto 0$ as $n \sto \infty$.
\end{prf}





%END chapter 18











\section{Exercises in chapter 19}

\begin{prf}\label{sol_cntr_cnt}(Solution to~\ref{cntr_cnt})
\noindent\textbf{\ref{cntr_cnt}}\quad  Let $f:M \sto N$ be a contraction and let $a \in M$. We
show $f$ is continuous at $a$. Given $\epsilon > 0$, choose $\delta = \epsilon$.  If $d(x,a) <
\delta$ then $d(f(x),f(a)) \le c d(x,a) \le d(x,a) < \delta = \epsilon$, where $c$ is a
contraction constant for~$f$.
\end{prf}

\begin{prf}\label{sol_cntr_exml}(Solution to~\ref{cntr_exm1})
If $(x,y)$ and $(u,v)$ are points in $\R^2$, then
 \begin{align*}  d\bigl(f(x,y),f(u,v)\bigr)
     &= \tfrac13\left[(u-x)^2 + (y-v)^2 + (x-y-u+v)^2\right]^{1/2} \\
     &= \tfrac13\left[2(x-u)^2 +
                     2(y-v)^2 - 2(x-u)(y-v)\right]^{1/2}\\
     &\le \tfrac13\left[2(x-u)^2 + 2(y-v)^2 +
                      2\abs{x-u}\,\abs{y-v}\right]^{1/2} \\
     &\le \tfrac13\left[2(x-u)^2 + 2(y-v)^2 + (x-u)^2 +
                 (y-v)^2\right]^{1/2} \\
     &= \tfrac{\sqrt3}3\left[(x-u)^2 + (y-v)^2\right]^{1/2} \\
     &= \tfrac1{\sqrt3} d\bigl((x,y),(u,v)\bigr)\,. \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_cmthm}(Solution to~\ref{cmthm})
Let $M$ be a complete metric space and $f \colon M \sto M$ be contractive.  Since $f$ is
contractive, there exists $c \in (0,1)$ such that
 \begin{equation}
        d(f(x),f(y)) \le c\, d(x,y)
 \end{equation}
for all $x$, $y \in M$.  First we establish the existence of a fixed point.  Define
inductively a sequence $\bigl(x_n\bigr)_{n=0}^\infty$ of points in $M$ as follows: Let $x_0$
be an arbitrary point in $M$.  Having chosen $x_0,\dots,x_n$ let $x_{n+1} = f(x_n)$.  We show
that $(x_n)$ is Cauchy.  Notice that for each $k \in \N$
 \begin{equation}\label{fpt_eq8}
     d(x_k,x_{k+1}) \le c^k d(x_0,x_1)\,.
 \end{equation}
[Inductive proof: If $k = 1$, then $d(x_1,x_2) = d\left(f(x_0),f(x_1)\right) \le
c\,d(x_0,x_1)$.  Suppose that \eqref{fpt_eq8} holds for $k = n$.  Then
$d\bigl(x_{n+1},x_{n+2}\bigr) = d\bigl(f(x_n),f(x_{n+1})\bigr) \le c\,d\bigl(x_n,x_{n+1}\bigr)
\le c \cdot c^n\,d\bigl(x_0,x_1\bigr) = c^{n+1}d\bigl(x_0,x_1\bigr)$.)  Thus whenever $m < n$
 \begin{align}
     d\bigl(x_m,x_n\bigr)
        &\le \sum_{k=m}^{n-1}d\bigl(x_k,x_{k+1}\bigr) \notag \\
        &\le \sum_{k=m}^{n-1}c^kd\bigl(x_0,x_1\bigr) \notag \\
        &\le d\bigl(x_0,x_1\bigr)\sum_{k=m}^\infty c^k  \notag \\
        &= d\bigl(x_0,x_1\bigr)\frac{c^m}{1-c}\,. \label{fpt_eq9}
 \end{align}
Since $c^m \sto 0$ as $m \sto \infty$, we see that
  \[ d(x_m,x_n) \sto 0 \qquad \text{as $m$, $n \sto \infty$}\,. \]
That is, the sequence $(x_n)$ is Cauchy.  Since $M$ is complete there exists a point $p$ in
$M$ such that $x_n \sto p$.  The point $p$ is fixed under $f$ since $f$ is continuous and
therefore
  \[ f(p) = f(\lim x_n) = \lim f(x_n) = \lim x_{n+1} = p\,. \]

Finally, to show that there is at most one point fixed by $f$, argue by contradiction.  Assume
that $f(p) = p$, $f(q) = q$, and $p \ne q$.  Then
 \begin{align*}
          d(p,q) &= d\bigl(f(p),f(q)\bigr) \\
                 &\le c\,d(p,q) \\
                 &< d(p,q)
 \end{align*}
which certainly cannot be true.
\end{prf}

\begin{prf}\label{sol_fpt_exr1}(Solution to~\ref{fpt_exr1})
(a) In inequality \eqref{fpt_eq6} of example~\ref{fpt_exm1} we found that $c = 0.4$ is a
contraction constant for the mapping~$T$. Thus, according to \ref{error1},
 \begin{align*}
      d_1(x_4,p) &\le  d_1(x_0,x_1)\frac{c^4}{1 - c} \\
                 &= (0.7 + 1.1)\frac{(0.4)^4}{1 - 0.4} \\
                 &= 0.0768.
 \end{align*}

\vskip 7 pt

(b) The $d_1$ distance between $x_4$ and $p$ is
   \begin{align*}
        d_1(x_4,p) &= \abs{1.0071 - 1.0000} + \abs{0.9987 - 1.0000} \\
                   &= 0.0084 \,.  \qedhere
   \end{align*}

(c) We wish to choose $n$ sufficiently large that $d_1(x_n,p) \le 10^{-4}$. According to
corollary \ref{error1} it suffices to find $n$ such that
   \[ d_1(x_0,x_1) \frac{c^n}{1 - c} \le 10^{-4}\,. \]
This is equivalent to requiring
   \[ (0.4)^n \le \frac{10^{-4} (0.6)}{1.8} = \tfrac13 10^{-4}\,. \]
For this, $n = 12$ suffices.
\end{prf}

\begin{prf}\label{sol_fpt_inteq}(Solution to~\ref{fpt_inteq})
Define $T$ on $\fml C([0,1],\R)$ as in the hint.  The space $\fml C([0,1],\R)$ is a complete
metric space by example~\ref{CMR_cmpl}.  To see that $T$ is contractive, notice that for $f,g
\in \fml C([0,1],\R)$ and $0 \le x \le 1$
 \begin{align*}
    \abs{Tf(x) - Tg(x)} &= \left\lvert\int_0^x t^2f(t)\,dt - \int_0^x t^2g(t)\,dt\right\rvert \\
                        &= \left\lvert\int_0^x t^2(f(t) - g(t))\,dt\right\rvert  \\
                        &\le \int_0^x t^2\abs{f(t) - g(t)}\,dt \\
                        &\le d_u(f,g) \int_0^x t^2\,dt \\
                        &= \tfrac13x^3\,d_u(f,g)\,.
 \end{align*}
Thus
 \begin{align*}
     d_u(Tf,Tg) &= \sup\{\abs{Tf(x) - Tg(x)}: 0 \le x \le 1\} \\
                &\le \tfrac13\,d_u(f,g)\,.
 \end{align*}
This shows that $T$ is contractive.

Theorem \ref{cmthm} tells us that the mapping $T$ has a unique fixed point in $\fml
C([0,1],\R)$.  That is, there is a unique continuous function on $[0,1]$ which
satisfies~\eqref{fpt_eq7}.

To find this function we start, for convenience, with the function $g_0 = 0$ and let $g_{n+1}
= Tg_n$ for all $n \ge 0$.  Compute $g_1$, $g_2$, $g_3$, and~$g_4$.
 \begin{align*}
            g_1(x) &= Tg_0(x) = \tfrac13 x^3, \\
            g_2(x) &= Tg_1(x)  \\
                   &= \tfrac13 x^3 + \int_0^x t^2
                         \left(\tfrac13t^3\right)\,dt \\
                   &= \tfrac13 x^3 + \tfrac1{3\cdot6}x^6\,, \\
            g_3(x) &= Tg_2(x) \\
                   &= \tfrac13 x^3 + \int_0^x t^2 \left(\tfrac13t^3
                         + \tfrac1{3\cdot6}x^6 \right)\,dt \\
                   &= \tfrac13 x^3 + \tfrac1{3\cdot6}x^6
                         + \tfrac1{3\cdot6\cdot9}x^9 \,, \\
%           g_4(x) &= Tg_3(x) \\
%                  &= \tfrac13 x^3 + \tfrac1{3\cdot6}x^6
%                       + \tfrac1{3\cdot6\cdot9}x^9
%                       + \tfrac1{3\cdot6\cdot9\cdot12}x^{12}\,.
 \end{align*}

It should now be clear that for every  $n \in \N$
  \[ g_n(x) = \sum_{k=1}^n \frac1{3^k\,k!}x^{3k}
           = \sum_{k=1}^n \frac1{k!}\left(\frac{x^3}3\right)^k  \]
and that the uniform limit of the sequence $(g_n)$ is the function $f$ represented by the
power series
  \[ \sum_{k=1}^\infty \frac1{k!} \left(\frac{x^3}3\right)^k\,. \]
Recall from elementary calculus that the power series expansion for $e^y$ (also written
$\exp(y)$) is
  \[ \sum_{k=0}^\infty \frac1{k!} y^k \]
for all $y$ in $\R$; that is,
  \[\sum_{k=1}^\infty \frac1{k!} y^k = e^y - 1\,.\]
Thus
 \begin{align*}
        f(x) &= \sum_{k=1}^\infty \frac1{k!} \left(\frac{x^3}3\right)^k \\
             &= \exp\left(\tfrac13 x^3\right) - 1\,.
 \end{align*}
Finally we check that this function satisfies \eqref{fpt_eq7} for all $x$ in~$\R$.
 \begin{align*}
     \tfrac13 x^3 + \int_0^x t^2 f(t)\,dt
           &= \tfrac13 x^3 + \int_0^x t^2 \left(\exp\left(\tfrac13 t^3\right) - 1\right)\,dt \\
           &= \tfrac13 x^3 + \left. \left(\exp\left(\tfrac13 t^3\right)
                                          - \tfrac13 t^3\right) \right\rvert_0^x  \\
           &= \exp\left(\tfrac13 x^3\right) - 1 \\
           &= f(x).    \qedhere
 \end{align*}
\end{prf}




%END chapter 19















\section{Exercises in chapter 20}

\begin{prf}\label{sol_vs_exer3}(Solution to~\ref{vs_exer3})
Suppose that $\vc 0$ and $\vc 0'$ are vectors in $V$ such that $x + \vc 0 = x$ and $x + \vc 0'
= x$ for all $x \in V$.  Then
 \[ \vc 0' = \vc 0'+ \vc 0 = \vc 0 + \vc 0' = \vc 0\,. \qedhere  \]
\end{prf}

\begin{prf}\label{sol_vs_exer1}(Solution to~\ref{vs_exer1})
The proof takes one line:
  \[ x = x + \vc 0 = x + (x + (-x)) = (x + x) + (-x) = x + (-x) = \vc 0\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_vs_exer2}(Solution to~\ref{vs_exer2})
Establish (a), (b), and (c) of the hint.
 \begin{enumerate}
  \item[(a)] Since $\alpha \vc 0 + \alpha \vc 0 = \alpha(\vc 0 + \vc 0) = \alpha \vc 0$,
we conclude from \ref{vs_exer1} that $\alpha \vc 0 = \vc 0$.
  \item[(b)] Use the same technique as in (a): since $0x + 0x = (0 + 0)x = 0x$, we deduce
from \ref{vs_exer1} that $0x = \vc 0$.
  \item[(c)] We suppose that $\alpha \ne 0$ and that $\alpha x = \vc 0$.
We prove that $x = \vc 0$.  Since the real number  $\alpha$ is not zero, its reciprocal
$a^{-1}$ exists.  Then
    \[ x = 1\cdot x = \bigl(\alpha^{-1}\alpha\bigr)x =
                           \alpha^{-1}(\alpha x) = \alpha^{-1}\vc 0 = \vc 0\,. \]
(The last equality uses part (a).)  \qedhere
 \end{enumerate}
\end{prf}

\begin{prf}\label{sol_vs_exer4}(Solution to~\ref{vs_exer4})
Notice that $(-x) + x  =  x + (-x) = \vc 0$.  According to \ref{vs_prob2}, the vector $x$ must
be the (unique) additive inverse of~$-x$.  That is, $x = -(-x)$.
\end{prf}

\begin{prf}\label{sol_vs_prop1}(Solution to~\ref{vs_prop1})
The set $W$ is closed under addition and scalar multiplication; so vector space axioms (1) and
(4) through (8) hold in $W$ because they do in~$V$.  Choose an arbitrary vector $x$ in~$W$.
Then using~(c) we see that the zero vector $\vc 0$ of $V$ belongs to $W$ because it is just
the result of multiplying $x$ by the scalar $0$ (see exercise~\ref{vs_exer2}). To show
that~(3) holds we need only verify that if $x \in W$, then its additive inverse $-x$ in $V$
also belongs to~$W$.  But this is clear from problem~\ref{vs_prob1} since the vector $-x$ is
obtained by multiplying $x$ by the scalar~$-1$.
\end{prf}

\begin{prf}\label{sol_intr_vsbs}(Solution to~\ref{intr_vsbs})
Use proposition \ref{vs_prop1}.

(a) The zero vector belongs to every member of $\sfml S$ and thus to~$\bigcap \sfml S$.
Therefore $\bigcap \sfml S \ne \emptyset$.

(b) Let $x$, $y \in \bigcap \sfml S$.  Then $x$, $y \in S$ for every $S \in \sfml S$.
Since each member of $\sfml S$ is a subspace, $x + y$ belongs to $S$ for every $S \in
\sfml S$. Thus $x + y \in \bigcap \sfml S$.

(c) Let $x \in \bigcap \sfml S$ and $\alpha \in \R$.  Then $x \in S$ for every $S \in
\sfml S$. Since each member of $\sfml S$ is closed under scalar multiplication, $\alpha
x$ belongs to $S$ for every $S \in \sfml S$.  Thus $\alpha x \in \bigcap \sfml S$.
\end{prf}

\begin{prf}\label{sol_vs_exer5}(Solution to~\ref{vs_exer5})
We wish to find scalars $\alpha$, $\beta$, $\gamma$, and $\delta$ such that
  \[ \alpha(1,0,0) + \beta(1,0,1) + \gamma(1,1,1) + \delta(1,1,0) = (0,0,0)\,. \]
This equation is equivalent to
  \[ (\alpha + \beta + \gamma + \delta, \gamma + \delta, \beta + \gamma)= (0,0,0)\,. \]
Thus we wish to find a (not necessarily unique) solution to the system of equations:
 \begin{alignat*}{5}
         &\alpha  & +&\beta &  +&\gamma & +&\delta &&= 0 \\
         &{}      &  &{}  &    +&\gamma & +&\delta &&= 0 \\
         &{}      &  &\beta &  +&\gamma & +&{}     &&= 0
 \end{alignat*}
One solution is  $\alpha = \gamma =1$, $\beta = \delta = -1$.
\end{prf}

\begin{prf}\label{sol_vs_exer6}(Solution to~\ref{vs_exer6})
We must find scalars $\alpha$, $\beta$, $\gamma \ge 0$ such that $\alpha + \beta + \gamma = 1$
and
  \[ \alpha(1,0) + \beta(0,1) + \gamma(3,0) = (2,1/4)\,. \]
This last vector equation is equivalent to the system of scalar equations
  \begin{equation*}\left\{
   \begin{aligned}
           \alpha + 3\gamma &= 2 \\
                      \beta &= \tfrac14\,.
   \end{aligned}\right.
  \end{equation*}
From $\alpha + \frac14 + \gamma = 1$ and $\alpha + 3\gamma = 2$, we conclude that $\alpha =
\frac18$ and $\gamma = \frac58$.
\end{prf}

\begin{prf}\label{sol_vs_exer7}(Solution to~\ref{vs_exer7})
In order to say that the intersection of the family of all convex sets which contain $A$
is the ``smallest convex set containing $A$'', we must know that this intersection is
indeed a convex set.  This is an immediate consequence of the fact that the intersection
of any family of convex sets is convex.  (Proof. Let $\sfml A$ be a family of convex
subsets of a vector space and let $x,y \in \bigcap \sfml A$.  Then $x,y \in A$ for every
$A \in \sfml A$.  Since each $A$ in $\sfml A$ is convex, the segment $[x,y]$ belongs to
$A$ for every~$A$.  Thus $[x,y] \subseteq \bigcap \sfml A$.)
\end{prf}





%END chapter 20
















\section{Exercises in chapter 21}

\begin{prf}\label{sol_lt_exam1}(Solution to~\ref{lt_exam1})
If $x$, $y \in \R^3$ and $\alpha \in \R$, then
 \begin{align*}
    T(x+y) &= T(x_1+y_1, x_2+y_2, x_3+y_3) \\
           &= (x_1+y_1+x_3+y_3, x_1+y_1-2x_2-2y_2) \\
           &= (x_1+x_3, x_1-2x_2) + (y_1+y_3, y_1-2y_2) \\
           &= Tx + Ty
 \end{align*}
and
 \begin{align*}
     T(\alpha x) &= T(\alpha x_1, \alpha x_2, \alpha x_3) \\
                 &= (\alpha x_1 + \alpha x_3,
                               \alpha x_1 - 2\alpha x_2) \\
                 &= \alpha(x_1+x_3, x_1-2x_2) \\
                 &= \alpha Tx .   \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_lt_exer1}(Solution to~\ref{lt_exer1})
Write $(2,1,5)$ as $2e^1 + e^2 + 5e^3$.  Use the linearity of $T$ to see that
 \begin{align*}
         T(2,1,5) &= T(2e^1 + e^2 + 5e^3) \\
                  &= 2Te^1 + Te^2 + 5Te^3 \\
                  &= 2(1,0,1) + (0,2,-1) + 5(-4,-1,3) \\
                  &= (2,0,2) + (0,2,-1) + (-20,-5,15) \\
                  &= (-18, -3, 16) .   \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_lt_prop1}(Solution to~\ref{lt_prop1})
(a) Let $x$ be any vector in~$V$; then (by proposition~\ref{vs_exer2}) $0\,x = \vc 0$.  Thus
$T(\vc 0) = T(0x) = 0\,Tx = \vc 0$.

(b) By proposition~\ref{vs_prob1}
 \begin{align*}
             T(x-y) &= T(x + (-y)) \\
                    &= T(x + (-1)y) \\
                    &= Tx + (-1)Ty \\
                    &= Tx - Ty .  \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_lt_exer2}(Solution to~\ref{lt_exer2})
First we determine where $T$ takes an arbitrary vector $(x,y,z)$ in its domain.
 \begin{align*}
     T(x,y,z) &= T(xe^1 + ye^2 + ze^3) \\
              &= xTe^1 + yTe^2 + zTe^3 \\
              &= x(1,-2,3) + y(0,0,0) + z(-2,4,-6) \\
              &= (x-2z, -2x+4z, 3x-6z) .
 \end{align*}

A vector $(x,y,z)$ belongs to the kernel of $T$ if and only if $T(x,y,z) = (0,0,0)$; that is,
if and only if $x-2z = 0$.  (Notice that the two remaining equations, $-2x+4z = 0$ and $3x-6z
= 0$, have exactly the same solutions.) Thus the kernel of $T$ is the set of points $(x,y,z)$
in $\R^3$ such that $x = 2z$.  This is a plane in $\R^3$ which contains the $y$-axis.  A
vector $(u,v,w)$ belongs to the range of $T$ if an only if there exists a vector $(x,y,z) \in
\R^3$ such that $(u,v,w) = T(x,y,z)$.  This happens if an only if
  \[ (u,v,w) = (x - 2z, -2x + 4z, 3x - 6z)\,; \]
that is, if an only if
 \begin{align*}
              u &= x - 2z \\
              v &= -2x + 4z = -2u \\
              w &= 3x - 6z = 3u .
 \end{align*}
Consequently, only points of the form $(u,-2u,3u) = u(1,-2,3)$ belong to~$\ran T$.  Thus the
range of $T$ is the straight line in $\R^3$ through the origin which contains the point
$(1,-2,3)$.
\end{prf}

\begin{prf}\label{sol_lt_prop2}(Solution to~\ref{lt_prop2})
According to proposition~\ref{vs_prop1} we must show that $\ran T$ is nonempty and that it is
closed under addition and scalar multiplication. That it is nonempty is clear from
proposition~\ref{lt_prop1}(a): $0 = T0 \in \ran T$.  Suppose that $u,v \in \ran T$.  Then
there exist $x,y \in V$ such that $u = Tx$ and $v = Ty$. Thus
  \[ u + v = Tx + Ty = T(x + y)\,; \]
so $u + v$ belongs to $\ran T$.  This shows that $\ran T$ is closed under addition.  Finally,
to show that it is closed under scalar multiplication let $u \in \ran T$ and $\alpha \in \R$.
There exists $x \in V$ such that $u = Tx$; so
  \[ \alpha u = \alpha Tx = T(\alpha x) \]
which shows that $\alpha u$ belongs to $\ran T$.
\end{prf}

\begin{prf}\label{sol_endo_vs}(Solution to~\ref{endo_vs})
Recall from example~\ref{vs_exam1} that under pointwise operations of addition and scalar
multiplication $\fml F(V,W)$ is a vector space.  To prove that $\ofml L(V,W)$ is a vector
space it suffices to show that it is a vector subspace of $\fml F(V,W)$.  This may be
accomplished by invoking proposition~\ref{vs_prop1}, according to which we need only verify
that $\ofml L(V,W)$ is nonempty and is closed under addition and scalar multiplication.  Since
the zero transformation (the one which takes every $x$ in $V$ to the zero vector in $W$) is
certainly linear, $\ofml L(V,W)$ is not empty. To prove that it is closed under addition we
verify that the sum of two linear transformations is itself linear. To this end let $S$ and
$T$ be members of $\ofml L(V,W)$.  Then for all $x$ and $y$ in $V$
 \begin{equation}\label{endo_vs1}
  \begin{split}
      (S + T)(x + y) &= S(x + y) + T(x + y) \\
                     &= Sx + Sy + Tx + Ty \\
                     &= (S + T)x + (S + T)y.
  \end{split}
 \end{equation}
(It is important to be cognizant of the reason for each of these steps.  There is no
``distributive law'' ate work here.  The first and last use the definition of addition as a
pointwise operation, while the middle one uses the linearity of $S$ and $T$.) Similarly, for
all $x$ in $V$ and $\alpha$ in~$\R$
 \begin{equation}\label{endo_vs2}
  \begin{split}
       (S + T)(\alpha x) &= S(\alpha x) + T(\alpha x) \\
                         &= \alpha Sx + \alpha Tx \\
                         &= \alpha (Sx + Tx) \\
                         &= \alpha (S+T)x .
  \end{split}
 \end{equation}
Equations~\eqref{endo_vs1} and~\eqref{endo_vs2} show that $S + T$ is linear and therefore
belongs to $\ofml L(V,W)$.

We must also prove that $\ofml L(V,W)$ is closed under scalar multiplication.  Let $T \in
\ofml L(V,W)$ and $\alpha \in \R$, and show that the function $\alpha T$ is linear.  For all
$x,y \in V$
 \begin{equation}\label{endo_vs3}
  \begin{split}
           (\alpha T)(x + y) &= \alpha(T(x + y)) \\
                             &= \alpha(Tx + Ty) \\
                             &= \alpha(Tx) + \alpha(Ty) \\
                             &= (\alpha T)x + (\alpha T)y .
  \end{split}
 \end{equation}
Finally, for all $x$ in $V$ and $\beta$ in~$\R$
 \begin{equation}\label{endo_vs4}
  \begin{split}
          (\alpha T)(\beta x) &= \alpha(T(\beta x)) \\
                              &= \alpha(\beta(Tx)) \\
                              &= (\alpha\beta)Tx \\
                              &= (\beta\alpha)Tx \\
                              &= \beta(\alpha(Tx)) \\
                              &= \beta((\alpha T)x)\, .
  \end{split}
 \end{equation}
Equations~\eqref{endo_vs3} and~\eqref{endo_vs4} show that $\alpha T$ belongs to~$\ofml
L(V,W)$.
\end{prf}

\begin{prf}\label{sol_prop_Tinv_lin}(Solution to~\ref{prop_Tinv_lin})
Since $T$ is bijective there exists a function $T^{-1} \colon W \sto V$ satisfying $T^{-1}
\circ T = I_V$ and $T \circ T^{-1} = I_W$.  We must show that this function is linear.  To
this end let $u,v \in W$.  Then
  \begin{align*}
      T(T^{-1}(u + v)) &= I_W(u + v) \\
                       &= u + v \\
                       &= I_W(u) + I_W(v) \\
                       &= T(T^{-1}(u) + T(T^{-1}(v) \\
                       &= T(T^{-1}u + T^{-1}v).
  \end{align*}
Since $T$ is injective the preceding computation implies that
  \[ T^{-1}(u+v) = T^{-1}u + T^{-1}v.\]
Similarly, from
  \[ TT^{-1}(\alpha x) = \alpha x = \alpha TT^{-1}x
                      = T(\alpha T^{-1}x) \]
we infer that
  \[ T^{-1}(\alpha x) = \alpha T^{-1}x. \qedhere \]
\end{prf}

\begin{prf}\label{sol_ex_mat_add}(Solution to~\ref{ex_mat_add})
 \begin{align*}
        a + b &= \begin{bmatrix}  5 & -3 &  3 & -2 \\
                                  2 & -2 &  1 &  4 \end{bmatrix}
           3a &= \begin{bmatrix} 12 &  6 &  0 & -3 \\
                                 -3 & -9 &  3 & 15 \end{bmatrix}
       a - 2b &= \begin{bmatrix}  2 & 12 & -6 &  1 \\
                                 -7 & -5 &  1 &  7 \end{bmatrix}\,.  \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_ex_mat_mult}(Solution to~\ref{ex_mat_mult})
$ab = \begin{bmatrix}
         2(1) + 3(2) + (-1)1  &  2(0) + 3(-1) + (-1)(-2) \\
         0(1) + 1(2) +  4(1)  &  0(0) + 1(-1) +   4(-2)
      \end{bmatrix} =
 \begin{bmatrix}
        7 & -1 \\
        6 & -9
 \end{bmatrix}$\,.
\end{prf}

\begin{prf}\label{sol_ex_mat_act}(Solution to~\ref{ex_mat_act})
 $ax = (1,4,1)$.
\end{prf}

\begin{prf}\label{sol_prop_matr_arith}(Solution to~\ref{prop_matr_arith}(a))
To show that the vectors $a(x+y)$ and $ax + ay$ are equal show that $(a(x + y))_j$ (that is,
the $j^{\text{th}}$ component of $a(x + y)$) is equal to $(ax + ay)_j$ (the $j^{\text{th}}$
component of $ax + ay$) for each $j$ in~$\N_m$. This is straight forward
 \begin{align*}
   (a(x + y))_j &= \sum_{k=1}^n a_k^j(x + y)_k \\
                &= \sum_{k=1}^n a_k^j(x_k + y_k) \\
                &= \sum_{k=1}^n (a_k^j x_k + a_k^j y_k) \\
                &= \sum_{k=1}^n a_k^j x_k + \sum_{k=1}^na_k^j y_k \\
                &= (ax)_j + (ay)_j \\
                &= (ax + ay)_j.   \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_ex_mat_act3}(Solution to~\ref{ex_mat_act3})
 \begin{align*}
    xay &= \begin{bmatrix} 1 & -2 &  0 \end{bmatrix}
           \begin{bmatrix}
                1 &  3 & -1 \\
                0 &  2 &  4 \\
                1 & -1 &  1
           \end{bmatrix}
           \begin{bmatrix} 3 \\ 0 \\ 1 \end{bmatrix} \\
        &= \begin{bmatrix} 1 & -2 &  0 \end{bmatrix}
           \begin{bmatrix} 2 \\ 4 \\ 4 \end{bmatrix} = -6\,.  \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_mmult_uniq_inv}(Solution to~\ref{mmult_uniq_inv})
Suppose $a$ is an $n \times n$-matrix with inverses $b$ and~$c$.  Then $ab = ba = I_n$ and $ac
= ca = I_n$.  Thus
  \[ b = bI_n = b(ac) = (ba)c = I_nc = c\,.   \qedhere \]
\end{prf}

\begin{prf}\label{sol_ex_mat_inv}(Solution to~\ref{ex_mat_inv})
Multiply $a$ and $b$ to obtain $ab = I_3$ and $ba = I_3$.  By the uniqueness of inverses
(proposition~\ref{mmult_uniq_inv}) $b$ is the inverse of~$a$.
\end{prf}

\begin{prf}\label{sol_ex_mat_inv2}(Solution to~\ref{ex_mat_inv2})
Expanding the determinant of $a$ along the first row (fact 5) we obtain
 \begin{align*}
            \det a &= \sum_{k=1}^3 a_k^1 C_k^1 \\
                   &= 1 \cdot (-1)^{1+1} \det\begin{bmatrix}
                                 3 & -1 \\
                                -1 &  1 \end{bmatrix}
                    + 0 \cdot (-1)^{1+2} \det\begin{bmatrix}
                                 0 & -1 \\
                                 1 &  1 \end{bmatrix}
                    + 2 \cdot (-1)^{1+3} \det\begin{bmatrix}
                                 0 &  3 \\
                                 1 & -1 \end{bmatrix} \\
                   &= 2 + 0 - 6 = -4.
 \end{align*}
Since $\det a \ne 0$, the matrix $a$ is invertible.  Furthermore,
 \begin{align*}
        a^{-1} &= (\det a)^{-1} \begin{bmatrix}
                    C_1^1 & C_2^1 & C_3^1 \\
                    C_1^2 & C_2^2 & C_3^2 \\
                    C_1^3 & C_2^3 & C_3^3 \end{bmatrix}^t \\
                &= -\frac14 \begin{bmatrix}
                    C_1^1 & C_1^2 & C_1^3 \\
                    C_2^1 & C_2^2 & C_2^3 \\
                    C_3^1 & C_3^2 & C_3^3 \end{bmatrix}^t \\
                &= -\frac14 \begin{bmatrix}
                     2 & -2 & -6 \\
                    -1 & -1 &  1 \\
                    -3 &  1 &  3 \end{bmatrix} \\
                &= \begin{bmatrix}
                    -\frac12 &  \frac12 &  \frac32 \\
                     \frac14 &  \frac14 & -\frac14 \\
                     \frac34 & -\frac14 & -\frac34 \end{bmatrix}.   \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_mrlt_exam}(Solution to~\ref{exer_mrlt_exam})
Since $Te^1 = T(1,0) = (1, 0, 2, -4)$ and $Te^2 = T(0,1) = (-3, 7, 1, 5)$ the matrix
representation of $T$ is given by
   \[ [T] =  \begin{bmatrix}  1 & -3 \\
                              0 &  7 \\
                              2 &  1 \\
                             -4 &  5
             \end{bmatrix}.  \qedhere  \]
\end{prf}

\begin{prf}\label{sol_prop_mrT_is_T}(Solution to~\ref{prop_mrT_is_T})
Let $a = [T]$.  Then $a_k^j = (Te^k)_j$ for each $j$ and~$k$.  Notice that the map
  \[ S \colon \R^n \sto \R^m \colon  x \mapsto ax \]
is linear by proposition~\ref{prop_matr_arith} (a) and~(b).  We wish to show that $S = T$.
According to problem~\ref{prob_lt_det_bas} (b) it suffices to show that $Se^k = Te^k$ for $1
\le k \le n$.  But this is essentially obvious: for each $j \in \N_m$
 \[ (Se^k)_j = (ae^k)_j = \sum_{l=1}^n a_l^j e_l^k
                                = a_k^j = (Te^k)_j. \]

To prove the last assertion of the proposition, suppose that $Tx = ax$ for all $x$ in $\R^n$.
By the first part of the proposition $[T]x = ax$ for all $x$ in $\R^n$.  But then
proposition~\ref{prop_mrlt_bas} implies that $[T] = a$.
\end{prf}

\begin{prf}\label{sol_prop_matrep_bij}(Solution to~\ref{prop_matrep_bij})
First we show that the map $T \mapsto [T]$ is surjective.  Given an $m \times n$-matrix $a$ we
wish to find a linear map $T$ such that $a = [T]$. This is easy: let $T \colon \R^n \sto \R^m
\colon  x \mapsto ax$.  By proposition~\ref{prop_matr_arith} (a) and (b) the map $T$ is
linear.  By proposition~\ref{prop_mrT_is_T}
  \[ {[T]x} = Tx = ax \qquad \text{for all $x \in \R^n$.} \]
Then proposition~\ref{prop_mrlt_bas} tells us that $[T] = a$.

Next we show that the map $T \mapsto [T]$ is injective.  If $[T] = [S]$, then by
proposition~\ref{prop_mrT_is_T}
  \[ Tx = [T]x = [S]x = Sx \qquad \text{for all $x \in \R^n$.} \]
This shows that $T = S$.
\end{prf}

\begin{prf}\label{sol_prop_arith_matrep}(Solution to~\ref{prop_arith_matrep})
By proposition~\ref{prop_mrlt_bas} it suffices to show that $[S + T]x = ([S] + [T])x$ for all
$x$ in $\R^n$.  By proposition~\ref{prop_mrT_is_T}
  \[ [S + T]x = (S + T)x = Sx + Tx = [S]x + [T]x = ([S] + [T])x\,. \]
The last step uses proposition~\ref{prop_matr_arith}(c).

(b) Show that $[\alpha T]x = (\alpha [T])x$ for all $x$ in~$\R^n$.
  \[ [\alpha T]x = (\alpha T)x = \alpha(Tx) = \alpha([T]x) = (\alpha[T])x\,. \]
The last step uses proposition~\ref{prop_matr_arith}(d).
\end{prf}





%END chapter 21
















\section{Exercises in chapter 22}

\begin{prf}\label{sol_exer_nls_r34}(Solution to~\ref{exer_nls_r34})
Here of course we use the usual norm on~$\R^4$.
 \begin{align*}
   \norm{f(a+ \lambda h)}
         &= \norm{f\bigl((4,2,-4) + (-\frac12)(2,4,-4)\bigr)} \\
         &= \norm{f(3,0,-2)} \\
         &= \norm{(-6,9,3,-3\sqrt2)} \\
         &= 3\bigl[(-2)^2 + 3^2 + 1^2 + (-\sqrt2)^2\bigr]^{1/2} \\
         &= 12      \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_nls_r32}(Solution to~\ref{exer_nls_r32})
Since
  \begin{align*}
    f(a + h) - f(a) - mh
       &= f(1+h_1, h_2, -2+h_3) - f(1,0,-2)
             - \begin{bmatrix}
                    6  &  0  &  0  \\
                    0  &  1  & -1
               \end{bmatrix}
               \begin{bmatrix}
                    h_1 \\
                    h_2 \\
                    h_3
               \end{bmatrix} \\
        &= \bigl(3(1+2h_1+{h_1}^2), h_2 + h_1h_2 + 2 - h_3\bigr)
                - (3,2) - (6h_1,h_2 - h_3) \\
        &= (3{h_1}^2,h_1h_2),
  \end{align*}
we have
  \[ \norm{f(a+h) - f(a) - mh} = (9{h_1}^4 + {h_1}^2{h_2}^2)^{1/2}
                               = \abs{h_1}(9{h_1}^2+{h_2}^2)^{1/2}\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_un_sin}(Solution to~\ref{exer_un_sin})
Using the second derivative test from beginning calculus (and checking the value of $f + g$ at
the endpoints of the interval) we see that $f(x) + g(x)$ assumes a maximum value of $\sqrt2$
at $x = \pi/4$ and a minimum value of $-\sqrt2$ at $x = 5\pi/4$.  So
  \[ \norm{f+g}_u = \sup\{\abs{f(x)+g(x)}\colon 0 \le x \le 2\pi\} = \sqrt2\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_norm_obv}(Solution to~\ref{prop_norm_obv})
Let $x \in V$.  Then
 \begin{enumerate}
  \item[(a)] $\norm{\vc 0} = \norm{0 \cdot x} = \abs 0\, \norm x = 0$.
  \item[(b)] $\norm{-x} = \norm{(-1)x} = \abs{-1}\,\norm x  = \norm x$.
  \item[(c)] $0 = \norm{\vc 0} = \norm{x + (-x)} \le \norm x + \norm{-x} = 2\norm x$. \qedhere
 \end{enumerate}
\end{prf}

\begin{prf}\label{sol_prop_ind_metr}(Solution to~\ref{prop_ind_metr}(a))
By proposition~\ref{prop_norm_obv}(b) it is clear that $\norm x < r$ if and only if $\norm{-x}
< r$.  That is, $d(x,\vc 0) < r$ if and only $d(-x,\vc 0) < r$.  Therefore, $x \in B_r(\vc 0)$
if and only if $-x \in B_r(\vc 0)$, which in turn holds if and only if $x = -(-x) \in -B_r(\vc 0)$.
\end{prf}

\begin{prf}\label{sol_prop_norm_equiv}(Solution to~\ref{prop_norm_equiv})
Suppose $\norm{\quad}_1$ and $\norm{\quad}_2$ are equivalent norms on~V. Let $d_1$ and $d_2$
be the metrics induced by $\norm{\quad}_1$ and $\norm{\quad}_2$, respectively.  (That is,
$d_1(x,y) = \norm{x - y}_1$ and $d_2(x,y) = \norm{x - y}_2$ for all $x$, $y \in V$.) Then
$d_1$ and $d_2$ are equivalent metrics.  Thus there exist $\alpha, \beta > 0$ such that
$d_1(x,y) \le \alpha d_2(x,y)$ and $d_2(x,y) \le \beta d_1(x,y)$ for all $x$, $y \in V$.  Then
in particular
  \[ \norm x_1 = \norm{x - \vc 0}_1 = d_1(x,\vc 0) \le \alpha d_2(x,\vc 0)
                               = \alpha \norm{x - \vc 0}_2  = \alpha \norm x_2 \]
and similarly
  \[ \norm x_2 = d_2(x,\vc 0) \le \beta d_1(x,\vc 0) = \beta \norm x_1 \]
for all $x$ in~$V$. Conversely, suppose there exist  $\alpha$, $\beta > 0$ such that $\norm
x_1 \le \alpha \norm x_2$ and $\norm x_2 \le \beta \norm x_1$ for all $x$ in~$V$.  Then for
all $x$, $y \in V$
  \[ d_1(x,y) = \norm{x - y}_1 \le \alpha \norm{x - y}_2 = \alpha d_2(x,y) \]
and similarly
  \[ d_2(x,y) \le \beta d_1(x,y)\,. \]
Thus $d_1$ and $d_2$ are equivalent metrics.
\end{prf}

\begin{prf}\label{sol_prop_scmlt_cont}(Solution to~\ref{prop_scmlt_cont})
Let $f\colon \R \times V \sto V\colon (\beta,x) \mapsto \beta x$.  We show that f is
continuous at an arbitrary point $(\alpha,a)$ in $\R \times V$. Given $\epsilon > 0$ let $M$
be any number larger than both $\abs\alpha$ and $\norm a + 1$.  Choose $\delta =
\min\{1,\epsilon/M\}$. Notice that
  \begin{align*}
    \abs{\beta - \alpha} + \norm{x-a}
             &= \norm{(\beta - \alpha, x - a)}_1  \\
             &= \norm{(\beta,x) - (\alpha,a)}_1 .
 \end{align*}
Thus whenever $\norm{(\beta,x) - (\alpha,a)}_1 < \delta$  we have
  \[ \norm x \le \norm a + \norm{x-a} < \norm a + \delta \le \norm a + 1 \le M \]
so that
  \begin{align*}
     \norm{f(\beta,x) - f(\alpha,a)}
       &= \norm{\beta x - \alpha a}  \\
       &\le \norm{\beta x - \alpha x} + \norm{\alpha x - \alpha a} \\
       &= \abs{\beta - \alpha}\,\norm x + \abs{\alpha}\,\norm{x - a} \\
       &\le  M(\abs{\beta - \alpha} + \norm{x - a}) \\
       &< M\,\delta \\
       &\le \epsilon.  \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_cor_scmlt}(Solution to~\ref{cor_scmlt})
If $\beta_n \sto \alpha$ in $\R$ and $x_n \sto a$ in~$V$, then $(\beta_n,x_n) \sto (\alpha,a)$
in $\R \times V$ by proposition~\ref{seq_prod}). According to the previous proposition
  \[ f\colon \R \times V \sto V\colon (\beta,x) \mapsto \beta x \]
is continuous.  Thus it follows immediately from proposition~\ref{seq_ch_cont} that
  \[ \beta_n x_n =f(\beta_n,x_n) \sto f(\alpha,a) = \alpha a\,. \]
\end{prf}




\begin{prf}\label{sol_exer_B_vs}(Solution to~\ref{exer_B_vs})
We know from example~\ref{vs_exam1} that $\fml F(S,V)$ is a vector space.  We show that $\fml
B(S,V)$ is a vector space by showing that it is a subspace of $\fml F(S,V)$.  Since $\fml
B(S,V)$ is nonempty (it contains every constant function), we need only verify that $f + g$
and $\alpha f$ are bounded when $f$, $g \in \fml B(S,V)$ and $\alpha \in \R$.  There exist
constants $M$, $N > 0$ such that $\norm{f(x)} \le M$ and $\norm{g(x)} \le N$ for all $x$
in~$S$. But then
  \[ \norm{(f+g)(x)} \le \norm{f(x) + g(x)} \le \norm{f(x)} + \norm{g(x)} \le M + N \]
and
  \[ \norm{(\alpha f)(x)} = \norm{\alpha f(x)} = \abs \alpha\,\norm{f(x)} \le \abs\alpha\,M\,. \]
Thus the functions $f+g$ and $\alpha f$ are bounded.
\end{prf}





%END chapter 22
















\section{Exercises in chapter 23}

\begin{prf}\label{sol_prop_equiv_cont}(Solution to~\ref{prop_equiv_cont})

$\text{(a)}\implies \text{(b)}$: Obvious.

$\text{(b)} \implies \text{(c)}$: Suppose $T$ is continuous at a point $a$ in $V$.
Given $\epsilon > 0$ choose $\delta > 0$ so that $\norm{Tx - Ta} < \epsilon$ whenever $\norm{x
- a} < \delta$.  If $\norm{h - \vc 0} = \norm h < \delta$ then $\norm{(a + h) - a} < \delta$
and $\norm{Th - T\vc 0} = \norm{Th} = \norm{T(a + h) - Ta} < \epsilon$.  Thus $T$ is
continuous at~$\vc 0$.

$\text{(c)} \implies \text{(d)}$: Argue by contradiction. Assume that $T$ is
continuous at $\vc 0$ but is not bounded.  Then for each $n \in \N$ there is a vector $x_n$ in
$V$ such that $\norm{Tx_n} > n\norm{x_n}$.  Let $y_n = \bigl(n\norm{x_n}\bigr)^{-1}x_n$.  Then
$\norm{y_n} = n^{-1}$; so $y_n \sto \vc 0$.  Since $T$ is continuous at $\vc 0$ we conclude
that $Ty_n \sto \vc 0$ as $n \sto \infty$.  On the other hand
  \[ \norm{Ty_n} = \bigl(n\norm{x_n}\bigr)^{-1}\norm{Tx_n} > 1 \]
for every~$n$.  This shows that $Ty_n \not\sto \vc 0$ as $n \sto \infty$, which contradicts
the preceding assertion.

$\text{(d)} \implies \text{(a)}$: If $T$ is bounded, there exists $M > 0$ such
that $\norm{Tx} \le M\norm x$ for all $x$ in~$V$.  It is easy to see that $T$ is
continuous at an arbitrary point $a$ in~$V$.  Given $\epsilon > 0$ choose $\delta =
\epsilon/M$. If $\norm{x - a} < \delta$, then
  \[ \norm{Tx - Ta} = \norm{T(x - a)} \le M\norm{x - a} < M\delta = \epsilon\,. \]
\end{prf}

\begin{prf}\label{sol_lem_equiv_norm}(Solution to~\ref{lem_equiv_norm})
The first equality is an easy computation.
  \begin{align*}
      \sup\{ \norm x^{-1}\,\norm{Tx}\colon x \ne \vc 0\}
            &= \inf\{M > 0 \colon M \ge \norm x^{-1}\norm{Tx} \text{for all $x \ne \vc 0$}\} \\
            &= \inf\{M > 0\colon \norm{Tx} \le M\norm x \text{for all $x$}\} \\
            &= \norm T
  \end{align*}
The second is even easier.
  \begin{align*}
      \sup\{ \norm x^{-1}\,\norm{Tx} \colon x \ne \vc 0\}
                     &= \sup\{\norm{T\bigl(\norm x^{-1} x\bigr)}\} \\
                     &= \sup\{\norm{Tu} \colon \norm u = 1\}.
  \end{align*}
To obtain the last equality notice that since
  \[ \{\norm{Tu} \colon \norm u = 1\} \subseteq \{\norm{Tx} \colon \norm x \le 1\} \]
it is obvious that
  \[ \sup\{\norm{Tu} \colon \norm u = 1\} \le \sup\{\norm{Tx} \colon \norm x \le 1\}\,. \]
On the other hand, if $\norm x \le 1$ and $x \ne 0$, then $v := \norm x^{-1}x$ is a unit
vector, and so
  \begin{align*}
      \norm{Tx} &\le \norm x^{-1}\norm{Tx}  \\
                &= \norm{Tv} \\
                &\le \sup\{\norm{Tu} \colon \norm u = 1\}.
  \end{align*}
Therefore
  \[ \sup\{\norm{Tx}\colon \norm x \le 1\} \le \sup\{\norm{Tu} \colon \norm u = 1\}\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_exam_norm}(Solution to~\ref{exer_exam_norm})

(a) Let $I\colon V \sto V \colon x \mapsto x$.  Then (by lemma~\ref{lem_equiv_norm})
  \[ \norm I = \sup\{\norm{Ix} \colon \norm x = 1\} = \sup\{\norm x \colon \norm x = 1\} = 1\,. \]

(b) Let $\wh{\vc 0} \colon V \sto W \colon x \mapsto \vc 0$. Then $\norm{\wh{\vc 0}} =
\sup\{\norm{\wh{\vc 0}x} \colon \norm x = 1\} = \sup\{0\} = 0.$

(c) We suppose $k = 1$. (The case $k = 2$ is similar.) Let $x$ be a nonzero vector in $V_1$
and $u = \norm x^{-1}x$. Since $\norm{(u,\vc 0)}_1 = \norm u + \norm{\vc 0} = \norm u = 1$, we
see (from lemma~\ref{lem_equiv_norm}) that
  \begin{align*}
     \norm{\pi_1}
         &= \sup\{\norm{\pi_1(x_1,x_2)} \colon \norm{(x_1,x_2)} = 1\} \\
         &\ge \norm{\pi_1(u,\vc 0)}  \\
         &= \norm u  \\
         &= 1.
  \end{align*}
On the other hand since $\norm{\pi_1(x_1,x_2)} = \norm{x_1} \le \norm{(x_1,x_2)}_1$ for all
$(x_1,x_2)$ in $V_1 \times V_2$, it follows from the definition of the norm of a
transformation that $\norm{\pi_1} \le 1$.
\end{prf}

\begin{prf}\label{sol_exer_norm_int}(Solution to~\ref{exer_norm_int})
Let $f$, $g \in \fml C$ and $\alpha \in \R$.  Then
  \[ J(f + g) = \int_a^b (f(x) + g(x))\,dx
              = \int_a^b f(x)\,dx + \int_a^b g(x)\,dx
              = Jf + Jg\]
and
  \[ J(\alpha f) = \int_a^b \alpha f(x)\,dx
               = \alpha\int_a^b f(x)\,dx = \alpha Jf.\]
Thus $J$ is linear.  If $f \in \fml C$, then
  \[\abs{Jf} = \abs{\int_a^b f(x)\,dx}
             \le \int_a^b \abs{f(x)}\,dx
             \le \int_a^b \norm f_u\,dx
             = (b - a)\norm f_u\,. \]
This shows that $J$ is bounded and that $\norm J \le b - a$.  Let $g(x) = 1$ for all $x$ in
$[a,b]$.  Then $g$ is a unit vector in $\fml C$ (that is, $\norm g_u = 1$) and $Jg = \int_a^b
g(x)\,dx = b - a$.  From lemma~\ref{lem_equiv_norm} we conclude that $\norm J \ge b - a$. This
and the preceding inequality prove that $\norm J = b - a$.
\end{prf}

\begin{prf}\label{sol_prop_opnorm}(Solution to~\ref{prop_opnorm})
It was shown in proposition~\ref{endo_vs} that $\ofml L(V,W)$ is a vector space. Since $\ofml
B(V,W)$ is a nonempty subset of $\ofml L(V,W)$ [it contains the zero transformation], we need
only show that sums and scalar multiples of bounded linear maps are bounded in order to
establish that $\ofml B(V,W)$ is a vector space. This is done below in the process of showing
that the map $T \mapsto \norm T$ is a norm.

Let $S$, $T \in \ofml B(V,W)$ and $\alpha \in \R$.  To show that $\norm{S + T} \le \norm S +
\norm T$ and $\norm{\alpha T} = \abs{\alpha} \norm T$ we make use of the characterization
$\norm T = \sup\{\norm{Tu}\colon \norm u = 1\}$ given in lemma~\ref{lem_equiv_norm}.  If $v$
is a unit vector in $V$, then
  \begin{align*}
      \norm{(S + T)v} &= \norm{Sv + Tv}  \\
                      &\le \norm{Sv} + \norm{Tv}  \\
                      &\le \sup\{\norm{Su} \colon \norm u = 1\} + \sup\{\norm{Tv} \colon \norm v = 1\} \\
                      &= \norm S + \norm T .
 \end{align*}
This shows that $S + T$ is bounded and that
  \begin{align*}
      \norm{S + T} &= \sup\{\norm{(S + T)v} \colon \norm v = 1\} \\
                   &\le \norm S + \norm T .
  \end{align*}
Also
  \begin{align*}
        \sup\{\norm{\alpha Tv} \colon \norm v = 1\}
                  &= \abs{\alpha} \sup\{\norm{Tv} \colon \norm v = 1\} \\
                  &= \abs{\alpha} \norm T ,
  \end{align*}
which shows that $\alpha T$ is bounded and that $\norm{\alpha T} = \abs{\alpha}\norm T$.

Finally, if $\sup\{\norm x^{-1}\norm{Tx}\colon x \ne \vc 0\} = \norm T = 0$, then $\norm
x^{-1}\norm{Tx} = 0$ for all $x$ in $V$, so that $Tx = \vc 0$ for all $x$ and therefore $T =
\vc 0$.
\end{prf}

\begin{prf}\label{sol_prop_comp_op}(Solution to~\ref{prop_comp_op})
The composite of linear maps is linear by proposition~\ref{prop_comp_lt}.  From
corollary~\ref{cor_norm_op} we have
  \[ \norm{TSx} \le \norm T \, \norm{Sx} \le \norm T \, \norm S \, \norm x \]
for all $x$ in~$U$.  Thus $TS$ is bounded and $\norm{TS} \le \norm T \, \norm S$.
\end{prf}

\begin{prf}\label{sol_prop_SW_prep}(Solution to~\ref{prop_SW_prep})
First deal with the case $\norm f_u \le 1$. Let $(p_n)$ be a sequence of polynomials on
$[0,1]$ which converges uniformly to the square root function (see~\ref{exam_approx_sqrt}).
Given $\epsilon > 0$, choose $n_0 \in \N$ so that $n \ge n_0$ implies $\abs{p_n((t) - \sqrt t}
\le \epsilon$ for all $t \in [0,1]$.  Since $\norm f_u \le 1$
  \[ \bigabs{p_n\bigl([f(x)]^2\bigr) - \abs{f(x)}}
             = \bigabs{p_n\bigl([f(x)]^2\bigr) - \sqrt{[f(x)]^2}}
             < \epsilon \]
whenever $x \in M$ and $n \ge n_0$.  Thus $p_n \circ f^2 \sto  \abs f\text{\,(unif)}$. For
every $n \in \N$ the function $p_n \circ f^2$ belongs to~$A$.  Consequently, $\abs f$ is the
uniform limit of functions in $A$ and therefore belongs to~$\clo A$.

If $\norm f_u > 1$ replace $f$ in the argument above by $g = f/\norm f_u$.
\end{prf}

\begin{prf}\label{sol_lem_SW_prep}(Solution to~\ref{lem_SW_prep})
Let $f \in \fml C(M,\R)$, $a \in M$, and $\epsilon > 0$.  According to
proposition~\ref{prop_sep_subalg} we can choose, for each $y \ne a$ in $M$, a function $\phi_y
\in A$ such that
  \[ \phi_y(a) = f(a) \qquad \text{ and } \qquad \phi_y(y) = f(y)\,. \]
And for $y = a$ let $\phi_y$ be the constant function whose value is $f(a)$. Since in either
case $\phi_y$ and $f$ are continuous functions which agree at $y$, there exists a neighborhood
$U_y$ of $y$ such that
  \[ \phi_y(x) < f(x) + \epsilon \]
for all $x \in U_y$. Clearly $\{U_y \colon y \in M\}$ covers $M$. Since $M$ is compact there
exist points $y_1, \dots, y_n$ in $M$ such that the family $\{U_{y_1}, \dots, U_{y_n}\}$
covers $M$. Let $g = \phi_{y_1} \land \dots \land \phi_{y_n}$. By corollary~\ref{cor_sup_clo}
the function $g$ belongs to $\clo A$. Now $g(a) = \phi_{y_1}(a) \land \dots \land
\phi_{y_n}(a) = f(a)$. Furthermore, given any $x$ in $M$ there is an index $k$ such that $x
\in U_{y_k}$. Thus
  \[ g(x) \le \phi_{y_k}(x) < f(x) + \epsilon \,. \]
\end{prf}

\begin{prf}\label{sol_prop_BVW_compl}(Solution to~\ref{prop_BVW_compl})
Let $(T_n)$ be a Cauchy sequence in the normed linear space $\ofml B(V,W)$.  For each $x$ in
$V$
  \[ \norm{T_mx - T_nx} \le  \norm{T_m - T_n}\,\norm x \sto 0 \]
as $m$, $n \sto \infty$.  Thus $(T_nx)$ is a Cauchy sequence in $W$ for each~$x$.  Since $W$
is complete, there exists a vector $Sx$ in $W$ such that $T_nx \sto Sx$.  Define the map
  \[ S \colon V \sto W \colon x \mapsto Sx\,. \]
It is easy to see that $S$ is linear: $S(x + y) = \lim T_n(x + y) = \lim(T_nx + T_ny) = \lim
T_nx + \lim T_ny = Sx + Sy$; $S(\alpha x) = \lim T_n(\alpha x) = \alpha \lim T_nx = \alpha
Sx$. For every $\epsilon > 0$ there exists $N \in \N$ such that $\norm{T_m - T_n} <
\frac12\epsilon$ whenever $m$, $n \ge N$.  Then for all such $m$ and $n$ and for all $x$ in
$V$
  \begin{align*}
        \norm{(S - T_n)x}
              &= \norm{Sx - T_nx} \\
              &\le \norm{Sx - T_mx} + \norm{T_mx - T_nx} \\
              &\le \norm{Sx - T_mx} + \norm{T_m - T_n}\norm x \\
              &\le \norm{Sx - T_mx} + \tfrac12\epsilon\norm x.
  \end{align*}
Taking limits as $m \sto \infty$ we obtain
  \[ \norm{(S - T_n)x} \le \tfrac12\epsilon\norm x \]
for all $n \ge N$ and $x \in V$.  This shows that $S - T_n$ is bounded and that $\norm{S -
T_n} \le \frac12\epsilon < \epsilon$ for $n \ge N$.  Therefore the transformation
  \[ S = (S - T_n) + T_n \]
is bounded and
  \[ \norm{S - Tn} \sto 0 \]
as $n \sto \infty$.  Since the Cauchy sequence $(T_n)$ converges in the space $\ofml B(V,W)$,
that space is complete.
\end{prf}

\begin{prf}\label{sol_prop_adj1}(Solution to~\ref{prop_adj1})
We wish to show that if $g \in W^*$, then $T^*g \in V^*$.  First we check linearity: if $x$,
$y \in V$ and $\alpha \in \R$, then
  \begin{align*}
      (T^*g)(x + y) &= gT(x + y)  \\
                    &= g(Tx + Ty) \\
                    &= gTx + gTy \\
                    &= (T^*g)(x) + (T^*g)(y)
  \end{align*}
and
 \[ (T^*g)(\alpha x) = gT(\alpha x) = g(\alpha Tx) = \alpha gTx = \alpha(T^*g)(x)\,. \]
To see that $T^*g$ is bounded use corollary~\ref{cor_norm_op}.  For every $x$ in $V$
  \[ \abs{(T^*g)(x)} = \abs{gTx} \le \norm g \, \norm{Tx}
                               \le \norm g \, \norm T \, \norm x\,. \]
Thus $T^*g$ is bounded and $\norm{T^*g} \le \norm T \, \norm g$.
\end{prf}






%END chapter 23

















\section{Exercises in chapter 24}

\begin{prf}\label{sol_exam_ucont}(Solution to~\ref{exam_ucont})
Given $ \epsilon > 0$ choose $\delta = \epsilon$. Assume $\abs{x - y} < \delta$.  Then
  \[ \abs{f(x) - f(y)} = \bigabs{\frac1x - \frac1y} = \bigabs{\frac{y - x}{xy}}
           = \bigabs{\frac{x - y}{xy}} \le \abs{x - y} < \delta = \epsilon\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exam_not_ucont}(Solution to~\ref{exam_not_ucont})
We must show that
  \[ (\exists\epsilon > 0)(\forall\delta > 0)(\exists x,y \in (0,1])
        \abs{x - y} < \delta \text{ and } \abs{f(x) - f(y)} \ge \epsilon\,. \]
Let $\epsilon = 1$.  Suppose $\delta > 0$.  Let $\delta_0 = \min\{1,\delta\}$, $x = \frac12
\delta_0$, and $y = \delta_0$. Then $x$, $y \in (0,1]$, $\abs{x - y} = \frac12\delta_0 \le
\frac12\delta < \delta$, and $\abs{f(x) - f(y)} = \abs{\frac2{\delta_0} - \frac1{\delta_0}} =
\frac1\delta_0 \ge 1$.
\end{prf}

\begin{prf}\label{sol_lem_conv_subseq}(Solution to~\ref{lem_conv_subseq})
Since $M$ is compact it is sequentially compact (by theorem~\ref{cpt_scpt}).  Thus the
sequence $(x_n)$ has a convergent subsequence $(x_{n_k})$.  Let $a$ be the limit of this
subsequence.  Since for each $k$
 \[ d\bigl(y_{n_k},a\bigr)
          \le d\bigl(y_{n_k}, x_{n_k}\bigr) + d\bigl(x_{n_k},a\bigr) \]
and since both sequences on the right converge to zero, it follows that $y_{n_k} \sto a$ as $k
\sto \infty$.
\end{prf}

\begin{prf}\label{sol_prop_cont_ucont}(Solution to~\ref{prop_cont_ucont})
Assume that $f$ is not uniformly continuous. Then there is a number $\epsilon > 0$ such that
for every $n$ in $\N$ there correspond points $x_n$ and $y_n$ in $M_1$ such that $d(x_n,y_n) <
1/n$ but $d\bigl(f(x_n), f(y_n)\bigr) \ge \epsilon$.  By lemma~\ref{lem_conv_subseq} there
exist subsequences $\bigl(x_{n_k}\bigr)$ of $(x_n)$ and $\bigl(y_{n_k}\bigr)$ of $(y_n)$ both
of which converge to some point $a$ in~$M_1$.  It follows from the continuity of $f$ that for
some integer $k$ sufficiently large, $d\bigl(f(x_{n_k}), f(a)\bigr) < \epsilon/2$ and
$d\bigl(f(y_{n_k}),f(a)\bigr) < \epsilon/2$. This contradicts the assertion that
$d\bigl(f(x_n),f(y_n)\bigr) \ge \epsilon$ for every $n$ in $\N$.
\end{prf}

\begin{prf}\label{sol_lem_ucont_lim}(Solution to~\ref{lem_ucont_lim})
By hypothesis there exists a point $a$ in $M_1$ such that $x_n \sto a$ and $y_n \sto a$. It is
easy to see that the ``interlaced'' sequence $(z_n) = (x_1, y_1, x_2, y_2, x_3, y_3,\dots)$
also converges to~$a$. By proposition~\ref{conv_cau} the sequence $(z_n)$ is Cauchy (in $M_1$
and therefore) in $S$, and by proposition~\ref{prop_ucont_Cauchy} (applied to the metric
space~$S$) the sequence $\bigl(f(z_n)\bigr)$ is Cauchy in~$M_2$.  The sequence
$\bigl(f(x_n)\bigr)$ is a subsequence of $\bigl(f(z_n)\bigr)$ and is, by hypothesis,
convergent.  Therefore, according to proposition~\ref{cs_css}, the sequence
$\bigl(f(z_n)\bigr)$ converges and
  \[ \lim f(x_n) = \lim f(z_n) = \lim f(y_n)\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_partition}(Solution to~\ref{exer_partition})
Just take the union of the sets of points in $P$ and $Q$ and put them in increasing order.
Thus
  \[ P \lor Q = \bigl(0, \tfrac15, \tfrac14, \tfrac13, \tfrac12,
                                \tfrac23, \tfrac34, \tfrac56, 1\bigr)\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_int_stepf}(Solution to~\ref{exer_int_stepf})
(a) Either sketch the graph of $\sigma$ or reduce the function algebraically to obtain
  \[ \sigma  = -\chi_{{}_\sst{\{2\}}} -2\chi_{{}_\sst{(2,3)}} - \chi_{{}_\sst{\{5\}}}\,. \]
Then the partition associated with $\sigma$ is $P = (0,2,3,5)$.

(b) $\sigma_Q = (0,0,-2,0,0)$.
\end{prf}

\begin{prf}\label{sol_exer_int_stepf2}(Solution to~\ref{exer_int_stepf2})
The values of $\sigma$ on the subintervals of $P$ are given by $\sigma_P = (0,-2,0)$. Multiply
each of these by the length of the corresponding subinterval:
  \[ \int_0^5\sigma = (2-0)(0) + (3-2)(-2) + (5-3)(0) = -2\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_lem_int_stepf}(Solution to~\ref{lem_int_stepf})
Perhaps the simplest way to go about this is to observe first that we can get from the
partition associated with $\sigma$ to the refinement $Q$ one point at a time.  That is, there
exist partitions
  \[ P_1 \preceq P_2 \preceq \dots \preceq P_r = Q \]
where $P_1$ is the partition associated with $\sigma$ and $P_{j+1}$ contains exactly one point
more than $P_j$ (for $1 \le j \le r-1$).

Thus it suffices to prove the following: If $\sigma$ is a step function on $[a,b]$, if $P =
(s_0, \dots, s_n)$ is a refinement of the partition associated with $\sigma$, and if $P
\preceq Q = (t_0, \dots,t_{n+1})$, then
  \[ \sum_{k=1}^{n+1}(\Delta t_k)y_k = \sum_{k=1}^n(\Delta s_k)x_k \]
where $\sigma_P = (x_1, \dots, x_n)$ and $\sigma_Q = (y_1, \dots, y_{n+1})$.

To prove this assertion, notice that since the partition $Q$ contains exactly one point more
than $P$, it must be of the form
  \[ Q = (s_0, \dots, s_{p-1}, u, s_p, \dots, s_n) \]
for some $p$ such that $1 \le p \le n$.  Thus
  \[y_k =
      \begin{cases} x_k, &\text{for $1 \le k \le p$}\\
                x_{k-1}, &\text{for $p+1 \le k \le n+1$.}
      \end{cases} \]
Therefore,
 \begin{align*}
    \sum_{k=1}^{n+1} (\Delta t_k)y_k
       &= \sum_{k=1}^{p-1} (\Delta t_k)y_k
             + (\Delta t_p)y_p + (\Delta t_{p+1})y_{p+1}
             + \sum_{k=p+2}^{n+1} (\Delta t_k)y_k \\
       &= \sum_{k=1}^{p-1} (\Delta s_k)x_k
             + (u - s_{p-1})x_p + (s_p - u)x_p
             + \sum_{k=p+2}^{n+1} (\Delta s_{k-1})x_{k-1} \\
       &= \sum_{k=1}^{p-1} (\Delta s_k)x_k
             + (s_p - s_{p-1})x_p
             + \sum_{k=p+1}^n (\Delta s_k)x_k \\
       &= \sum_{k=1}^n (\Delta s_k)x_k  \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_prop_int_adj_int}(Solution to~\ref{prop_int_adj_int})
That $\sigma$ is an $E$ valued step function on $[a,b]$ is obvious.  Let $Q = (u_0, \dots,
u_m)$ and $R = (v_0, \dots,  v_n)$ be the partitions associated with $\tau$ and $\rho$,
respectively; and suppose that $\tau_Q = (y_1, \dots, y_m)$ and $\rho_R = (z_1, \dots, z_n)$.
For $1 \le k \le m+n$, let
  \[ t_k =
      \begin{cases}  u_k, &\text{for $0 \le k \le m$} \\
                 v_{k-m}, &\text{for $m+1 \le k \le m+n$}
      \end{cases} \]
and $P = (t_0, \dots, t_{m+n})$.  Also define
  \[ x_k =
       \begin{cases}  y_k, &\text{for $1 \le k \le m$} \\
                  z_{k-m}, &\text{for $m+1 \le k \le m+n$}.
       \end{cases} \]
Then $P$ is a partition of $[a,b]$ and $\sigma_P = (x_1, \dots, x_{m+n})$.  Furthermore,
 \begin{align*}
      \int_a^b\sigma
             &= \sum_{k=1}^m+n (\Delta t_k)x_k \\
             &= \sum_{k=1}^m (\Delta t_k)x_k
                  + \sum_{k=m+1}^{m+n} (\Delta t_k)x_k \\
             &= \sum_{k=1}^m (\Delta u_k)y_k
                  + \sum_{k=m+1}^{m+n} (\Delta v_{k-m})z_{k-m} \\
             &= \sum_{k=1}^m (\Delta u_k)y_k
                  + \sum_{k=1}^{n} (\Delta v_k)z_k \\
             &= \int_a^c \tau + \int_c^b \rho.
 \end{align*}
(The third equality requires the observation that $\Delta t_{m+1} = v_1 - u_m = v_1 - c = v_1 - v_0$.)
\end{prf}

\begin{prf}\label{sol_prop_cont_reg}(Solution to~\ref{prop_cont_reg})
Let $f\colon [a,b] \sto E$ be continuous.  Given $\epsilon > 0$ we find a step function
$\sigma$ such that $\norm{f - \sigma}_u < \epsilon$. Since the domain of $f$ is compact,
proposition~\ref{prop_cont_ucont} guarantees that $f$ is uniformly continuous.  Thus there
exists $\delta > 0$ such that $\norm{f(u) - f(v)} < \epsilon/2$ whenever $u$ and $v$ are
points in $[a,b]$ such that $\abs{u-v} < \delta$.  Choose a partition $(t_0, \dots, t_n)$ of
$[a,b]$ so that $t_k - t_{k-1} < \delta$ for each $k = 1, \dots, n$.

Define $\sigma\colon [a,b] \sto E$ by $\sigma(s) = f(t_{k-1})$ if $t_{k-1} \le s \le t_k$ $(1
\le k \le n)$ and define $\sigma(b) = f(b)$.  It is easy to see that $\norm{f(s) - \sigma(s)}
< \epsilon/2$ for every $s$ in $[a,b]$.  Thus $\norm{f - \sigma}_u < \epsilon$; so $f$ belongs
to the closure of the family of step functions.
\end{prf}

\begin{prf}\label{sol_thm_Cint_blt}(Solution to~\ref{thm_Cint_blt})
Let $\clo{\fml S}$ be the closure of $\fml S([a,b],E)$ in the space $\fml B([a,b],E)$. If $g,
h \in \clo{\fml S}$, then there exist sequences $(\sigma_n)$ and $(\tau_n)$ of step functions
which converge uniformly to $g$ and $h$, respectively.  Then $(\sigma_n + \tau_n)$ is a
sequence of step functions and $\sigma_n + \tau_n \sto g + h \text{\,(unif)}$; so $g + h \in
\clo{\fml S}$.  Thus
 \begin{align*}
     \int(g + h)
            &= \lim \int(\sigma_n + \tau_n) \\
            &= \lim\left(\int\sigma_n + \int\tau_n\right) \\
            &= \lim\int\sigma_n + \lim\int\tau_n \\
            &= \int g + \int h.
 \end{align*}
Similarly, if $\alpha \in \R$, then $(\alpha \sigma_n)$ is a sequence of step functions which
converges to $\alpha g$.  Thus $\alpha g \in \clo{\fml S}$ and
  \[  \int(\alpha g) = \lim \int(\alpha \sigma_n) = \lim \left(\alpha\int \sigma_n\right)
            = \alpha \lim \int \sigma_n = \alpha \int g\,. \]
The map $\int\colon  \clo{\fml S} \sto E$ is bounded since it is both linear and uniformly
continuous (see \ref{thm_ext_uc} and~\ref{prop_lin_ucont}).
\end{prf}

\begin{prf}\label{sol_prop_int_compop}(Solution to~\ref{prop_int_compop})
The function $f$, being regulated, is the uniform limit in $\fml B([a,b],E)$ of a sequence
$(\sigma_n)$ of step functions.  Since $\int\sigma_n \sto \int f$ in $E$ and $T$ is
continuous, we see that
  \begin{equation}\label{eqn_compop1}
         T\left(\int f\right) = \lim T \left(\int\sigma_n\right)\,.
  \end{equation}
By problem~\ref{prob_int_compop} each $T \circ \sigma_n$ is an $F$ valued step function and
  \begin{equation}\label{eqn_compop2}
         \int(T \circ \sigma_n) = T\left(\int\sigma_n\right) \qquad \text{for each $n$.}
  \end{equation}
For every $t$ in $[a,b]$
  \begin{align*}
        \norm{(T \circ f - T \circ \sigma_n)(t)}  \\
                  &= \norm{T\bigl((f - \sigma_n)(t)\bigr)} \\
                  &\le \norm T \norm{(f - \sigma_n)(t)} \\
                  &\le \norm T \norm{f - \sigma_n}_u
  \end{align*}
so
  \[ \norm{T \circ f - T \circ \sigma_n}_u \le \norm T \norm{f - \sigma_n}_u\,. \]
Since $\norm{f - \sigma_n}_u \sto 0$, we conclude that
  \[ T \circ \sigma_n \sto T \circ f \text{(unif)} \]
in $\fml B([a,b],F)$.  Thus $T \circ f$ is regulated and
  \begin{equation}\label{eqn_compop3}
             \int(T \circ f) = \lim\int(T \circ \sigma_n).
  \end{equation}
The desired conclusion follows immediately from \eqref{eqn_compop1}, \eqref{eqn_compop2},
and~\eqref{eqn_compop3}.
\end{prf}





%END chapter 24
















\section{Exercises in chapter 25}

\begin{prf}\label{sol_prop_bdd_not_o}(Solution to~\ref{prop_bdd_not_o})
Suppose that $T \in \ofml B \cap \lobo o$. Then given $\epsilon > 0$, we may choose $\delta >
0$ so that $\norm{Ty} \le \epsilon\norm y$ whenever $\norm y < \delta$.   Let $x$ be an
arbitrary unit vector.  Choose $0 < t < \delta$. Then $\norm{tx} = t < \delta$; so $\norm{Tx}
= \norm{T(\frac1t tx)} = \frac1t\norm{T(tx)} \le \frac1t\epsilon\norm{tx} = \epsilon$.  Since
this last inequality holds for every unit vector $x$, $\norm T \le \epsilon$.  And since
$\epsilon$ was arbitrary, $\norm T = 0$.  That is, $T = \vc 0$.
\end{prf}

%Alternative proof
%Suppose that $T \in \ofml B \cap \lobo o$.  Then given $\epsilon
%>0$, we may choose $\delta > 0$ so that $\norm{Ty} \le
%\epsilon\norm y$ whenever $\norm y < \delta$.  Let $x$ be an
%arbitrary nonzero vector and write $x=\alpha y$ where $\alpha =
%\frac2{\delta} \norm x$ and $y = \frac{\delta}{2\norm x}x$.  Since
%$\norm y = \frac12\delta < \delta$, we see that
%\[\norm{Tx} = \alpha\norm{Ty} \le \alpha \epsilon \norm y =
%\frac12 \alpha \epsilon \delta = \epsilon \norm x.\]
%This last inequality holds for every vector $x$; so $\norm T \le
%\epsilon$.  Since $\epsilon$ was arbitrary, $\norm T
%= 0$.  That is, $T=0$.   \qed

\begin{prf}\label{sol_prop_O_cl_add}(Solution to~\ref{prop_O_cl_add})
If $f$, $g \in \lobo O$, then there exist positive numbers $M$, $N$, $\delta$, and $\eta$ such
that $\norm{f(x)} \le M\norm x$ whenever $\norm x < \delta$ and $\norm{g(x)} \le N\norm x$
whenever $\norm x < \eta$.  Then $\norm{f(x) + g(x)} \le (M+N)\norm x$ whenever $\norm x <
\min\{\delta, \eta\}$.  So $f + g \in \lobo O$.

If $\alpha \in \R$, then $\norm{\alpha f(x)} \le \abs\alpha M \norm x$ whenever $\norm x <
\delta$; so $\alpha f \in \lobo O$.
\end{prf}

\begin{prf}\label{sol_prop_comp_Oo}(Solution to~\ref{prop_comp_Oo})
The domain of $f \circ g$ is taken to be the set of all $x$ in $V$ such that $g(x)$ belongs to
the domain of $f$; that is, $\dom(f \circ g) = g^\gets(\dom f)$.  Since $f \in \lobo O$ there
exist $M$, $\delta > 0$ such that $\norm{f(y)} \le M \norm y$ whenever $\norm y < \delta$.
Given $\epsilon > 0$, choose $\eta > 0$ so that $\norm{g(x)} \le \frac{\epsilon}M\norm x$
whenever $\norm x < \eta$.  If $\norm x < \min\{\eta, \frac M{\epsilon} \delta\}$, then
$\norm{g(x)} \le \frac{\epsilon}M\norm x < \delta$, so that $\norm{(f \circ g)(x)}\| \le
M\norm{g(x)} \le \epsilon\norm x$.
\end{prf}

\begin{prf}\label{sol_prop_o_x_vect}(Solution to~\ref{prop_o_x_vect})
Suppose $w \ne \vc 0$.  If $\epsilon > 0$, then there exists $\delta > 0$ such that
$\abs{\phi(x)} \le \frac{\epsilon}{\norm w} \norm x$ whenever $\norm x < \delta$.  Thus
$\norm{(\phi w)(x)} = \abs{\phi(x)} \norm w \le \epsilon \norm x$ when $\norm x < \delta$.
\end{prf}

\begin{prf}\label{sol_prop_O_x_fcn}(Solution to~\ref{prop_O_x_fcn})
There exist positive numbers $M$, $N$, $\delta$, and $\eta$ such that $\norm{\phi(x)} \le
M\norm x$ whenever $\norm x < \delta$ and $\norm{f(x)} \le N\norm x$ whenever $\norm x <
\eta$.  Suppose that $\epsilon > 0$. If $x \in V$ and $\norm x < \min\bigl\{\epsilon(MN)^{-1},
\delta, \eta\bigr\}$, then
  \[ \norm{(\phi f)(x)} = \abs{\phi(x)} \norm{f(x)} \le MN\norm x^2 \le \epsilon \norm x\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_tan_equiv}(Solution to~\ref{prop_tan_equiv})
Reflexivity is obvious.  Symmetry: If $f \simeq g$, then $f - g \in \lobo o$; so $g - f =
(-1)(f - g) \in \lobo o$ by proposition~\ref{prop_o_cl_add}. This proves $g \simeq f$.
Transitivity: If $f \simeq g$ and $g \simeq h$, then both $f - g$ and $g - h$ belong to $\lobo
o$; thus $f \simeq h$, since $f - h = (f - g) + (g - h) \in \lobo o + \lobo o \subseteq \lobo
o$ (again by~\ref{prop_o_cl_add}).
\end{prf}

\begin{prf}\label{sol_prop_tan_lin}(Solution to~\ref{prop_tan_lin})
By the preceding proposition $S \simeq T$. Thus $S - T \in \ofml B \cap \lobo o = \{\vc 0\}$
by proposition~\ref{prop_bdd_not_o}.
\end{prf}

\begin{prf}\label{sol_prop_tan_x_vec}(Solution to~\ref{prop_tan_x_vec})
This requires only a simple computation: $\phi w - \psi w = (\phi - \psi)w \in \lobo o(V,\R)
\cdot W \subseteq \lobo o(V,W)$ by proposition~\ref{prop_o_x_vect}.
\end{prf}

\begin{prf}\label{sol_exer_diff_R2}(Solution to~\ref{exer_diff_R2})
The map $(x,y) \mapsto 7x - 9y$ is clearly continuous and linear.  So all that needs to be
verified is condition (iii) of remark~\ref{rem_prop_diff}:
 \begin{align*}
     &\lim_{(x,y) \sto (0,0)}\frac{\Delta f_{(1,-1)}(x,y) -
                                       (7x - 9y)}{\sqrt{x^2 + y^2}}  \\
   = &\lim_{(x,y) \sto (0,0)}\frac{3(x+1)^2 - (x+1)(y-1) +
                                  4(y-1)^2 - 8 - 7x + 9y}{\sqrt{x^2+y^2}} \\
   = &\lim_{(x,y) \sto (0,0)}\frac{3x^2 - xy +4y^2}{\sqrt{x^2 + y^2}} = 0.
                                \qquad\text{(See problem~\ref{prob_lim_Rn}(a).)}
 \end{align*}
(The equation $z = 7x - 9y$ represents a plane through the origin.)
\end{prf}

\begin{prf}\label{sol_exer_diff_R3R2}(Solution to~\ref{exer_diff_R3R2})
Since
  \begin{align*}
      \Delta f_{(1,-1,0)}(h,j,k) &= f(h + 1,j - 1,k) - f(1,-1,0) \\
            &=\bigl((h + 1)^2(j - 1)-7, 3(h + 1)k + 4(j - 1)\bigr) - (-8,-4) \\
            &=\bigl(h^2(j - 1) + 2hj - 2h + j, 3hk + 4j + 3k\bigr)
  \end{align*}
and
  \begin{align*}
       M(h,j,k) &= \begin{bmatrix}
                        r & s & t \\
                        u & v & w
                   \end{bmatrix} (h,j,k) \\
                &= (rh + sj + tk, uh + vj + wk),
  \end{align*}
we find that the first coordinate of the Newton quotient
  \[ \frac{\Delta f_{(1,-1,0)}(h,j,k) - M(h,j,k)}{\norm{(h,j,k)}} \]
turns out to be
  \[ \frac{h^2(j-1) + 2hj - (2+r)h + (1-s)j - tk}{\sqrt{h^2 + j^2 + k^2}}\,. \]
If we choose $r = -2$, $s = 1$, and $t = 0$, then the preceding expression approaches zero as
$(h,j,k) \sto (0,0,0)$.  (See problem~\ref{prob_lim_Rn}(a).)   Similarly, the second
coordinate of the Newton quotient is
  \[ \frac{3hk - uh + (4 - v)j + (3 - w)k}{\sqrt{h^2 + j^2 + k^2}}\,, \]
which approaches zero as $(h,j,k) \sto (0,0,0)$ if we choose $u = 0$, $v = 4$, and $w = 3$.
We conclude from the uniqueness of differentials (proposition~\ref{prop_diff_uniq}) that
  \[ \bigl[df_{(1,-1,0,)}\bigr] = \begin{bmatrix}
                                        -2 & 1 & 0 \\
                                         0 & 4 & 3
                                  \end{bmatrix}\,. \]
Equivalently we may write
  \[ df_{(1,-1,0)}(h,j,k) = (-2h + j, 4j + 3k)\,. \qedhere  \]
\end{prf}

\begin{prf}\label{sol_prop_Leib_rul}(Solution to~\ref{prop_Leib_rul})
It is easy to check that $\phi(a)df_a + d\phi_a\cdot f(a)$ is bounded and linear.  From our
hypotheses $\Delta f_a \simeq df_a$ and $\Delta\phi_a \simeq d\phi_a$ we infer (using
propositions \ref{prop_tan_sum} and \ref{prop_tan_x_vec}) that $\phi(a)\Delta f_a \simeq
\phi(a)df_a$ and that $\Delta\phi_a\cdot f(a) \simeq d\phi_a\cdot f(a)$.  Then from
corollary~\ref{cor_diff_cont} and proposition~\ref{prop_O_x_fcn} we conclude that
$\Delta\phi_a\,\Delta f_a$ belongs to $\lobo O(V,\R)\cdot\lobo O(V,W)$ and therefore to $\lobo
o(V,W)$.  That is, $\Delta\phi_a\,\Delta f_a \simeq 0$.  Thus by propositions
\ref{prop_del_mult} and \ref{prop_tan_sum}
  \begin{align*}
       \Delta(\phi f)_a
            &= \phi(a)\cdot\Delta f_a + \Delta\phi_a\cdot f(a) + \Delta\phi_a\cdot\Delta f_a \\
            &\simeq \phi(a)df_a + d\phi_a\cdot f(a) + 0 \\
            &= \phi(a)df_a + d\phi_a\cdot f(a).   \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_thm_ch_rul}(Solution to~\ref{thm_ch_rul})
Our hypotheses are $\Delta f_a \simeq df_a$ and $\Delta g_{f(a)} \simeq dg_{f(a)}$. By
proposition~\ref{prop_diff_O} $\Delta f_a \in \lobo O$. Then by
proposition~\ref{prop_tan_comp_O}
  \begin{equation}\label{eqn_chnrl1} \Delta g_{f(a)} \circ \Delta f_a
                                       \simeq dg_{f(a)}  \circ \Delta f_a
  \end{equation}
and by proposition~\ref{prop_lin_comp_tan}
  \begin{equation}\label{eqn_chnrl2} dg_{f(a)} \circ \Delta f_a
                                       \simeq dg_{f(a)} \circ df_a\,.
  \end{equation}
According to proposition~\ref{prop_del_comp}
  \begin{equation}\label{eqn_chnrl3} \Delta(g \circ f)_a
                               \simeq \Delta g_{f(a)} \circ \Delta f_a\,.
  \end{equation}
From \eqref{eqn_chnrl1}, \eqref{eqn_chnrl2},\eqref{eqn_chnrl3}, and
proposition~\ref{prop_tan_equiv} it is clear that
  \[ \Delta(g \circ f)_a \simeq dg_{f(a)} \circ df_a\,. \]
Since $dg_{f(a)} \circ df_a$ is a bounded linear transformation, the desired conclusion is an
immediate consequence of proposition~\ref{prop_diff_uniq}.
\end{prf}

\begin{prf}\label{sol_exer_tang_curv}(Solution to~\ref{exer_tang_curv})

(a)  $Dc(\pi/3) = (-\sin(\pi/3), \cos(\pi/3)) = (-\sqrt3/2, 1/2)$.

(b) $l(t) = (1/2,\sqrt3/2)+ t(-\sqrt3/2,1/2) = \frac12(1 - \sqrt3t, \sqrt3 + t)$.

(c) $x + \sqrt3 y = 2$.
\end{prf}

\begin{prf}\label{sol_prop_diff_der}(Solution to~\ref{prop_diff_der})
If $c$ is differentiable at $a$, then there exists a bounded linear transformation $dc_a\colon
\R \sto V$ which is tangent to $\Delta c_a$ at $0$.  Then
  \begin{align*}
        Dc(a) &= \lim_{h \sto 0}\frac{\Delta c_a(h)}h \\
              &= \lim_{h \sto 0}\frac{\Delta c_a(h) - dc_a(h) + dc_a(h)}h \\
              &= \lim_{h \sto 0}\frac{\Delta c_a(h) - dc_a(h)}h
                                   + \lim_{h \sto 0}\frac{h\,dc_a(1)}h \\
              &= dc_a(1).     \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_thm_ftc}(Solution to~\ref{thm_ftc})
Let $\epsilon > 0$. Since $f$ is continuous at $c$ and the interval $J$ is open, we may choose
$\delta > 0$ so that $c + h \in J$ and $\norm{\Delta f_c(h)} < \epsilon$ whenever $\abs h <
\delta$.  Thus if $0 < \abs h < \delta$,
  \begin{alignat*}{2} \norm{\Delta F_c(h) - h\,f(c)}
     &= \bignorm{\int_a^{c + h}f - \int_a^cf - h\,f(c)} &&{}\\
     &= \bignorm{\int_c^{c + h}f(t)\,dt - \int_c^{c + h}f(c)\,dt}&&{}\\
     &= \bignorm{\int_c^{c + h}\Delta f_c(t - c)\,dt} &&{}\\
     &\le \bigabs{\int_c^{c + h}\norm{\Delta f_c(t - c)}\,dt}
             &&\quad\text{(by~\ref{prop_norm_Cint})} \\
     &< \epsilon\,\abs h\,. &&{}
  \end{alignat*}
It follows immediately that
  \[ \bignorm{\frac{\Delta F_c(h)}h - f(c)} < \frac1{\abs h}\epsilon\abs h = \epsilon \]
whenever $0 < \abs h < \delta$; that is,
  \[ DF(c) = \lim_{h \sto 0}\frac{\Delta F_c(h)}h = f(c)\,.  \qedhere  \]
\end{prf}

\begin{prf}\label{sol_prop_diff_dd}(Solution to~\ref{prop_diff_dd})
If $l(t) = a + tv$, then
  \begin{align*}
     D(f \circ l)(0)
         &= \lim_{t \sto 0}\frac1t\Delta(f \circ l)_0(t) \\
         &= \lim_{t \sto 0}\frac1t((f \circ l)(0 + t) - (f \circ l)(0))\\
         &= \lim_{t \sto 0}\frac1t(f(a + tv) - f(a)) \\
         &= \lim_{t \sto 0}\frac1t\Delta f_a(tv) \\
         &= D_vf(a).  \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_dd1}(Solution to~\ref{exer_dd1})
Let $l(t) = a + tv$. Then
  \begin{align*}
       (f \circ l)(t) &= f(a+tv) \\
            &= f((1,1) + t(\tfrac35,\tfrac45)) \\
            &= f(1 + \tfrac35t, 1 + \tfrac45t) \\
            &= \tfrac12\ln\bigl((1+\tfrac35t)^2
                         + (1+\tfrac45t)^2\bigr) \\
            &= \tfrac12\ln\bigl(2 + \tfrac{14}5t + t^2\bigr).
  \end{align*}
It follows that $D(f \circ l)(t) = \frac12\bigl(\frac{14}5+2t\bigr) \bigl(2 + \frac{14}5t +
t^2\bigr)^{-1}$, so that $D_vf(a) = D(f \circ l)(0) = \frac7{10}$.
\end{prf}

\begin{prf}\label{sol_exer_dd2}(Solution to~\ref{exer_dd2})
As usual let $l(t) = a + tv$. Then
  \begin{align*} (\phi \circ l)(t)
         &= \int_0^{\pi/2}(\cos x + Da(x) + tDv(x))^2\,dx \\
         &= \int_0^{\pi/2}(2\cos x + t\sin x)^2\,dx \\
         &= \int_0^{\pi/2}4\cos^2x\,dx
                      + 4t\int_0^{\pi/2}\sin x \cos x\,dx
                      + t^2\int_0^{\pi/2}\sin^2 x\,dx\,.
  \end{align*}
Differentiating we obtain
  \[ D(\phi \circ l)(t) = 4\int_0^{\pi/2}\sin x \cos x\,dx
                                  + 2t\int_0^{\pi/2}\sin^2 x\,dx\,; \]
so
  \[ D_v\phi(a) = D(\phi \circ l)(0) = 4\int_0^{\pi/2}\sin x \cos x\,dx = 2\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_dd_diff}(Solution to~\ref{prop_dd_diff})
If $l = a + tv$, then, since $l(0) = a$ and $Dl(0) = v$, we have
  \begin{alignat*}{2} D_vf(a) &= D(f \circ l)(0) &&{}\\
         &= df_{l(0)}(Dl(0))   &&\quad\text{(by~\ref{prob_der_curv1})} \\
         &= df_a(v).           &&{}
  \end{alignat*}  \qedhere
\end{prf}

\begin{prf}\label{sol_prop_diff_prodsp}(Solution to~\ref{prop_diff_prodsp})
If $x \in \dom f^1 \cap \dom f^2$, then
  \begin{align*}
      \bigl((j_1 \circ f^1) + (j_2 \circ f^2)\bigr)(x)
            &= j_1\bigl(f^1(x)\bigr) + j_2\bigl(f^2(x)\bigr) \\
            &= \bigl(f^1(x),0\bigr) + \bigl(0,f^2(x)\bigr) \\
            &= \bigl(f^1(x),f^2(x)\bigr) \\
            &= f(x)\,.
  \end{align*}
Being the sum of composites of differentiable functions, $f$ is differentiable, and
  \begin{alignat*}{2}
     df_a &= d\bigl((j_1 \circ f^1) + (j_2 \circ f^2)\bigr)_a &&{}\\
          &= d\bigl(j_1 \circ f^1\bigr)_a + d\bigl(j_2 \circ f^2\bigr)_a
                 &&\quad\text{(by~\ref{prop_diff_sum})} \\
          &= d\bigl(j_1\bigr)_{f^1(a)} \circ d\bigl(f^1\bigr)_a
                 + d\bigl(j_2\bigr)_{f^2(a)}\circ d\bigl(f^2\bigr)_a
                 &&\quad\text{(by~\ref{thm_ch_rul})} \\
          &= j_1 \circ d\bigl(f^1\bigr)_a + j_2 \circ d\bigl(f^2\bigr)_a
                 &&\quad\text{(by~\ref{prob_diff_lt})} \\
          &= \left(d\bigl(f^1\bigr)_a \,,\, d\bigl(f^2\bigr)_a\right)\,. &&{}
  \end{alignat*}   \qedhere
\end{prf}

\begin{prf}\label{sol_cor_diff_prodcurv}(Solution to~\ref{cor_diff_prodcurv})
By propositions~\ref{prop_diff_der} and~\ref{prop_der_diff} a curve has a derivative at $t$ if
and only if it is differentiable at~$t$. Thus the desired result is an immediate consequence
of the following easy computation:
  \begin{align*} Dc(t) &= dc_t(1) \\
                       &= \left(d\bigl(c^1\bigr)_t(1)\,,\, d\bigl(c^2\bigr)_t(1)\right) \\
                       &= \left(Dc^1(t)\,,\,Dc^2(t)\right)\,.   \qedhere
  \end{align*}
\end{prf}





%END chapter 25














\section{Exercises in chapter 26}

\begin{prf}\label{sol_exer_no_mvt}(Solution to~\ref{exer_no_mvt})
Let $f\colon [0,2\pi] \sto \R^2\colon  t \mapsto (\cos t, \sin t)$.  Then $f$ is continuous on
$[0,2\pi]$ and differentiable on $(0,2\pi)$.  Notice that $f(2\pi) - f(0) = (1,0) - (1,0) =
(0,0)$.  But $Df(t) = (-\sin t, \cos t)$. Certainly there is no number $c$ such that
$2\pi(-\sin c, \cos c) = (0,0)$.
\end{prf}

\begin{prf}\label{sol_thm_mvt_curv}(Solution to~\ref{thm_mvt_curv})
Given $\epsilon > 0$, define $h(t) = \norm{f(t) - f(a)} - (t - a)(M + \epsilon)$ for $a \le t
\le b$.  Since $f$ is continuous on $[a,b]$, so is~$h$.  Let $A = h^\gets (-\infty,
\epsilon]$.  The set $A$ is nonempty (it contains~$a$) and is bounded above (by $b$).  By the
\emph{least upper bound axiom}~\ref{axiom_lub} it has a supremum, say~$l$. Clearly $a \le l
\le b$.  Since $h$ is continuous and $h(a) = 0$, there exists $\eta > 0$ such that $a \le t <
a + \eta$ implies $h(t) \le \epsilon$.  Thus $[a, a + \eta) \subseteq A$ and $l > a$. Notice
that since $h$ is continuous the set $A$ is closed (proposition~\ref{prop_cont_closed}); and
since $l \in \clo A$ (see example~\ref{sup_in_clo}), $l$ belongs to~$A$.

We show that $l = b$.  Assume to the contrary that $l < b$.  Since $f$ is differentiable at
$l$, there exists $\delta > 0$ such that if $t \in (l, l + \delta)$ then $\norm{(t -
l)^{-1}(f(t) - f(l))} < M + \epsilon$.  Choose any point $t$ in $(l,l + \delta)$. Then
  \begin{align*}
      h(t) &= \norm{f(t) - f(a)} - (t - a)(M + \epsilon) \\
           &\le \norm{f(t) - f(l)} + \norm{f(l) - f(a)}
               - (t - l)(M + \epsilon) - (l - a)(M + \epsilon) \\
           &< (t - l)(M + \epsilon) + h(l) - (t - l)(M + \epsilon) \\
           &= h(l) \\
           &\le \epsilon\,.
  \end{align*}
This says that $t \in A$, which contradicts the fact that $l$ is an upper bound for $A$.  Thus
$l = b$ and $h(b) \le \epsilon$. That is,
  \[ \norm{f(b) - f(a)} \le (M + \epsilon)(b - a) + \epsilon\,. \]
Since $\epsilon$ was arbitrary,
  \[ \norm{f(b) - f(a)} \le M(b-a)\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_inj_proj}(Solution to~\ref{exer_inj_proj})

(a) For every $x$ in $V_k$
  \[ (\pi_k \circ j_{{}_{\sst k}})(x) = \pi_k(0,\dots, 0, x, 0,\dots, 0) = x\,. \]

(b) For every $x$ in $V$
  \begin{align*}
     \sum_{k=1}^n (j_{{}_{\sst k}} \circ \pi_k)(x)
          &= \sum_{k=1}^n j_{{}_{\sst k}}(x_k) \\
          &= (x_1, 0, \dots ,0) + \dots + (0, \dots, 0, x_n) \\
          &= (x_1, \dots, x_n) \\
          &= x\,.   \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_Del_inj}(Solution to~\ref{exer_Del_inj})
For every $h$ in $B_k$
  \begin{align*}
      (\Delta f_a \circ j_{{}_{\sst k}})(h)
              &= \Delta f_a (j_{{}_{\sst k}}(h)) \\
              &= f(a + j_{{}_{\sst k}}(h)) - f(a) \\
              &= g(h) - g(\vc 0) \\
              &= \Delta g_{\vc 0}(h)\,. \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_prop_par_dffntl}(Solution to~\ref{prop_par_dffntl})
Fix a point $(a,b)$ in $U$.  We make two observations about the notation introduced in the
hint.  First,
  \begin{equation}\label{eqn_par_dffntl4}
        d(h^v)_z = d_1f_{(a + z,b + v)}\,.
  \end{equation}
[Proof:  Since $f(a + z + s, b+v) = h^v(z + s) = (h^v \circ T_z)(s)$, we see that $d_1f_{(a +
z,b + v)} = d(h^v \circ T_z)_{\vc 0} = d(h^v)_{T_z(\vc 0)} \circ d(T_z)_{\vc 0} = d(h^v)_z
\circ I = d(h^v)_z\,.$]

Second,
 \begin{equation}\label{eqn_par_dffntl5}
    \Delta f_{(a,b)}(u,v) = \Delta (h^v)_{\vc 0}(u) + \Delta g_{\vc 0}(v)\,.
 \end{equation}
[Proof:
  \begin{align*}
      \Delta f_{(a,b)}(u,v)
            &= f(a + u,b + v) - f(a,b)  \\
            &= f(a + u,b + v) - f(a,b + v) + f(a,b + v) - f(a,b) \\
            &= h^v(u) - h^v(\vc 0) + g(v) - g(\vc 0) \\
            &= \Delta (h^v)_{\vc 0}(u) + \Delta g_{\vc 0}(v)\,.]
  \end{align*}

Let $\epsilon > 0$.  By hypothesis the second partial differential of $f$ exists at~$(a,b)$.
That is, the function $g$ is differentiable at $\vc 0$ and
  \[ \Delta g_{\vc 0} \simeq dg_{\vc 0} = d_2f_{(a,b)} = T\,. \]
Thus there exists $\delta_1 > 0$ such that
\begin{equation}\label{ineq_par_dffntl1}
          \norm{\Delta g_{\vc 0}(v) - Tv} \le \epsilon\norm v
\end{equation}
whenever $\norm v < \delta_1$.

Since $d_1f$ is assumed to (exist and) be continuous on $U$, there exists $\delta_2 > 0$ such
that
 \begin{equation}\label{ineq_par_dffntl2}
     \norm{d_1f_{(a + s,b + t)} - d_1f_{(a,b)}} < \epsilon
 \end{equation}
whenever $\norm{(s,t)}_1 < \delta_2$.  Suppose then that $(u,v)$ is a point in $U$ such that
$\norm{(u,v)}_1 < \delta_2$.  For each $z$ in the segment $[\vc 0,u]$
  \[ \norm{(z,v)}_1 = \norm z + \norm v \le \norm u + \norm v = \norm{(u,v)}_1 < \delta_2 \]
so by~\eqref{eqn_par_dffntl4} and~\eqref{ineq_par_dffntl2}
  \[ \norm{d(h^v)_z - S} = \norm{d_1f_{(a + z, b + v)} - d_1f_{(a,b)}} < \epsilon\,. \]
Thus according to the version of the \emph{mean value theorem} given in
corollary~\ref{cor_mvt_nls}
 \begin{equation}\label{ineq_par_dffntl3}
     \norm{\Delta (h^v)_{\vc 0}(u) - Su} \le \epsilon \norm u
 \end{equation}
whenever $\norm{(u,v)}_1 < \delta_2$.

Now let $\delta = \min\{\delta_1, \delta_2\}$.  Suppose $\norm{(u,v)}_1 < \delta$.  Then since
$\norm v \le \norm u + \norm v = \norm{(u,v)}_1 < \delta \le \delta_1$
inequality~\eqref{ineq_par_dffntl1} holds, and since $\norm{(u,v)}_1 < \delta \le \delta_2$
inequality~\eqref{ineq_par_dffntl3} holds.  Making use of these two inequalities
and~\eqref{eqn_par_dffntl5} we obtain
  \begin{align*}
      \norm{\Delta f_{(a,b)}(u,v) - R(u,v)}
          &= \norm{\Delta (h^v)_{\vc 0}(u) + \Delta g_{\vc 0}(v) - Su - Tv} \\
          &\le \norm{\Delta (h^v)_{\vc 0}(u) - Su} + \norm{\Delta g_{\vc 0}(v) - Tv} \\
          &\le \epsilon\,\norm u + \epsilon\,\norm v \\
          &= \epsilon \,\norm{(u,v)}_1\,.
  \end{align*}
Thus $\Delta f_{(a,b)} \simeq R$ showing that $f$ is differentiable at $(a,b)$ and that its
differential is given by
 \begin{equation}\label{eqn_par_dffntl6}
      df_{(a,b)} = R = d_1f_{(a,b)} \circ \pi_1 + d_2f_{(a,b)} \circ\pi_2\,.
 \end{equation}
That $df$ is continuous is clear from~\eqref{eqn_par_dffntl6} and the hypothesis that $d_1f$
and $d_2f$ are continuously differentiable.
\end{prf}

\begin{prf}\label{sol_exer_pd_R4}(Solution to~\ref{exer_pd_R4})
First we compute $df_a$. A straightforward calculation gives
  \[ \frac{\Delta f_a(h)}{\norm h_1}
             = \frac{h_1 + 2\,h_2 - 3\,h_3 + 6\,h_4 + h_2{}^2
                      + 2\,h_1h_2 + h_1h_2{}^2 + 3\,h_3h_4} {\norm h_1}\,. \]
From this it is clear that the desired differential is given by
  \[ df_a(h) = h_1 + 2\,h_2 - 3\,h_3 + 6\,h_4 \]
for then
  \[ \frac{\Delta f_a(h) - df_a(h)}{\norm h_1}
            = \frac{h_2{}^2 + 2\,h_1h_2 + h_1h_2{}^2 + 3\,h_3h_4}
                          {\norm h_1} \sto 0 \qquad \text{as $h \sto 0$} \]
\textbf{Note.}  In the preceding computation the use of the product norm $\norm{\quad}_1$ for
$\R^4$ rather than the usual Euclidean norm is both arbitrary and harmless (see
problem~\ref{prob_norms_Rn}.

(a)  Compose $df_a$ with the injection
  \[ j_{{}_{\sst 1}} \colon \R \sto \R\times R\times\R\times\R \colon x \mapsto (x,0,0,0)\,. \]
Then
  \[ d_1f_a(x) = df_a(j_{{}_{\sst 1}}(x)) = df_a(x,0,0,0) = x \]
for all $x$ in~$\R$.

(b)  This has exactly the same answer as part (a)---although the rationale is slightly
different.  The appropriate injection map is
  \[ j_{{}_{\sst 1}} \colon \R \sto \R^3 \colon  x \mapsto (x,\vc 0) \]
(where $\vc 0$ is the zero vector in $\R^3$).  We may rewrite~\eqref{eqn_pd_R4} in this case
as
  \[ f(x,y) = xy_1{}^2 + 3\,y_2y_3 \]
for all $x \in \R$ and $y \in \R^3$.  Also write $a = (b,c)$ where $b = 1 \in \R$ and $c =
(1,2,-1) \in \R^3$, and write $h = (r,s)$ where $r \in \R$ and $s \in \R^3$.  Then
  \[ df_a(h) = df_{(b,c)}(r,s) = r + 2\,s_1 - 3\,s_2 + 6\,s_3 \]
so that
  \[ d_1f(x) = df_a(j_{{}_{\sst 1}}(x)) = df_a(x,\vc 0) = x \]
for all $x$ in~$\R$.

(c)  Here the appropriate injection is
  \[j_{{}_{\sst 1}} \colon \R^2 \sto \R^2 \times \R^2 \colon  x \mapsto (x,\vc 0) \]
where $\vc 0$ is the zero vector in~$\R^2$.  Rewrite~\eqref{eqn_pd_R4} as
  \[ f(x,y) = x_1x_2{}^2 + 3\,y_1y_2 \]
for all $x$, $y \in \R^2$.  Let $a = (b,c)$ where $b = (1,1)$ and $c = (2,-1)$; and let $h =
(r,s)$ where $r$, $s \in \R^2$. Then
  \[ df_a(h) = df_{(b,c)}(r,s) = r_1 + 2\,r_2 - 3\,s_1 + 6\,s_2 \]
so that
  \[ d_1f_a(x) = df_a(j_{{}_{\sst 1}}(x)) = df_a(x,\vc 0) = x_1 + 2\,x_2 \]
for all $x$ in~$\R^2$.

(d) As far as the partial differential $d_1$ is concerned, this is essentially the same
problem as~(c).  However, in this case the injection $j_{{}_{\sst 1}}$ is given by
  \[ j_{{}_{\sst 1}} \colon \R^2 \sto \R^2 \times \R \times \R \colon x \mapsto (x,0,0)\,.\]
Equation~\eqref{eqn_pd_R4} may be written
  \[ f(x,y,z) = x_1x_2{}^2 + 3\,yz \]
for all $x \in \R^2$ and $y$, $z \in \R$.  Let $a = (b,c,d)$ where $b = (1,1)$, $c = 2$, and
$d = -1$; and let $h = (q,r,s)$ where $q \in \R^2$ and $r$, $s \in \R$.  Then
  \[ df_a(h) = df_{(b,c,d)}(q,r,s) = q_{{}_{\sst 1}} + 2\,q_{{}_{\sst 2}} - 3\,r + 6\,s \]
so that
  \[ d_1f_a(x) = df_a(j_{{}_{\sst 1}}(x)) = df_{(b,c,d)}(x,0,0) = x_1 + 2\,x_2\,. \]

(e) Here $j_{{}_{\sst 1}} \colon \R^3 \sto \R^3 \times \R \colon x \mapsto (x,0)$.
Rewrite~\eqref{eqn_pd_R4} as
  \[ f(x,y) = x_1x_2{}^2 + 3\,x_3y \]
for all $x \in \R^3$ and $y \in \R$.  Let $a = (b,c)$ with $b = (1,1,2)$ and $c = -1$; and let
$h = (r,s)$ where $r \in \R^3$ and $s \in \R$.  Then
  \[ df_a(h) = df_{(b,c)}(r,s) = r_1 + 2\,r_2 -3\,r_3 + 6\,s \]
so that
  \[ d_1f_a(x) = df_a(j_{{}_{\sst 1}}(x)) = df_{(b,c)}(x,0) = x_1 + 2\,x_2 - 3\,x_3\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_pd_R3}(Solution to~\ref{exer_pd_R3})
Just do what you have always done: hold two of the variables constant and differentiate with
respect to the other.  (See the paragraph after equation~\eqref{eqn_def_pd}.)
 \begin{align*}
   f_1(x,y,z) &= (3\,x^2y^2\sin z,2\,x);\text{ so }f_1(a)=(12,2).\\
   f_2(x,y,z) &= (2\,x^3y\sin z, \cos z);\text{ so }f_2(a)=(-4,0).\\
   f_3(x,y,z) &= (x^3y^2 \cos z,-y\sin z);\text{ so }f_3(a)=(0,2). \qedhere
 \end{align*}
\end{prf}

\begin{prf}\label{sol_lem_int_2cont}(Solution to~\ref{lem_int_2cont})
Let $\epsilon > 0$. Since $[a,b] \times [c,d]$ is compact, the continuous function $f$ must be
uniformly continuous (see proposition~\ref{prop_cont_ucont}).  Thus there exists $\delta > 0$
such that $\norm{(x,y) - (u,v)}_1 < \delta$ implies $\norm{f(x,y) - f(u,v)} < \epsilon\,(b -
a)^{-1}$.  Suppose that $y$ and $v$ lie in $[c,d]$ and that $\abs{y - v} < \delta$. Then
$\norm{(x,y) - (x,v)}_1 < \delta$ for all $x$ in $[a,b]$; so $\norm{f(x,y) - f(x,v)} <
\epsilon\,(b - a)^{-1}$ from which it follows that
  \begin{align*}
    \norm{g(y) - g(v)}
            &= \bignorm{\int_a^bf^y - \int_a^bf^v}  \\
            &= \bignorm{\int_a^b(f^y - f^v)}          \\
            &\le \int_a^b\norm{f^y(x) - f^v(x)}\,dx \\
            &= \int_a^b\norm{f(x,y) - f(x,v)}\,dx   \\
            &< \int_a^b\epsilon\,(b - a)^{-1}\,dx \\
            &= \epsilon\,.
 \end{align*}
Thus $g$ is uniformly continuous.
\end{prf}

\begin{prf}\label{sol_prop_diff_und_int}(Solution to~\ref{prop_diff_und_int})
Let $h(y) = \int_a^bf_2(x,y)\,dx$.  By lemma~\ref{lem_int_2cont} the function $h$ is
continuous and therefore integrable on every interval of the form $[c,z]$ where $c \le z \le
d$.  Then by proposition~\ref{prop_chord_int} we have
  \begin{align*}
    \int_c^zh
       &= \int_c^z\int_a^bf_2(x,y)\,dx\,dy \\
       &= \int_a^b\int_c^zf_2(x,y)\,dy\,dx \\
       &= \int_a^b\int_c^z\frac d{dy}\bigl({}^xf(y)\bigr)\,dy\,dx \\
       &= \int_a^b\bigl({}^xf(z) - {}^xf(c)\bigr)\,dx \\
       &= \int_a^b\bigl(f(x,z) - f(x,c)\bigr)\,dx \\
       &= g(z) - g(c)\,.
  \end{align*}
Differentiating we obtain
  \[ h(z) = g'(z) \]
for $c < z < d$.  This shows that $g$ is continuously differentiable on $(c,d)$ and that
  \begin{align*}
      \frac d{dy}\int_a^bf(x,y)\,dx
            &= g'(y)                 \\
            &= h(y)                  \\
            &= \int_a^b f_2(x,y)\,dx \\
            &= \int_a^b\pd fy (x,y)\,dx\,. \qedhere
  \end{align*}
\end{prf}






%END chapter 26


















\section{Exercises in chapter 27}

\begin{prf}\label{sol_exer_ip1}(Solution to~\ref{exer_ip1})
If $x$, $y$, and $z \in \R^n$ and $\alpha \in \R$, then
  \begin{alignat*}{2}
      \langle x , y + z \rangle &= \langle y + z , x \rangle  &
                &\quad\text{(by (c))}  \\
      &= \langle y , x \rangle + \langle z , x \rangle        &
                &\quad\text{(by (a))}   \\
      &= \langle x , y \rangle + \langle x , z \rangle        &
                &\quad\text{(by (c))}
  \end{alignat*}
and
  \begin{alignat*}{2}
      \langle x , \alpha y \rangle
                &= \langle \alpha y , x \rangle      &
                &\quad\text{(by (c))}  \\
      &= \alpha \langle y , x \rangle                &
                &\quad\text{(by (b))}   \\
      &= \alpha \langle x , y \rangle                &
                &\quad\text{(by (c))}
  \end{alignat*}  \qedhere
\end{prf}

\begin{prf}\label{sol_exer_angle}(Solution to~\ref{exer_angle})
The domain of the arccosine function is the closed interval~$[-1,1]$.  According to the
\emph{Schwarz inequality} $\abs{\langle x , y \rangle} \le \norm x \norm y$; equivalently,
  \[ -1 \le \frac{\langle x,y \rangle}{\norm x \norm y} \le 1 \]
for nonzero vectors $x$ and~$y$.  This shows that $\langle x,y \rangle\norm x^{-1}\norm
y^{-1}$ is in the domain of arccosine.
\end{prf}

\begin{prf}\label{sol_exer_angle2}(Solution to~\ref{exer_angle2})
If $x = (1,0,1)$ and $y = (0,-1,1)$, then $\langle x,y \rangle = 1$ and $\norm x = \norm y =
\sqrt2$.  So
  \[ \measuredangle(x,y) = \arccos\left(\frac{\langle x,y \rangle}{\norm x \norm y}\right)
                         = \arccos\frac12 = \frac\pi3\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_exam_blf}(Solution to~\ref{exam_blf})
The computations
  \[ \sbsb{\psi}b(x+y) = \langle x+y,b \rangle
                 = \langle x,b \rangle + \langle y,b \rangle
                 = \sbsb{\psi}b(x) + \sbsb{\psi}b(y) \]
and
  \[ \sbsb{\psi}b(\alpha x) = \langle \alpha x,b \rangle
                      = \alpha \langle x,b \rangle
                      = \alpha \sbsb{\psi}b(x) \]
show that $\sbsb{\psi}b$ is linear.  Since
  \[ \abs{\sbsb{\psi}b(x)} = \abs{\langle x,b \rangle} \le \norm b \norm x \]
for every $x$ in $\R^n$, we conclude that $\sbsb{\psi}b$ is bounded and that
$\norm{\sbsb{\psi}b} \le \norm b$.  On the other hand, if $b \ne \vc 0$, then $\norm
b^{-1}b$ is a unit vector, and since
  \[ \abs{\sbsb{\psi}b(\norm b^{-1}b)} = \langle \norm b^{-1}b,b \rangle
                                 = \norm b^{-1}\langle b,b \rangle = \norm b \]
we conclude (from lemma \ref{lem_equiv_norm}) that $\norm{\sbsb{\psi}b} \ge \norm b$.
\end{prf}

\begin{prf}\label{sol_prop_max_dd}(Solution to~\ref{prop_max_dd})
By proposition~\ref{prop_dd_diff} we have for every unit vector $u$ in~$\R^n$
  \begin{align*}
        D_u\phi(a)  &= d\phi_a(u)  \\
                    &= \langle u,\nabla\phi(a) \rangle  \\
                    &= \norm{\nabla\phi(a)}\cos\theta
  \end{align*}
where $\theta = \measuredangle(u,\nabla\phi(a))$.  Since $\phi$ and $a$ are fixed we maximize
the directional derivative $D_u\phi(a)$ by maximizing $\cos\theta$.  But $\cos\theta = 1$ when
$\theta = 0$; that is, when $u$ and $\nabla\phi(a)$ point in the same direction.  Similarly,
to minimize $D_u\phi(a)$ choose $\theta = \pi$ so that $\cos\theta = -1$.
\end{prf}

\begin{prf}\label{sol_cons_energ}(Solution to~\ref{cons_energ})
It suffices, by proposition~\ref{prop_sc_const}, to show that the derivative of the total
energy $TE$ is zero.
  \begin{align*}
         D(TE) &= D(KE) + D(PE) \\
               &= \tfrac12mD\langle v,v \rangle + D(\phi\circ x) \\
               &= \tfrac12m(2\langle v,Dv \rangle)
                    + \langle Dx, (\nabla\phi)\circ x \rangle \\
               &= m\langle v,a \rangle
                    + \langle v,-F \circ x \rangle  \\
               &= m\langle v,a \rangle - m\langle v,a \rangle \\
               &= 0 .
  \end{align*}
(The third equality uses \ref{prop_ip_diff} and~\ref{prop_cr_curv}; the second last uses
\emph{Newton's second law}.)
\end{prf}

\begin{prf}\label{sol_prop_grad_pd}(Solution to~\ref{prop_grad_pd})
Using the hint we compute
  \begin{alignat*}{2}
    \nabla\phi(a)
        &= \sum_{k=1}^n \langle \nabla\phi(a),e^k \rangle e^k  &
                    &\quad\text{(by~\ref{prop_ip_expan})}  \\
        &= \sum_{k=1}^n d\phi_a(e^k)e^k         & & \\
        &= \sum_{k=1}^n D_{e^k}\phi(a) e^k         &
                    &\quad\text{(by~\ref{prop_dd_diff})}  \\
        &= \sum_{k=1}^n \phi_k(a) e^k .
  \end{alignat*}   \qedhere
\end{prf}

\begin{prf}\label{sol_exer_comp_dd}(Solution to~\ref{exer_comp_dd})
By proposition \ref{prop_dd_diff}
  \[ D_u\phi(a) = d\phi_a(u) = \langle u,\nabla\phi(a) \rangle\,. \]
Since
  \begin{align*}
     \nabla\phi(w,x,y,z) &= \sum_{k=1}^4 \phi_k(w,x,y,z)e^k \\
                         &= (z,-y,-x,w)
  \end{align*}
we see that
  \[ \nabla\phi(a) = (4,-3,-2,1)\,. \]
Thus
  \[ D_u\phi(a) = \langle (\tfrac12, -\tfrac12, \tfrac12, -\tfrac12), (4,-3,-2,1) \rangle = 2\,. \qedhere \]
\end{prf}

\begin{prf}\label{sol_mthd_st_desc}(Solution to~\ref{mthd_st_desc})
As suggested in the hint, let $c\colon t \mapsto \bigl(x(t),y(t)\bigr)$ be the desired curve
and set
  \[ c(0) = \bigl(x(0),y(0)\bigr) = a = (2,-1)\,. \]
At each point $c(t)$ on the curve set the tangent vector $Dc(t)$ equal to
$-\bigl(\nabla\phi\bigr)\bigl(c(t)\bigr)$.  Then for every $t$ we have
  \begin{align*}
         \bigl(Dx(t),Dy(t)\bigr)
              &= -\bigl(\nabla\phi\bigr)\bigl(x(t),y(t)\bigr) \\
              &= \bigl(-4x(t), -12y(t)\bigr).
  \end{align*}
The two resulting equations
  \[ Dx(t) = -4 x(t) \qquad\text{and}\qquad Dy(t) = -12y(t) \]
have as their only nonzero solutions
  \[ x(t) = x(0)e^{-4t} = 2e^{-4t} \]
and
  \[ y(t) = y(0)e^{-12t} = -e^{-12t}\,. \]
Eliminating the parameter we obtain
  \[y(t) = -e^{-12t} = -\bigl(e^{-4t}\bigr)^3
         = -\bigl(\frac12x(t)\bigr)^3
         = -\frac18\bigl(x(t)\bigr)^3\,. \]
Thus the path of steepest descent (in the $xy$-plane) follows the curve $y = -\frac18x^3$ from
$x = 2$ to $x = 0$ (where $\phi$ obviously assumes its minimum).
\end{prf}

\begin{prf}\label{sol_prop_jm_pd}(Solution to~\ref{prop_jm_pd})
By proposition \ref{prop_mrlt_bas} it suffices to show that
  \[ [df_a] e^l = [f_k^j(a)] e^l \]
for $1 \le l \le n$.  Since the $i^{\text{th}}$ coordinate ($1 \le i \le m$) of the vector
which results from the action of the matrix $[f_k^j(a)]$ on the vector $e^l$ is
  \[ \sum_{k=1}^n f_k^i(a)(e^l)_k = f_l^i(a) \]
we see that
  \begin{alignat*}{2}
     [f_k^j(a)] e^l &= \sum_{i=1}^m f_l^i(a) \hat e^i & & \\
                    &= f_l(a)      &    &\quad\text{(by proposition~\ref{prop_pd_vvf})} \\
                    &= df_a(e^l)   &    &   \\
                    &= [df_a] e^l. &    &
  \end{alignat*}  \qedhere
\end{prf}




\begin{prf}\label{sol_exer_jm_comp}(Solution to~\ref{exer_jm_comp})

(a) By proposition~\ref{prop_jm_pd}
  \[ [df_{(w,x,y,z)}] = \begin{bmatrix}
                                xz      &   wz   &      0       &        wx      \\
                                 0      &   2x   &      4y      &        6z      \\
                            y\arctan z  &    0   &  w\arctan z  &  wy(1+z^2)^{-1}
  \end{bmatrix}\,. \]
Therefore
  \[ [df_a] = \begin{bmatrix}
                   1    &   1   &    0    &   1   \\
                   0    &   2   &    4    &   6   \\
                 \pi/4  &   0   &  \pi/4  &  1/2
  \end{bmatrix}\,. \]

(b)
  \begin{align*}
          df_a(v) &= [df_a] v             \\
                  &= \begin{bmatrix}
                          1    &   1   &    0    &   1   \\
                          0    &   2   &    4    &   6   \\
                        \pi/4  &   0   &  \pi/4  &  1/2
                     \end{bmatrix}
                                      \begin{bmatrix}
                                           0  \\
                                           2  \\
                                          -3  \\
                                           1
                                      \end{bmatrix}  \\
                  &= \bigl(3,-2, \tfrac14(2-3\pi)\bigr)\,.     \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_crpd_not}(Solution to~\ref{exer_crpd_not})
Let
  \[ g \colon \R^4 \sto \R^2 \colon (u,v,w,x) \mapsto \bigl(y(u,v,w,x), z(u,v,w,x)\bigr) \]
and
  \[ f\colon \R^2 \sto \R^4\colon  (s,t) \mapsto \bigl(u(s,t),v(s,t),w(s,t),x(s,t)\bigr)\,. \]
Here it is appropriate to think of the variables and functions as being arranged in the
following fashion.
  \begin{equation*}
   \begin{CD}
    \begin{matrix} s \\ t \end{matrix}  @>f>>
    \begin{matrix} u \\ v \\ w \\ x \end{matrix} @>g>>
    \begin{matrix} y \\ z \end{matrix}
   \end{CD}
  \end{equation*}
The expression $\pd ut$ is then taken to represent the function~$f_2^1$.  The expression $\pd
zu$ appearing in the statement of the exercise represents $g_1^2 \circ f$.  [One's first
impulse might be to let $\pd zu$ be just~$g_1^2$.  But this cannot be correct.  The product of
$\pd zu$ and $\pd ut$ is defined only at points where both are defined.  The product of
$g_1^2$ (whose domain lies in $\R^4$) and $f_2^1$ (whose domain is in $\R^2$) is never
defined.]  On the left side of the equation the expression $\pd zt$ is the partial derivative
with respect to $t$ of the composite function $f \circ g$.  Thus it is expressed functionally
as $(g \circ f)_2^2$.

Using proposition~\ref{prop_cr_pd} we obtain
  \begin{align*}
    \pd zt &= (g \circ f)_2^2 \\
           &= \sum_{i=1}^4 (g_i^2 \circ f) f_2^i \\
           &= \pd zu \pd ut + \pd zv \pd vt + \pd zw \pd wt + \pd zx \pd xt
  \end{align*}
This equation is understood to hold at all points $a$ in $\R^2$ such that $f$ is
differentiable at $a$ and $g$ is differentiable at~$f(a)$.
\end{prf}

\begin{prf}\label{sol_exer_crpd_xmp}(Solution to~\ref{exer_crpd_xmp})
Since
  \begin{equation*}
    \bigl[df_{(x,y,z)}\bigr] =  \begin{bmatrix}
                                     y^2 & 2xy &   0 \\
                                       3 &   0 & -2z \\
                                      yz &  xz &  xy \\
                                      2x &  2y &   0 \\
                                      4z &   0 &  4x
                                \end{bmatrix}
  \end{equation*}
we see that
  \begin{equation*}
    \bigl[df_a\bigr] = \begin{bmatrix}
                            0 &  0 &  0 \\
                            3 &  0 &  2 \\
                            0 & -1 &  0 \\
                            2 &  0 &  0 \\
                           -4 &  0 &  4
                       \end{bmatrix}\,.
  \end{equation*}
And since
  \begin{equation*}
    \bigl[dg_{(s,t,u,v,w)}\bigr] = \begin{bmatrix}
                                      2s &     0 & 2u &  2v &    0 \\
                                     2sv & -2w^2 &  0 & s^2 & -4tw
                                   \end{bmatrix}
  \end{equation*}
we see that
  \begin{equation*}
    \bigl[dg_{f(a)}\bigr] = \bigl[dg_{(0,2,0,1,1)}\bigr] =
                    \begin{bmatrix}
                          0 &  0 &  0 &  2 &  0 \\
                          0 & -2 &  0 &  0 & -8 \\
                    \end{bmatrix}\,.
  \end{equation*}
Thus by equation~\eqref{eqn_cr_sv1}
  \begin{align*}
   \bigl[d(g \circ f)_a\bigr]
                &= \bigl[dg_{f(a)}\bigr] \bigl[df_a\bigr] \\
                &= \begin{bmatrix}
                        0 &  0 &  0 &  2 &  0 \\
                        0 & -2 &  0 &  0 & -8 \\
                   \end{bmatrix}
                            \begin{bmatrix}
                                 0 &  0 &  0 \\
                                 3 &  0 &  2 \\
                                 0 & -1 &  0 \\
                                 2 &  0 &  0 \\
                                -4 &  0 &  4
                            \end{bmatrix} \\
                                 &= \begin{bmatrix}
                                         4 & 0 &   0 \\
                                        26 & 0 & -36 \\
                                    \end{bmatrix}\,.   \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_crpd_xmp2}(Solution to~\ref{exer_crpd_xmp2})
Use formula~\eqref{eqn_dwdt3}.  It is understood that $\pd xt$ and $\pd yt$ must be evaluated
at the point $(1,1)$; and since $x(1,1) = 2$ and $y(1,1) = \pi/4$, the partials $\pd wx$, $\pd
wy$, and $\cpd wt{x,y}$ are to be evaluated at the point $(2,\pi/4,1)$.  Calculate the terms
appearing on the right hand side of~\eqref{eqn_dwdt3}:
  \begin{alignat*}{2}
    \pd xt &= 2t;              & \qquad   &\text{so } \pd xt(1,1) = 2,    \\
    \pd yt &= \frac s{1+t^2};  & \qquad   &\text{so } \pd yt(1,1) = 1/2,  \\
    \pd wx &= -\frac{2y}{x^2}; & \qquad   &\text{so } \pd wx(2,\pi/4,1) = -\pi/8, \\
    \pd wy &= \frac2x;         & \qquad   &\text{so } \pd wy(2,\pi/4,1) = 1, \text{ and} \\
       \cpd wt{x,y} &= 3t^2;   & \qquad   &\text{so } \cpd wt{x,y}(2,\pi/4,1) = 3\,.
  \end{alignat*}
Therefore
  \[ \cpd wts(1,1) = -\frac\pi8\cdot2 + 1\cdot\frac12 + 3
                   = \frac72 - \frac\pi4. \qedhere  \]
\end{prf}

\begin{prf}\label{sol_exer_crpd_xmp3}(Solution to~\ref{exer_crpd_xmp3})
We proceed through steps (a)--(g) of the hint.
 \begin{enumerate}
  \item[(a)] Define $g(x,y) = y/x$ and compute its differential
   \begin{align*}
         \bigl[dg_{(x,y)}\bigr] &= \bigl[g_1(x,y)\qquad g_2(x,y)\bigr]  \\
                                &= [-yx^{-2} \qquad x^{-1}\bigr].
   \end{align*}
  \item[(b)] Then compute the differential of $\phi \circ g$
   \begin{align*}
      \bigl[d(\phi \circ g)_{(x,y)}\bigr] &= \bigl[d\phi_{g(x,y)}\bigr] \bigl[dg_{(x,y)}\bigr]\\
                                          &= \phi'\bigl(g(x,y)\bigr)\bigl[dg_{(x,y)}\bigr] \\
                                          &= \phi'(yx^{-1})\bigl[-yx^{-2}\qquad x^{-1}\bigr] \\
                                          &= \bigl[-yx^{-2}\phi'(yx^{-1}) \qquad
                                                              x^{-1}\phi'(yx^{-1})\bigr].  \\
   \end{align*}
  \item[(c)]Let $G(x,y) = \bigl(x,\phi(y/x)\bigr)$ and use (b) to calculate~$\bigl[dG_{(x,y)}\bigr]$
   \begin{align*}
         \bigl[dG_{(x,y)}\bigr] &= \begin{bmatrix}
                                       G_1^1(x,y)  &  G_2^1(x,y) \\
                                       G_1^2(x,y)  &  G_2^2(x,y) \\
                                   \end{bmatrix}  \\
                                &= \begin{bmatrix}
                                       1           &             0         \\
                            -yx^{-2}\phi'(yx^{-1}) &  x^{-1}\phi'(yx^{-1})
                                   \end{bmatrix}
   \end{align*}
  \item[(d)] Let $m(x,y) = xy$ and compute its differential
      \[\bigl[dm_{(x,y)}\bigr] = \bigl[ m_1(x,y) \qquad m_2(x,y)\bigr]
                               = \bigl[ y \qquad x \bigr]\,. \]
  \item[(e)] Since $h(x,y) = x\phi(yx^{-1}) = m(G(x,y))$ we see that  $h = m \circ G$ and therefore
   \begin{align*}
      \bigl[dh_{(x,y)}\bigr]
           &= \bigl[d(m \circ G)_{(x,y)}\bigr] \\
           &= \bigl[dm_{G(x,y)}\bigr] \bigl[dG_{(x,y)}\bigr]\\
           &= \bigl[G^2(x,y) \qquad  G^1(x,y)\bigr] \bigl[dG_{(x,y)}\bigr]  \\
           &= \bigl[\phi(yx^{-1}) \qquad  x \bigr]
                         \begin{bmatrix}
                               1           &             0         \\
                    -yx^{-2}\phi'(yx^{-1}) &  x^{-1}\phi'(yx^{-1})
                         \end{bmatrix}  \\
          &= \bigl[ \phi(yx^{-1} - yx^{-1}\phi'(yx^{-1}) \qquad  \phi'(yx^{-1}) \bigr]\,.
   \end{align*}
  \item[(f)] Since $j(x,y) = xy + x\phi(yx^{-1}) = m(x,y) + h(x,y)$, we see that
   \begin{align*}
         \bigl[dj_{(x,y)}\bigr]
              &= \bigl[dm_{(x,y)}\bigr] + \bigl[dh_{(x,y)}\bigr]\\
              &= \bigl[y + \phi(yx^{-1} - yx^{-1}\phi'(yx^{-1}) \qquad x + \phi'(yx^{-1}) \bigr]\,.
   \end{align*}
  \item[(g)] Then finally,
    \begin{align*}
        xj_1(x,y) + yj_2(x,y)
          &= x\bigl(y + \phi(yx^{-1} - yx^{-1}\phi'(yx^{-1})\bigr) + y\bigl(x + \phi'(yx^{-1})\bigr) \\
          &= xy + x\phi(yx^{-1}) + yx  \\
          &= xy + j(x,y)     \qedhere
    \end{align*}
 \end{enumerate}
\end{prf}

\begin{prf}\label{sol_exer_crpd_xmp4}(Solution to~\ref{exer_crpd_xmp4})
Let $h$ be as in the hint.  Then
  \[ \bigl[dh_{(x,y)}\bigr] = \begin{pmatrix}
                                   2x &   -2y  \\
                                   2y &    2x
                              \end{pmatrix} \]
so
  \begin{align*}
     \bigl[dg_{(x,y)}\bigr]
         &= \bigl[d(f \circ h)_{(x,y)}\bigr] \\
         &= \bigl[df_{h(x,y)}\bigr]\bigl[dh_{(x,y)}\bigr]\\
         &= \bigl[f_1(h(x,y)) \qquad f_2(h(x,y))\bigr]
             \begin{bmatrix}
                  2x  &   -2y  \\
                  2y  &    2x
             \end{bmatrix}  \\
         &= \bigl[2xf_1(h(x,y)) + 2y f_2(h(x,y))  \qquad
               -2yf_1(h(x,y)) + 2xf_2(h(x,y))\bigr]\,.
  \end{align*}
Therefore
  \begin{align*}
        yg_1(x,y) - xg_2(x,y)
          &= 2xyf_1(h(x,y)) + 2y^2f_2(h(x,y)) + 2xyf_1(h(x,y)) - 2x^2f_2(h(x,y))  \\
          &= 4xyf_1(h(x,y)) - 2(x^2-y^2)f_2(h(x,y)) \\
          &= 2h^2(x,y)f_1(h(x,y)) - 2h^1(x,y)f_2(h(x,y))\,.
  \end{align*}
This computation, incidentally, gives one indication of the attractiveness of notation
which omits evaluation of partial derivatives. If one is able to keep in mind the points
at which the partials are being evaluated, less writing is required.
\end{prf}






%END chapter 27




















\section{Exercises in chapter 28}

\begin{prf}\label{sol_exer_par_sum1}(Solution to~\ref{exer_par_sum1})
If $n$ is odd then the $n^{\text{th}}$ partial sum $s_n$ is~$1$; if $n$ is even then $s_n = 0$.
\end{prf}

\begin{prf}\label{sol_exer_par_sum2}(Solution to~\ref{exer_par_sum2})
Use problem~\ref{prob_geomser2}.  The $n^{\text{th}}$ partial sum of the sequence
$\bigl(\frac12, \frac14, \frac18,\dots\bigr)$ is
  \begin{align*}
       s_n &= \frac12 + \frac14 + \frac18 + \dots + \frac1{2^n} \\
           &= \sum_{k=1}^n \left(\frac12\right)^k \\
           &= \sum_{k=0}^n \left(\frac12\right)^k - 1 \\
           &= \frac{1 - (\frac12)^{n+1}}{1 - \frac12} - 1 \\
           &= 2 - \left(\frac12\right)^n - 1 \\
           &= 1 - 2^{-n}.   \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_exer_inf_ser1}(Solution to~\ref{exer_inf_ser1})
For the sequence given in exercise~\ref{exer_par_sum1}, the corresponding series
$\sum_{k=1}^\infty a_k$ is the sequence $(1,0,1,0,1,\dots)$ (of partial sums).  For the
sequence in exercise~\ref{exer_par_sum2}, the series $\sum_{k=1}^\infty a_k$ is the sequence
$\bigl(\frac12, \frac34, \frac78,\dots\bigr)$ (of partial sums).
\end{prf}

\begin{prf}\label{sol_exer_inf_ser2}(Solution to~\ref{exer_inf_ser2})
For the sequence $(a_k)$ given in~\ref{exer_par_sum1} the corresponding sequence of
partial sums $(1,0,1,0,1,\dots)$ does not converge. Thus the sequence $(1,-1,1,-1,\dots)$
is not summable. Equivalently, the series $\sum_{k=1}^\infty (-1)^{k+1}$ diverges.

For the sequence $(a_k)$ of~\ref{exer_par_sum2}, the $n^{\text{th}}$ partial sum is $1 -
2^{-n}$ (see~\ref{exer_par_sum2}).  Since $\lim_{n \sto \infty} s_n = \lim_{n \sto
\infty} \bigl(1 - 2^{-n}\bigr) = 1$ (see proposition~\ref{prop_lim_pwrs}), we conclude
that the sequence $(1/2, 1/4, 1/8, \dots)$ is summable; in other words, the series
$\sum_{k=1}^\infty 2^{-k}$ converges.  The sum of this series is~$1$; that is
  \[ \sum_{k=1}^\infty 2^{-k} = 1\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_terms_zero}(Solution to~\ref{prop_terms_zero})
Suppose that $\sum_{k=1}^\infty a_k = b$.  If $s_n = \sum_{k=1}^n a_k$, then it is easy to see
that for each $n$ we may write $a_n$ as $s_n - s_{n-1}$ (where we let $s_0 = 0$).  Take limits
as $n \sto \infty$ to obtain
  \[ a_n = s_n - s_{n-1} \sto b - b = 0\,.    \qedhere \]
\end{prf}

\begin{prf}\label{sol_exam_harm_ser}(Solution to~\ref{exam_harm_ser})
Assume that the series $\sum_{k=1}^\infty k^{-1}$ converges.  Let $s_n = \sum_{k=1}^n k^{-
1}$.  Since the sequence $(s_n)$ of partial sums is assumed to converge, it is Cauchy (by
proposition~\ref{conv_cau}).  Thus there exists an index $p$ such that $\abs{s_n - s_p} <
\frac12$ whenever $n \ge p$.  We obtain a contradiction by noting that
  \begin{align*}
            \abs{s_{2p} - s_p}
                     &= \sum_{k=p+1}^{2p}\frac1k \\
                     &\ge \sum_{k=p+1}^{2p}\frac1{2p} \\
                     &= \frac p{2p} \\
                     &= \frac12\,.            \qedhere
  \end{align*}
\end{prf}

\begin{prf}\label{sol_prop_Cauchy_crit}(Solution to~\ref{prop_Cauchy_crit})
Let $\sum a_k$ be a convergent series in the normed linear space~$V$.  For each $n$ in $\N$
let $s_n = \sum_{k=1}^n a_k$.  Then $(s_n)$ is a convergent sequence.  By
proposition~\ref{conv_cau} it is Cauchy. Thus given $\epsilon > 0$ we may choose $n_0$ in $\N$
so that $n > m \ge n_0$ implies
  \begin{equation}\label{eqn_Cauchy_crit}
     \bignorm{\sum_{k=m+1}^n a_k} = \norm{s_n - s_m} < \epsilon\,.
  \end{equation}

For the second assertion of the proposition, suppose that $V$ is complete.  Suppose further
that $(a_k)$ is a sequence in $V$ for which there exists $n_0 \in \N$ such
that~\eqref{eqn_Cauchy_crit} holds whenever $n > m \ge n_0$. (As above, $s_n = \sum_{k=1}^n
a_k$.) This says that the sequence $(s_n)$ of partial sums is Cauchy, and since $V$ is
complete, the sequence $(s_n)$ converges. That is, the series $\sum a_k$ converges.
\end{prf}

\begin{prf}\label{sol_exer_M_test}(Solution to~\ref{exer_M_test})
Let $f_n(x) = x^n\bigl(1 + x^n\bigr)^{-1}$ for every $n \in \N$ and $x \in [-\delta, \delta]$.
Also let $M_n = \delta^n(1 - \delta)^{-1}$.  Since $0 < \delta < 1$, the series $\sum M_n =
\sum \delta^n(1 - \delta)^{-1}$ converges (by problem~\ref{prob_geomser2}).  For $\abs x \le
\delta$, we have $-x^n \le \abs x^n \le \delta^n \le \delta$; so $x^n \ge -\delta$ and $1 +
x^n \ge 1 - \delta$.  Thus
  \[ \abs{f_n(x)} = \frac{\abs x^n}{1 + x^n} \le \frac{\delta^n}{1 - \delta} = M_n\,. \]
Thus
  \[ \norm{f_n}_u = \sup\{\abs{f_n(x)}\colon \abs x \le \delta\} \le M_n \]
By the \emph{Weierstrass M-test} (proposition~\ref{M_test}), the series $\sum_{k=1}^\infty
f_k$ converges uniformly.
\end{prf}

\begin{prf}\label{sol_ratio_test}(Solution to~\ref{ratio_test})
As was remarked after proposition~\ref{prop_Cauchy_crit}, the convergence of a series is not
affected by altering any finite number of terms. Thus without loss of generality we suppose
that $a_{k+1} \le \delta a_k$ for all~$k$.  Notice that $a_2 \le \delta a_1$, $a_3 \le \delta
a_2 \le \delta^2 a_1$, $a_4 \le \delta^3 a_1$, \emph{etc.} In general, $a_k \le \delta^{k-1}
a_1$ for all~$k$. The geometric series $\sum \delta^{k-1}$ converges by
problem~\ref{prob_geomser2}. Thus by the \emph{comparison test} (proposition~\ref{comp_test}),
the series $\sum a_k$ converges.  The second conclusion follows similarly from the
observations that $a_k \ge M^{k-1} a_1$ and that $\sum M^{k-1}$ diverges.
\end{prf}

\begin{prf}\label{sol_prop_compl_abssum}(Solution to~\ref{prop_compl_abssum})
Suppose that $V$ is complete and that $(a_k)$ is an absolutely summable sequence in~$V$.  We
wish to show that $(a_k)$ is summable.  Let $\epsilon > 0$.  Since $\sum\norm{a_k}$ converges
in $\R$ and $\R$ is complete, we may invoke the \emph{Cauchy criterion}
(proposition~\ref{prop_Cauchy_crit}) to find an integer $n_0$ such that $n > m \ge n_0$
implies $\sum_{k=m+1}^n \norm{a_k} < \epsilon$.  But for all such $m$ and $n$
  \[ \bignorm{\sum_{k=m+1}^n a_k} \le \sum_{k=m+1}^n \norm{a_k} < \epsilon\,. \]
This, together with the fact that $V$ is complete, allows us to apply for a second time the
\emph{Cauchy criterion} and to conclude that $\sum a_k$ converges.  That is, the sequence
$(a_k)$ is summable.

For the converse suppose that every absolutely summable sequence in $V$ is summable.  Let
$(a_k)$ be a Cauchy sequence in~$V$.  In order to prove that $V$ is complete we must show that
$(a_k)$ converges.  For each $k$ in $\N$ we may choose a natural number $p_k$ such that
$\norm{a_n - a_m} \le 2^{-k}$ whenever $n > m \ge p_k$.  Choose inductively a sequence $(n_k)$
in $\N$ as follows. Let $n_1$ be any integer such that $n_1 \ge p_1$.  Having chosen integers
$n_1 < n_2 < \dots < n_k$ in $\N$ so that $n_j \ge p_j$ for $1 \le j \le k$, choose $n_{k+1}$
to be the larger of $p_{k+1}$ and $n_k + 1$.  Clearly, $n_{k+1} > n_k$ and $n_{k+1} \ge
p_{k+1}$. Thus $\bigl(a_{n_k}\bigr)$ is a subsequence of $(a_n)$ and (since $n_{k+1} > n_k \ge
p_k$ for each $k$) $\bignorm{a_{n_{k+1}} - a_{n_k}} < 2^{-k}$ for each $k$ in~$\N$. Let $y_k =
a_{n_{k+1}} - a_{n_k}$ for each~$k$.  Then $(y_n)$ is absolutely summable since
$\sum\norm{y_k} < \sum 2^{-k} = 1$. Consequently $(y_k)$ is summable in~$V$.  That is, there
exists $b$ in $V$ such that $\sum_{k=1}^j y_k \sto b$ as $j \sto \infty$. However, since
$\sum_{k=1}^j y_k = a_{n_{j+1}} - a_{n_1}$, we see that
  \[ a_{n_{j+1}} \sto a_{n_1} + b \qquad \text{as }j \sto \infty\,. \]
This shows that $\bigl(a_{n_k}\bigr)$ converges.  Since $(a_n)$ is a Cauchy sequence having a
convergent subsequence it too converges (proposition~\ref{cs_css}).  But this is what we
wanted to show.
\end{prf}

\begin{prf}\label{sol_exer_bddfn_ba}(Solution to~\ref{exer_bddfn_ba})

(a) It follows immediately from
  \[ \abs{(fg)(x)} = \abs{f(x)}\abs{g(x)}
                   \le \norm f_u \norm g_u \qquad \text{for every $x \in S$} \]
that
  \[ \norm{fg}_u = \sup\{\abs{(fg)(x)}\colon x \in S\} \le \norm f_u\,\norm g_u\,. \]

(b) Define $f$ and $g$ on $[0,2]$ by
 \begin{equation*}
   f(x) =
   \begin{cases}
     1  & \text{if $0 \le x \le 1$}, \\
     0  & \text{if $1 < x \le 2$}
   \end{cases}
 \end{equation*}
and $g(x) = 1 - f(x)$.  Then $\norm f_u = \norm g_u = 1$, but $\norm{fg}_u = 0$.
\end{prf}

\begin{prf}\label{sol_prop_inv_ps}(Solution to~\ref{prop_inv_ps})
Since $\norm x < 1$, the series $\sum_{k=0}^\infty \norm x^k$ converges by
problem~\ref{prob_geomser2}.  Condition (e) in the definition of normed algebras is that
$\norm{xy} \le \norm x \, \norm y$.  An easy inductive argument shows that $\norm{x^n} \le
\norm x^n$ for all $n$ in~$\N$.  We know that $\norm x^n \sto 0$ (by
proposition~\ref{prop_lim_pwrs});  so $\norm{x^n} \sto 0$ also. Thus (by
proposition~\ref{prop_norm_cont}(d)) $x^n \sto 0$ as $n \sto \infty$.  Furthermore, comparing
$\sum_0^\infty \norm{x^k}$ with the series $\sum_0^\infty \norm x^k$ shows that the former
converges (see proposition~\ref{comp_test}). But this says just that the series $\sum_0^\infty
x^k$ converges absolutely.  It then follows from proposition~\ref{prop_compl_abssum} that
$\sum_0^\infty x^k$ converges.  Letting $s_n = \sum_{k=0}^n x^k$ we see that
  \begin{align*}
    \smash[b]{(\vc 1 - x)\sum_{k=0}^\infty x^k}
                  &= (\vc 1 - x)\lim s_n \\
                  &= \lim\bigl((\vc 1 - x)s_n\bigr) \\
                  &= \lim\bigl(\vc 1 - x^{n+1}\bigr) \\
                  &= \boldsymbol 1.
 \end{align*}
Similarly, $\bigl(\sum_0^\infty x^k\bigr)(\vc 1 - x) = \vc 1$. This shows that $\vc 1 - x$ is
invertible and that its inverse $(\vc 1 - x)^{-1}$ is the geometric series~$\sum_0^\infty x^k$.
\end{prf}

\begin{prf}\label{sol_prop_inv_cont}(Solution to~\ref{prop_inv_cont})
Let $a \in \inv A$. We show that $r$ is continuous at~$a$.  Given $\epsilon > 0$ choose
$\delta$ to be the smaller of the numbers $\frac12\norm{a^{-1}}^{-1}$ and
$\frac12\norm{a^{-1}}^{-2}\epsilon$. Suppose that $\norm{y - a} < \delta$ and prove that
$\norm{r(y) - r(a)} < \epsilon$.  Let $x = \vc 1 - a^{-1}y$.  Since
  \[ \norm x = \norm{a^{-1}a - a^{-1}y} \le \norm{a^{-1}} \norm{y-a}
                < \norm{a^{-1}}\delta
                \le \norm{a^{-1}}\tfrac12\norm{a^{-1}}^{-1}
                = \tfrac12 \]
we conclude from \ref{prop_inv_ps} and~\ref{cor_inv_ps} that $\vc 1 - x$ is invertible and
that
  \begin{equation}\label{eqn_inv_cont}
     \norm{(\vc 1 - x)^{-1} - \vc 1} \le \frac{\norm x}{1 - \norm x}
 \end{equation}
Thus
 \begin{alignat*}{2}
   \norm{r(y) - r(a)}
       &= \norm{y^{-1}(a - y)a^{-1}} &
                 &\quad\text{(by \ref{prop_prop_inv}(e))} \\
       &\le \norm{y^{-1}a - \vc 1}\, \norm{a^{-1}} && \\
       &= \norm{(a^{-1}y)^{-1} - \vc 1}\, \norm{a^{-1}} &
                 &\quad\text{(by \ref{prop_prop_inv}(d))} \\
       &= \norm{(\vc 1 - x)^{-1} - \vc 1} \,
                      \norm{a^{-1}} && \\
       &\le \frac{\norm x}{1 - \norm x} \norm{a^{-1}}  &
                 &\quad\text{(by inequality \eqref{eqn_inv_cont})} \\
       &\le 2 \norm x \,\norm{a^{-1}}  &
                 &\quad\text{(because $\norm x \le \tfrac12$)} \\
       &< 2 \norm{a^{-1}}^2    && \\
       &\le \epsilon.
 \end{alignat*}
\end{prf}

\begin{prf}\label{sol_thm_Mertens}(Solution to~\ref{thm_Mertens})
Throughout the proof we use the notation introduced in the hint.  To avoid triviality we
suppose that $(a_k)$ is not identically zero.  Since $u_n$ is defined to be $\sum_{k=0}^n c_k
= \sum_{k=0}^n \sum_{j=0}^k a_jb_{k-j}$ it is clear that $u_n$ can be obtained by finding the
sum of each column of the matrix $\bigl[d_{jk}\bigr]$ and then adding these sums. On the other
hand the expression
  \[ \sum_{k=0}^n a_{n-k}t_k = \sum_{k=0}^n\sum_{j=0}^k a_{n-k}b_j \]
is obtained by finding the sum of each row of the matrix $[d_{jk}]$ and then adding the sums.
It is conceivable that someone might find the preceding argument too ``pictorial'', depending
as it does on looking at a ``sketch'' of the matrix~$[d_{jk}]$.  It is, of course, possible to
carry out the proof in a purely algebraic fashion.  And having done so, it is also quite
conceivable that one might conclude that the algebraic approach adds more to the amount of
paper used than to the clarity of the argument.  In any event, here, for those who feel more
comfortable with it, is a formal verification of the same result.
  \begin{align*}
     u_n &= \sum_{k=0}^n c_k \\
         &= \sum_{k=0}^n\sum_{j=0}^k a_jb_{k-j} \\
         &= \sum_{k=0}^n\sum_{j=0}^k d_{jk} \\
         &= \sum_{k=0}^n\sum_{j=0}^n d_{jk} \\
         &= \sum_{j=0}^n\sum_{k=0}^n d_{jk} \\
         &= \sum_{j=0}^n\sum_{k=j}^n d_{jk} \\
         &= \sum_{j=0}^n\sum_{k=j}^n a_jb_{k-j} \\
         &= \sum_{j=0}^n a_j \sum_{r=0}^{n-j} b_r \\
         &= \sum_{j=0}^n a_j t_{n-j} \\
         &= \sum_{k=0}^n a_{n-k}t_k .
  \end{align*}
Now that equation~\eqref{eqn_Mertens1} has been established we see that
  \begin{align*}
      u_n &= \sum_{k=0}^n a_{n-k}b + \sum_{k=0}^n a_{n-k}(t_k - b) \\
          &= s_nb + \sum_{k=0}^n a_{n-k}(t_k - b).
  \end{align*}
Since $s_nb \sto ab$, it remains only to show that the last term on the right approaches $0$
as $n \sto \infty$.  Since
  \begin{align*}
        \bignorm{\sum_{k=0}^n a_{n-k}(t_k - b)}
             &\le \sum_{k=0}^n \norm{a_{n-k}(t_k - b)} \\
             &\le \sum_{k=0}^n \norm{a_{n-k}}\,\norm{(t_k - b)} \\
             &= \sum_{k=0}^n \alpha_{n-k}\beta_k
  \end{align*}
it is sufficient to prove that given any $\epsilon > 0$ the quantity $\sum_{k=0}^n
\alpha_{n-k}\beta_k$ is less than $\epsilon$ whenever $n$ is sufficiently large.

Let $\alpha = \sum_{k=0}^\infty \norm{a_k}$.  Then $\alpha > 0$. Since $\beta_k \sto 0$ there
exists $n_1$ in $\N$ such that $k \ge n_1$ implies $\beta_k < \epsilon/(2\alpha)$.  Choose
$\beta > \sum_{k=0}^{n_1} \beta_k$.   Since $\alpha_k \sto 0$, there exists $n_2$ in $\N$ such
that $k \ge n_2$ implies $\alpha_k < \epsilon/(2\beta)$.

Now suppose that $n \ge n_1 + n_2$.  If $0 \le k \le n_1$, then $n - k \ge n -n_1 \ge n_2$, so
that $\alpha_{n-k} < \epsilon/(2\beta)$.  This shows that
  \begin{align*}
          p &= \sum_{k=0}^{n_1} \alpha_{n-k}\beta_k \\
            &\le \epsilon(2\beta)^{-1}\sum_{k=0}^{n_1}\beta_k \\
            &< \epsilon/2.
  \end{align*}
Furthermore,
  \begin{align*}
      q &= \sum_{k=n_1+1}^n \alpha_{n-k}\beta_k \\
        &\le \epsilon(2\alpha)^{-1}\sum_{k=n_1+1}^n \alpha_{n-k}\\
        &\le \epsilon(2\alpha)^{-1}\sum_{j=0}^\infty \norm{a_j} \\
        &= \epsilon/2 .
  \end{align*}
Thus
  \[ \sum_{k=0}^n \alpha_{n-k}\beta_k  =  p + q < \epsilon\,.  \qedhere \]
\end{prf}

\begin{prf}\label{sol_prop_pser_unif_conv}(Solution to~\ref{prop_pser_unif_conv})
Let $0 < s < r$, let $M > 0$ be such that $\norm{a_k}r^k \le M$ for every $k$ in~$\N$, and let
$\rho = s/r$.  Let $f_k(x) = a_kx^k$ for each $k$ in $\N$ and $x$ in $B_s(0)$.  For each such
$k$ and $x$
  \begin{align*}
    \norm{f_k(x)}
      &= \norm{a_kx^k} \le \norm{a_k}\norm x^k \le \norm{a_k}s^k\\
      &= \norm{a_k}r^k\rho^k  \le  M\rho^k.
  \end{align*}
Thus $\norm{f_k}_u \le M\rho^k$ for each~$k$.  Since $0 < \rho < 1$, the series $\sum M\rho^k$
converges.  Then, according to the \emph{Weierstrass M-test} (proposition~\ref{M_test}), the
series $\sum a_kx^k = \sum f_k(x)$ converges uniformly on~$B_s(0)$.  The parenthetical comment
in the statement of the proposition is essentially obvious: For $a \in B_r(0)$ choose $s$ such
that $\norm a < s < r$. Since $\sum a_kx^k$ converges uniformly on $B_s(0)$, it converges
at~$a$ (see problem~\ref{prob_uc_pwc}).
\end{prf}

\begin{prf}\label{sol_unif_lim_dffrntls}(Solution to~\ref{unif_lim_dffrntls})
Let $a$ be an arbitrary point of~$U$.  Let $\phi = \lim_{n \sto \infty} d(f_n)$. We show that
$\Delta F_a \simeq T$ where $T = \phi(a)$. We are supposing that $d(f_n) \sto
\phi\text{\,(unif)}$ on~$U$.  Thus given $\epsilon > 0$ we may choose $N$ in $\N$ so that
  \[ \sup\bigl\{\bignorm{d\bigl(f_n\bigr)_x - \phi(x)} \colon x \in U\bigr\}
                                                          < \tfrac18\epsilon \]
whenever $x \in U$ and $n \ge N$.  Let $g_n = f_n - f_N$ for all $n \ge N$.  Then for all such
$n$ and all $x \in U$ we have
  \[ \bignorm{d\bigl(g_n\bigr)_x}
           \le \bignorm{d\bigl(f_n\bigr)_x - \phi(x)} + \bignorm{\phi(x) - d\bigl(f_N\bigr)_x}
           < \tfrac14\epsilon\,. \]
Also it is clear that
  \[ \bignorm{d\bigl(g_n\bigr)_x - d\bigl(g_n\bigr)_a} \le \bignorm{d\bigl(g_n\bigr)_x}
                               + \bignorm{d\bigl(g_n\bigr)_a} < \tfrac12\epsilon \]
for $x \in U$ and $n \ge N$.  According to corollary~\ref{cor_mvt_nls}
  \[ \bignorm{\Delta\bigl(g_n\bigr)_a(h) - d\bigl(g_n\bigr)_a(h)}
                                         \le \tfrac12 \epsilon \norm h \]
whenever $n \ge N$ and $h$ is a vector such that $a + h \in U$. Thus
  \[ \bignorm{\Delta\bigl(f_n\bigr)_a(h) - d\bigl(f_n\bigr)_a(h) - \Delta\bigl(f_N\bigr)_a(h)
            + d\bigl(f_N\bigr)_a(h)} \le \tfrac12\epsilon\norm h \]
when $n \ge N$ and $a + h \in U$.  Taking the limit as $n \sto \infty$ we obtain
  \begin{equation}\label{eqn_uldffr1}
     \bignorm{(\Delta F_a(h) - Th) - \bigl(\Delta\bigl(f_N\bigr)_a(h)
                   - d\bigl(f_N\bigr)_a(h)\bigr)} \le \frac12\epsilon\norm h
  \end{equation}
for $h$ such that $a+h \in U$.  Since $f_N$ is differentiable, $\Delta\bigl(f_N\bigr)_a \simeq
d\bigl(f_N\bigr)_a$; thus there exists $\delta > 0$ such that $B_\delta(a) \subseteq U$ and
  \begin{equation}\label{eqn_uldffr2}
    \bignorm{\Delta\bigl(f_N\bigr)_a(h) - d\bigl(f_N\bigr)_a(h)} < \tfrac12\epsilon\norm h
  \end{equation}
for all $h$ such that $\norm h < \delta$. From~\eqref{eqn_uldffr1} and~\eqref{eqn_uldffr2} it
is clear that
  \[ \norm{\Delta F_a(h) - Th}  <  \epsilon\norm h \]
whenever $\norm h < \delta$. Thus $\Delta F_a \simeq T$, which shows that $F$ is
differentiable at $a$ and
  \[ dF_a = T = \lim_{n \sto \infty} d\bigl(f_n\bigr)_a\,.    \qedhere \]
\end{prf}






%END chapter 28

















\section{Exercises in chapter 29}

\begin{prf}\label{sol_exer_not_c1}(Solution to~\ref{exer_not_c1})
Let $U = V = \R$ and $f(x) = x^3$ for all $x$ in~$\R$.  Although $f$ is continuously
differentiable and does have an inverse, it is not $\fml C^1$-invertible. The inverse function
$x \mapsto x^{\frac13}$ is not differentiable at~$0$.
\end{prf}

\begin{prf}\label{sol_exer_c1_inv}(Solution to~\ref{exer_c1_inv})
Set $y = x^2 - 6x + 5$ and solve for $x$ in terms of~$y$.  After completing the square and
taking square roots we have
  \[ \abs{x - 3} = \sqrt{y + 4} \,. \]
Thus there are two solutions $x = 3 + \sqrt{y + 4}$ and $x = 3 - \sqrt{y + 4}$. The first of
these produces values of $x$ no smaller than $3$ and the second produces values no larger
than~$3$. Thus for $x=1$ we choose the latter.  A local $\fml C^1$-inverse of $f$ is given on
the interval $f^{\sto}(0,2) = (-3,5)$ by
  \[ \locinv f(y) = 3 - \sqrt{y+4} \,.    \qedhere \]
\end{prf}

\begin{prf}\label{sol_exer_der_finv}(Solution to~\ref{exer_der_finv})
In order to apply the chain rule to the composite function $\locinv f \circ f$ we need to know
that \emph{both} $f$ and $\locinv f$ are differentiable. But differentiability of $\locinv f$
was not a hypothesis.  Indeed, the major difficulty in proving the \emph{inverse function
theorem} is showing that a local $\fml C^1$-inverse of a $\fml C^1$-function is in fact
differentiable (at points where its differential does not vanish).  Once that is known, the
argument presented in~\ref{exer_der_finv} correctly derives the formula for $D\locinv f(b)$.
\end{prf}

\begin{prf}\label{sol_xmpl_ift_12}(Solution to~\ref{xmpl_ift_12})
Let $f(x,y) = x^2y + \sin(\frac{\pi}2 xy^2) - 2$ for all $x$ and $y$ in~$\R$.
 \begin{enumerate}
  \item[(a)]  There exist a neighborhood $V$ of $1$ and a function $h \colon V \sto \R$
which satisfy
     \begin{alignat*}{2}
       &\text{(i)}  &&\quad h(1) = 2; \quad\text{ and} \\
       &\text{(ii)} &&\quad f(x,h(x)) = 0  \quad\text{ for all $x$ in $V$.}
     \end{alignat*}
  \item[(b)] Let $G(x,y) = (x,f(x,y))$ for all $x,y \in \R$. Then $G$ is continuously
differentiable and
  \[ \begin{bmatrix} dG_{(1,2)} \end{bmatrix} =
                    \begin{bmatrix}   1   &    0  \\
                                4 + 2\pi  &  1 + 2\pi
                    \end{bmatrix} \,. \]
Thus $dG_{(1,2)}$ is invertible, so by the \emph{inverse function theorem} $G$ has a local
$\fml C^1$-inverse, say $H$, defined on some neighborhood $W$ of $(1,0) = G(1,2)$. Let $V =
\{x:(x,0) \in W\}$ and $h(x) = H^2(x,0)$ for all $x$ in~$V$.  The function $h$ is in $\fml
C^1$ because $H$ is. Condition (i) is satisfied by $h$ since
  \begin{align*}
     (1,2) &= H(G(1,2)) \\
           &= H(1,f(1,2)) \\
           &= H(1,0) \\
           &= (H^1(1,0)\,,\,H^2(1,0)) \\
           &= (H^1(1,0)\,,\,h(1))
  \end{align*}
and (ii) holds because
  \begin{align*}
     (x,0) &= G(H(x,0)) \\
           &= G(H^1(x,0)\,,\,H^2(x,0)) \\
           &= (H^1(x,0)\,,\,f(H^1(x,0),H^2(x,0))) \\
           &= (x\,,\,f(x,h(x)))
  \end{align*}
for all $x$ in $V$.
  \item[(c)] Let $G$, $H$, and $h$ be as in (b). By the \emph{inverse function theorem}
%\[\spreadlines{1\jot}
  \begin{align*}
    \begin{bmatrix} dH_{(1,0)} \end{bmatrix}
                    &= \begin{bmatrix} dH_{G(1,2)} \end{bmatrix} \\
                    &= {\begin{bmatrix} dG_{(1,2)} \end{bmatrix}}^{-1} \\
                    &= {\begin{bmatrix}
                               1     &     0   \\
                           4 + 2\pi  &   1 + 2\pi
                        \end{bmatrix}}^{-1} \\
                    &= {\begin{bmatrix}
                               1     &       0 \\
      -\frac{4 + 2\pi}{1 + 2\pi}     &   \frac1{1 + 2\pi}
                        \end{bmatrix}} \,.
  \end{align*}
Then $\frac{dy}{dx}$ at $(1,2)$ is just $h'(1)$ and
    \[ h'(1) = H^2_1(1,0) = -\frac{4+2\pi}{1+2\pi} \,.   \qedhere  \]
 \end{enumerate}
\end{prf}

\begin{prf}\label{sol_xmpl_ift_13}(Solution to~\ref{xmpl_ift_13})
Let $f(x,y,z) = x^2z + yz^2 - 3z^3 - 8$ for all $x$, $y$, $z \in \R$.
 \begin{enumerate}
  \item[(a)] There exist a neighborhood $V$ of $(3,2)$ and a function $h \colon V \sto \R$
which satisfy
    \begin{alignat*}{2}
      &\text{(i)}  &&\quad h(3,2) = 1; \quad\text{ and} \\
      &\text{(ii)} &&\quad f(x,y,h(x,y)) = 0 \quad\text{ for all $x$, $y  \in V.$}
    \end{alignat*}

  \item[(b)] Let $G(x,y,z) := (x,y,f(x,y,z))$ for all $x$, $y$, $z \in \R$. Then
     \[\begin{bmatrix} dG_{(3,2,1)} \end{bmatrix} =
             \begin{bmatrix}  1  &  0  &  0  \\
                              0  &  1  &  0   \\
                              6  &  1  &  4
            \end{bmatrix} \]
so $dG_{(3,2,1)}$ is invertible. By the \emph{inverse function theorem} $G$ has a local $\fml
C^1$-inverse $H$ defined on some neighborhood $W$ of $(3,2;0) = G(3,2,1)$. Write $H =
(H^1,H^2)$ where $\ran{H^1} \subseteq \R^2$ and $\ran{H^2} \subseteq \R$. Let $V = \{(x,y):
(x,y,0) \in W\}$ and $h(x,y) = H^2(x,y;0)$.  The function $h$ belongs to $\fml C^1$ because
$H$ does. Now condition (i) holds because
   \begin{align*}
        (3,2;1) &= H(G(3,2;1)) \\
                &= H(3,2;f(3,2,1)) \\
                &= H(3,2;0) \\
                &= (H^1(3,2;0)\,;\,H^2(3,2;0)) \\
                &= (H^1(3,2;0)\,;\,h(3,2))
   \end{align*}
and condition (ii) follows by equating the third components of the first and last terms of the
following computation
   \begin{align*}
        (x,y;0) &= G(H(x,y;0)) \\
                &= G(H^1(x,y;0);H^2(x,y;0)) \\
                &= (H^1(x,y;0)\,;\,f(H^1(x,y;0);H^2(x,y;0))) \\
                &= (x,y\,;\,f(x,y;h(x,y)))\,.
   \end{align*}

  \item[(c)]  We wish to find $\cpd zxy$ and $\cpd zyx$ at $(3,2,1)$; that is, $h_1(3,2)$ and
$h_2(3,2)$, respectively. The \emph{inverse function theorem} tells us that
%\spreadlines{1\jot}
   \begin{align*}
     \begin{bmatrix} dH_{(3,2,0)} \end{bmatrix}
         &= \begin{bmatrix} dH_{G(3,2,1)} \end{bmatrix} \\
         &= {\begin{bmatrix} dG_{(3,2,1)} \end{bmatrix}}^{-1} \\
         &= {\begin{bmatrix}   1  &  0  &  0 \\
                               0  &  1  &  0 \\
                               6  &  1  &  4  \end{bmatrix}}^{-1} \\
         &= {\begin{bmatrix}   1     &     0    &    0 \\
                    0     &     1    &    0 \\
                -\frac32  & -\frac14 &  \frac14  \end{bmatrix}}\,.
   \end{align*}
Thus at $(3,2,1)$
     \[ \cpd zxy = h_1(3,2) = \pd{H^2}x(3,2) = -\frac32 \]
and
     \[ \cpd zyx = h_2(3,2) = \pd{H^2}y(3,2) = -\frac14 \,.   \qedhere \]
 \end{enumerate}
\end{prf}

\begin{prf}\label{sol_xmpl_ift_24}(Solution to~\ref{xmpl_ift_24})
Let $f = (f^1,f^2)$ where
  \[ f^1(u,v;x,y) = 2u^3vx^2 + v^2x^3y^2 - 3u^2y^4 \]
and
  \[ f^2(u,v;x,y) = 2uv^2y^2 - uvx^2 + u^3xy - 2\,. \]
 \begin{enumerate}
   \item[(a)]  There exist a neighborhood $V$ of $(a,b)$ in $\R^2$ and a function
$h \colon V \sto \R^2$ which satisfy
    \begin{alignat*}{2}
        &\text{(i)} &&\quad h(a,b) = (c,d); \quad\text{ and} \\
        &\text{(ii)}  &&\quad f(u,v;h(u,v)) = (0,0) \quad\text{ for all $u$, $v \in V.$}
    \end{alignat*}
  \item[(b)] Let $G(u,v;x,y) := (u,v;f(u,v;x,y))$ for all $u$, $v$, $x$, $y \in \R$.
Then $G$ is continuously differentiable and
    \[ \begin{bmatrix} dG_{(1,1)} \end{bmatrix}
               = \begin{bmatrix}  1  &  0  &  0  &  0  \\
                                  0  &  1  &  0  &  0  \\
                                  0  &  4  &  7  & -10 \\
                                  4  &  3  & -1  &  3
                 \end{bmatrix} \,. \]
Since $\det \bigl[dG_{(1,1)}\bigr] = 11 \ne 0$, we know from the \emph{inverse function
theorem} that $G$ is locally $\fml C^1$-invertible at $(1,1)$. That is, there exist a
neighborhood $W$ of $G(1,1;1,1) = (1,1;0,0)$ in $\R^4$ and a local $\fml C^1$-inverse $H:W
\sto \R^4$ of~$G$. Write $H$ in terms of its component functions, $H = (H^1,H^2)$ where $\ran
H^1$ and $\ran H^2$ are contained in $\R^2$, and set $h(u,v) = H^2(u,v;0,0)$ for all $(u,v)$
in $V := \{(u,v): (u,v;0,0) \in W\}$. Then $V$ is a neighborhood of $(1,1)$ in $\R^2$ and the
function $h$ is continuously differentiable because $H$ is. We conclude that $h(1,1) = (1,1)$
from the following computation.
   \begin{align*}
      (1,1;1,1) &= H(G(1,1;1,1)) \\
                &= H(1,1;f(1,1;1,1)) \\
                &= (H^1(1,1;f(1,1;1,1))\,;\,H^2(1,1;f(1,1;1,1))) \\
                &= (1,1\,;\,H^2(1,1;0,0)) \\
                &= (1,1\,;\,h(1,1)) \,.
   \end{align*}
And from
   \begin{align*}
      (u,v;0,0) &= G(H(u,v;0,0)) \\
                &= G(H^1(u,v;0,0);H^2(u,v;0,0)) \\
                &= (H^1(u,v;0,0)\,;\,f(H^1(u,v;0,0);H^2(u,v;0,0))) \\
                &= (u,v\,;\,f(u,v;h(u,v)))
   \end{align*}
we conclude that (ii) holds; that is,
        \[ f(u,v;h(u,v)) = (0,0) \]
for all $u$, $v \in V$.         \qedhere
 \end{enumerate}
\end{prf}




%END chapter 29

\endinput
